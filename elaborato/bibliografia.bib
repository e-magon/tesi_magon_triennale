% doi significa Digital Object Identifier, è un identificatore univoco per ogni articolo, libro, documento, ecc.

@article{mu2025moe_survey,
	title={A Comprehensive Survey of Mixture-of-Experts: Algorithms, Theory, and Applications},
	author={Mu, Siyuan and Lin, Sen},
	journal={arXiv:2503.07137},
	year={2025},
	doi={10.48550/arXiv.2503.07137},
	note={Version 3, last revised 18 Apr 2025},
	abstract={Artificial intelligence (AI) has achieved astonishing successes in many domains, especially with the recent breakthroughs in the development of foundational large models. These large models, leveraging their extensive training data, provide versatile solutions for a wide range of downstream tasks. However, as modern datasets become increasingly diverse and complex, the development of large AI models faces two major challenges: (1) the enormous consumption of computational resources and deployment difficulties, and (2) the difficulty in fitting heterogeneous and complex data, which limits the usability of the models. Mixture of Experts (MoE) models has recently attracted much attention in addressing these challenges, by dynamically selecting and activating the most relevant sub-models to process input data. It has been shown that MoEs can significantly improve model performance and efficiency with fewer resources, particularly excelling in handling large-scale, multimodal data. Given the tremendous potential MoE has demonstrated across various domains, it is urgent to provide a comprehensive summary of recent advancements of MoEs in many important fields. Existing surveys on MoE have their limitations, e.g., being outdated or lacking discussion on certain key areas, and we aim to address these gaps. In this paper, we first introduce the basic design of MoE, including gating functions, expert networks, routing mechanisms, training strategies, and system design. We then explore the algorithm design of MoE in important machine learning paradigms such as continual learning, meta-learning, multi-task learning, and reinforcement learning. Additionally, we summarize theoretical studies aimed at understanding MoE and review its applications in computer vision and natural language processing. Finally, we discuss promising future research directions. ----- The introduction further motivates MoE by detailing how growing data heterogeneity and computational costs make dense architectures hard to scale, and explains that sparsely activated experts can specialize on different modalities, tasks, and domains while keeping inference efficient. It reviews representative MoE successes in large language models and other areas, argues that prior surveys are outdated or too narrow, and clearly positions this work as a unified treatment of MoE basics, algorithms, theory, and applications in CV and NLP, with a roadmap of the paper’s structure. ----- The conclusion recaps that the survey synthesizes design patterns, algorithmic uses, and theoretical insights around MoE, and shows how MoE improves specialization, scalability, and adaptability across many learning paradigms. It emphasizes open challenges such as stable training, efficient systems, principled architecture design, deeper theory, and domain-tailored algorithms, and calls for further research to fully exploit MoE’s potential in emerging application areas.}
}

@article{ji2024adapting_llm_log,
	title={Adapting Large Language Models to Log Analysis with Interpretable Domain Knowledge},
	author={Ji, Yuhe and Liu, Yilun and Yao, Feiyu and He, Minggui and Tao, Shimin and Zhao, Xiaofeng and Chang, Su and Yang, Xinhua and Meng, Weibin and others},
	journal={arXiv:2412.01377},
	year={2024},
	doi={10.48550/arXiv.2412.01377},
	note={Accepted by CIKM 2025. Version 2, last revised 26 Aug 2025},
	abstract={Log analysis represents a critical sub-domain within AI applications that facilitates automatic approaches to fault and error management of large-scaled software systems, saving labors of traditional manual methods. While existing solutions using large language models (LLMs) show promise, they are limited by a significant domain gap between natural and log languages (the latter contains rich domain-specific tokens such as status codes, IP addresses, resource pathes), which restricts their effectiveness in real-world applications. However, directly adapting general-purpose LLMs to log analysis using raw logs may degrade their performance due to inconsistent token distribution. In this paper, we present a domain adaptation approach that addresses these limitations by integrating interpretable domain knowledge into open-source LLMs through continual pre-training (CPT), which bridges this domain gap by adapting LLMs on interpretable natural texts with log knowledge (instead of raw logs) to reduce distribution discrepancy. To achieve this, we developed NLPLog, a comprehensive dataset containing over 250,000 question-answer pairs on log-related knowledge. Our resulting model, SuperLog, achieves the best performance across four log analysis tasks, with an average accuracy improvement of 12.01% over the second-best model. Ablation study also suggests advantages of domain adaption using interpretable log knowledge over using raw logs. ----- The introduction further motivates the problem by emphasizing that manual inspection of large, complex log streams is no longer scalable, and shows that prompting general-purpose or proprietary LLMs directly on raw logs often yields suboptimal results due to the linguistic gap between terse, irregular logs and natural language. It reviews prior efforts that fine-tune or continually pre-train smaller models on raw logs, highlighting issues such as catastrophic forgetting, reduced interpretability of model predictions, and deployment constraints when relying on API-based proprietary models. These limitations motivate their proposal to adapt open-source LLMs with interpretable log-domain knowledge expressed as natural-language question-answer pairs, instantiated through the NLPLog dataset and the SuperLog model. ----- The conclusion summarizes that continually pre-training open-source LLMs on interpretable log knowledge effectively bridges the gap between natural language and log language, enabling SuperLog to deliver state-of-the-art accuracy and human-understandable explanations across diverse log analysis tasks, including unseen domains. It stresses that ablation studies confirm the superiority of using interpretable log knowledge over raw logs for domain adaptation, and that releasing the NLPLog dataset is intended to catalyze further work on domain-specific CPT and practical, interpretable LLM-based log analysis.}
}

@article{beck2025system_log_parsing,
	title={System Log Parsing with Large Language Models: A Review},
	author={Beck, Viktor and Landauer, Max and Wurzenberger, Markus and Skopik, Florian and Rauber, Andreas},
	journal={arXiv:2504.04877},
	year={2025},
	doi={10.48550/arXiv.2504.04877},
	note={Version 2, last revised 15 May 2025},
	abstract={Log data provides crucial insights for tasks like monitoring, root cause analysis, and anomaly detection. Due to the vast volume of logs, automated log parsing is essential to transform semi-structured log messages into structured representations. Recent advances in large language models (LLMs) have introduced the new research field of LLM-based log parsing. Despite promising results, there is no structured overview of the approaches in this relatively new research field with the earliest advances published in late 2023. This work systematically reviews 29 LLM-based log parsing methods. We benchmark seven of them on public datasets and critically assess their comparability and the reproducibility of their reported results. Our findings summarize the advances of this new research field, with insights on how to report results, which data sets, metrics and which terminology to use, and which inconsistencies to avoid, with code and results made publicly available for transparency. ----- The introduction situates log parsing as a prerequisite for scalable log analytics, explaining how massive, semi-structured logs from modern systems overwhelm manual inspection and require automated template extraction. It reviews conventional parsers, highlighting their reliance on dataset-specific formats, regular expressions, and tuning, and their tendency to work well on curated benchmarks like LogHub while struggling to generalize to new log sources. It then motivates LLM-based log parsing as a way to combine syntactic and semantic understanding via in-context learning, fine-tuning, and caching, and outlines the paper's research questions and contributions: a feature-based taxonomy of 29 approaches and a benchmark of representative frameworks. ----- The conclusion synthesizes evidence that LLM-based parsers can better adapt to diverse log formats and unseen templates than traditional methods, particularly when combined with techniques such as in-context learning, retrieval-augmented prompts, caching, and template revision. At the same time, it stresses ongoing challenges, including high computational cost, susceptibility to hallucinations, uneven code quality, and inconsistent experimental setups that hamper reproducibility and fair comparison. The authors argue for more standardized benchmarks and reporting practices, position their GitHub repository and evaluation as a step toward that goal, and call for future work on more efficient, transparent, and user-friendly LLM-based log parsing systems.}
}

@article{kostina2025llm_text_classification,
	title={Large Language Models For Text Classification: Case Study And Comprehensive Review},
	author={Kostina, Arina and Dikaiakos, Marios D. and Stefanidis, Dimosthenis and Pallis, George},
	journal={arXiv:2501.08457},
	year={2025},
	doi={10.48550/arXiv.2501.08457},
	note={Version 1, last revised 14 Jan 2025},
	abstract={Unlocking the potential of Large Language Models (LLMs) in data classification represents a promising frontier in natural language processing. In this work, we evaluate the performance of different LLMs in comparison with state-of-the-art deep-learning and machine-learning models, in two different classification scenarios: i) the classification of employees' working locations based on job reviews posted online (multiclass classification), and 2) the classification of news articles as fake or not (binary classification). Our analysis encompasses a diverse range of language models differentiating in size, quantization, and architecture. We explore the impact of alternative prompting techniques and evaluate the models based on the weighted F1-score. Also, we examine the trade-off between performance (F1-score) and time (inference response time) for each language model to provide a more nuanced understanding of each model's practical applicability. Our work reveals significant variations in model responses based on the prompting strategies. We find that LLMs, particularly Llama3 and GPT-4, can outperform traditional methods in complex classification tasks, such as multiclass classification, though at the cost of longer inference times. In contrast, simpler ML models offer better performance-to-time trade-offs in simpler binary classification tasks. ----- The introduction frames text classification as a core NLP task whose importance grows with the volume of digital text, and motivates studying LLMs alongside traditional deep and classical models on two realistic scenarios: fake news detection and employee-review classification by work-location status. It explains that the work systematically compares many LLMs, a strong encoder-based RoBERTa baseline, and Naive Bayes/SVM, while probing how model size, quantization, and a spectrum of prompting strategies (zero- and few-shot, chain-of-thought, emotional prompting, role-playing, naming the assistant) jointly affect F1-score and inference time, with the goal of clarifying when LLMs provide practical advantages over established methods. ----- The conclusion finds that large LLMs can match or surpass traditional approaches on more complex multiclass problems, but that their accuracy gains usually come with substantially higher latency and computational cost. It highlights that Naive Bayes and SVM remain very competitive for simpler binary tasks, and that RoBERTa offers a strong accuracy-efficiency compromise, so model choice should depend on task difficulty and resource constraints. The authors also stress that prompt design significantly influences performance—techniques like chain-of-thought and few-shot examples often help, whereas some persona-style prompts can hurt—and outline future work expanding to more datasets, domains, and model families to better understand how scale, architecture, training data, and price-performance trade-offs shape LLMs' value in text classification.}
}

@article{elkhatiba2024performance_benchmarking,
	title={Performance Benchmarking of Traditional Machine Learning and Transformer Models for Multi-Class Text Classification},
	author={El Khatiba, Omar and Alkhatib, Nabeel},
	journal={International Journal of Computer (IJC)},
	volume={55},
	number={1},
	pages={102--116},
	year={2025},
	url={https://ijcjournal.org/InternationalJournalOfComputer/article/view/2404},
	note={Published: 2025-07-14. Loyola University New Orleans},
	abstract={Text classification is a fundamental task in natural language processing (NLP), widely applied in areas such as spam detection, sentiment analysis, and text categorization. This study presents a comparative analysis of three distinct machine learning paradigms—traditional machine learning algorithms (like Random Forest, XGBoost, support vector machine and Naive Bayes), a custom-built transformer architecture, and transfer learning or pre-trained transformer models (BERT, DistilBERT, RoBERTa, ELECTRA)—on the multi-class news classification dataset. While traditional models provided competitive baselines with up to 90.47% accuracy, modern transformer architecture surpassed them, achieving 91% accuracy when trained from scratch. The highest performance was observed with transfer learning using pre-trained models, where RoBERTa achieved 94.54% accuracy, DistillBERT achieved 94.32% accuracy, BERT achieved 94.07% accuracy and ELECTRA achieved 93.66%. These findings highlight the significance of contextual embeddings and large-scale pretraining in advancing text classification performance. ----- The introduction motivates news classification as an important instance of text classification for organizing digital content, powering personalized feeds, and filtering misinformation, and notes that classical ML models with bag-of-words or TF-IDF features are efficient and interpretable but struggle with long-range dependencies and semantic nuance. It then highlights how transformer-based architectures and pre-trained language models (e.g., BERT, DistilBERT, RoBERTa, ELECTRA) better capture contextual meaning, and sets up an empirical comparison on the AG News dataset between traditional classifiers and transformers trained from scratch or fine-tuned from checkpoints, evaluating accuracy, training time, and resource efficiency within a unified experimental framework. ----- The conclusion confirms that transformer-based models clearly outperform traditional classifiers on AG News, with all transformer variants surpassing the best traditional method and RoBERTa achieving the top accuracy and F1-score. It emphasizes that DistilBERT offers a strong balance between performance and efficiency, that class difficulty varies (with Sports easiest and Business/Technology more challenging), and that the choice between traditional and transformer-based approaches should weigh accuracy requirements against training time and computational resources. Overall, the study validates the impact of attention-based architectures for text classification and provides practical guidance for practitioners selecting models for real-world news categorization tasks.}
}

@article{alassan2024comparison_opensource_proprietary,
	title={Comparison of Open-Source and Proprietary LLMs for Machine Reading Comprehension: A Practical Analysis for Industrial Applications},
	author={Alassan, Mahaman Sanoussi Yahaya and Espejel, Jessica López and Bouhandi, Merieme and Dahhane, Walid and Ettifouri, El Hassane},
	journal={arXiv:2406.13713},
	year={2024},
	doi={10.48550/arXiv.2406.13713},
	note={Version 2, last revised 6 Dec 2024},
	abstract={Large Language Models (LLMs) have recently demonstrated remarkable performance in various Natural Language Processing (NLP) applications, such as sentiment analysis, content generation, and personalized recommendations. Despite their impressive capabilities, there remains a significant need for systematic studies concerning the practical application of LLMs in industrial settings, as well as the specific requirements and challenges related to their deployment in these contexts. This need is particularly critical for Machine Reading Comprehension (MCR), where factual, concise, and accurate responses are required. To date, most MCR rely on Small Language Models (SLMs) or Recurrent Neural Networks (RNNs) such as Long Short-Term Memory (LSTM). This article presents a comparative analysis between open-source LLMs and proprietary models on this task, aiming to identify light and open-source alternatives that offer comparable performance to proprietary models. ----- The introduction situates the work in the broader rise of LLMs in NLP and focuses on machine reading comprehension as a key capability for industrial applications such as document analysis, customer support, and knowledge management. It highlights the practical trade-offs between proprietary models, which often deliver state-of-the-art accuracy but at high financial and computational cost with opacity and data-governance concerns, and open-source models, which are more flexible and affordable but may lag in performance. The authors argue that there is a lack of systematic comparisons for MRC in industrial settings and outline their contribution: benchmarking proprietary and open-source LLMs on accuracy, efficiency, scalability, and resource usage to guide practitioners in model selection. ----- The conclusion shows that proprietary LLMs like GPT-3.5 and GPT-4 still achieve the strongest overall accuracy and speed for MRC, but that well-configured open-source models such as Mistral-7B-OpenOrca and LLaMA-2 can reach competitive performance while enabling on-premise deployment, customization, and better control over sensitive data. It emphasizes how quantization and architectural optimizations allow open-source models to offer attractive cost--performance trade-offs, and notes that secure hosting options for proprietary APIs (e.g., Azure OpenAI) partially mitigate confidentiality issues. The authors foresee continued improvements in open-source LLMs and task-specific instruction tuning, arguing that these trends will further narrow the performance gap and support broader industrial adoption where flexibility, privacy, and local data management are priorities.}
}

@article{bendiouis2024deploying_opensource,
	title={Deploying Open-Source Large Language Models: A performance Analysis},
	author={Bendi-Ouis, Yannis and Dutartre, Dan and Hinaut, Xavier},
	journal={arXiv:2409.14887},
	year={2024},
	doi={10.48550/arXiv.2409.14887},
	note={Version 4, last revised 12 Jun 2025},
	abstract={Since the release of ChatGPT in November 2022, large language models (LLMs) have seen considerable success, including in the open-source community, with many open-weight models available. However, the requirements to deploy such a service are often unknown and difficult to evaluate in advance. To facilitate this process, we conducted numerous tests at the Centre Inria de l'Université de Bordeaux. In this article, we propose a comparison of the performance of several models of different sizes (mainly Mistral and LLaMa) depending on the available GPUs, using vLLM, a Python library designed to optimize the inference of these models. Our results provide valuable information for private and public groups wishing to deploy LLMs, allowing them to evaluate the performance of different models based on their available hardware. This study thus contributes to facilitating the adoption and use of these large language models in various application domains. ----- The introduction frames the work in the rapid industrial adoption of LLM-based services and the concentration of compute and control in a few proprietary providers. It argues that open-weight models from actors like Meta, Mistral AI, and DeepSeek are essential for transparency and digital sovereignty, but that even "small" models still require careful engineering, quantization, and hardware planning to be served efficiently to many users. The authors explain how vLLM and related toolchains make high-throughput, multi-user serving feasible on realistic GPU setups, motivating their empirical study of deployment trade-offs for different open-source models and GPUs. ----- The discussion and conclusion show that open-source LLMs such as Mistral-family models and LLaMA-3 can be deployed locally on V100 and A100 GPUs with strong performance and scalability, especially when combined with quantization and architectures like Mixture-of-Experts. They highlight that latency grows sublinearly with concurrent users up to a threshold, that context length and VRAM impose practical limits, and that a modest number of high-end GPUs is enough to offer competitive alternatives to proprietary APIs. The paper emphasizes the strategic benefits of local deployment for data confidentiality and organizational autonomy, and anticipates that newer hardware, larger open-weight models, and improved compression techniques will further democratize high-quality LLM serving in research centers, universities, and industry.}
}

@article{white2024livebench,
	title={LiveBench: A Challenging, Contamination-Limited LLM Benchmark},
	author={White, Colin and Dooley, Samuel and Roberts, Manley and Pal, Arka and Feuer, Ben and Jain, Siddhartha and Shwartz-Ziv, Ravid and Jain, Neel and Saifullah, Khalid and Pandas, Siddartha and Hay, Neel and Hegde, Chinmay and Tenenbaum, Jonas and Frankle, Jonathan and Goldblum, Micah and Goldstein, Tom},
	journal={arXiv:2406.19314},
	year={2024},
	doi={10.48550/arXiv.2406.19314},
	note={Version 2, last revised 18 Apr 2025. ICLR 2025 Spotlight},
	abstract={Test set contamination, wherein test data from a benchmark ends up in a newer model's training set, is a well-documented obstacle for fair LLM evaluation and can quickly render benchmarks obsolete. To mitigate this, many recent benchmarks crowdsource new prompts and evaluations from human or LLM judges; however, these can introduce significant biases, and break down when scoring hard questions. In this work, we introduce a new benchmark for LLMs designed to be resistant to both test set contamination and the pitfalls of LLM judging and human crowdsourcing. We release LiveBench, the first benchmark that (1) contains frequently-updated questions from recent information sources, (2) scores answers automatically according to objective ground-truth values, and (3) contains a wide variety of challenging tasks, spanning math, coding, reasoning, language, instruction following, and data analysis. To achieve this, LiveBench contains questions that are based on recently-released math competitions, arXiv papers, news articles, and datasets, and it contains harder, contamination-limited versions of tasks from previous benchmarks such as Big-Bench Hard, AMPS, and IFEval. We evaluate many prominent closed-source models, as well as dozens of open-source models ranging from 0.5B to 405B in size. LiveBench is difficult, with top models achieving below 70% accuracy. We release all questions, code, and model answers. Questions are added and updated on a monthly basis, and we release new tasks and harder versions of tasks over time so that LiveBench can distinguish between the capabilities of LLMs as they improve in the future. We welcome community engagement and collaboration for expanding the benchmark tasks and models. ----- The introduction explains that standard ML benchmarks are increasingly unreliable for LLMs because many test questions are already present in web-scale training corpora, leading to severe test set contamination and overfitting signals on datasets like Codeforces and GSM8K. It reviews how newer benchmarks rely on LLM or human judges and crowdsourced questions to reduce contamination, but highlights their drawbacks: biased and inconsistent judgments, preference for verbose or self-generated answers, and limited, skewed question diversity from human annotators. Against this backdrop, the authors motivate LiveBench as a framework that jointly minimizes contamination and judging/crowdsourcing biases by using automatically scored, objectively answerable questions drawn from frequently updated real-world sources. ----- The conclusion reiterates that LiveBench operationalizes this framework by combining three key properties: continuously refreshed questions from recent math contests, arXiv papers, and datasets; automatic scoring against ground-truth labels instead of subjective LLM judges; and a broad suite of challenging tasks spanning math, coding, reasoning, language, instruction following, and data analysis. It emphasizes that questions become harder and are updated over time, include contamination-limited variants of existing benchmarks, and that all questions, code, and model outputs are openly released and extended monthly. The authors invite community contributions to expand tasks and models, positioning LiveBench as an evolving benchmark that can more reliably differentiate LLM capabilities as the field progresses.}
}



@inproceedings{shinde2018review_ml_dl,
	title={A review of machine learning and deep learning applications},
	author={Shinde, Pramila P and Shah, Seema},
	booktitle={2018 Fourth International Conference on Computing Communication Control and Automation (ICCUBEA)},
	year={2018},
	organization={IEEE},
	doi={10.1109/ICCUBEA.2018.8697857},
	abstract={Machine learning is one of the fields in the modern computing world. A plenty of research has been undertaken to make machines intelligent. Learning is a natural human behavior which has been made an essential aspect of the machines as well. There are various techniques devised for the same. Traditional machine learning algorithms have been applied in many application areas. Researchers have put many efforts to improve the accuracy of that machinelearning algorithms. Another dimension was given thought which leads to deep learning concept. Deep learning is a subset of machine learning. So far few applications of deep learning have been explored. This is definitely going to cater to solving issues in several new application domains, sub-domains using deep learning. A review of these past and future application domains, sub-domains, and applications of machine learning and deep learning are illustrated in this paper.}
}

@inproceedings{ma2024llmparser,
	title={LLMParser: An Exploratory Study on Using Large Language Models for Log Parsing},
	author={Ma, Zeyang and Chen, An Ran and Kim, Dong Jae and Chen, Tse-Hsun and Wang, Shaowei},
	booktitle={Proceedings of the 46th International Conference on Software Engineering},
	year={2024},
	publisher={ACM},
	doi={10.48550/arXiv.2404.18001},
	abstract={Logs are important in modern software development with runtime information. Log parsing is the first step in many log-based analyses, that involve extracting structured information from unstructured log data. Traditional log parsers face challenges in accurately parsing logs due to the diversity of log formats, which directly impacts the performance of downstream log-analysis tasks. In this paper, we explore the potential of using Large Language Models (LLMs) for log parsing and propose LLMParser, an LLM-based log parser based on generative LLMs and few-shot tuning. We leverage four LLMs, Flan-T5-small, Flan-T5-base, LLaMA-7B, and ChatGLM-6B in LLMParsers. Our evaluation of 16 open-source systems shows that LLMParser achieves statistically significantly higher parsing accuracy than state-of-the-art parsers (a 96% average parsing accuracy). We further conduct a comprehensive empirical analysis on the effect of training size, model size, and pre-training LLM on log parsing accuracy. We find that smaller LLMs may be more effective than more complex LLMs; for instance where Flan-T5-base achieves comparable results as LLaMA-7B with a shorter inference time. We also find that using LLMs pre-trained using logs from other systems does not always improve parsing accuracy. While using pre-trained Flan-T5-base shows an improvement in accuracy, pre-trained LLaMA results in a decrease (decrease by almost 55% in group accuracy). In short, our study provides empirical evidence for using LLMs for log parsing and highlights the limitations and future research direction of LLM-based log parsers. ----- The introduction situates log parsing as an essential but challenging first step for large-scale log analysis, explaining how massive and heterogeneous logs overwhelm manual inspection and how existing parsers based on pattern mining, clustering, and parsing trees often miss variables and hurt downstream anomaly detection and diagnosis. It motivates viewing log parsing as a translation task from raw logs to log templates and argues that generative LLMs can better capture both natural-language text and code-like tokens. The authors introduce LLMParser, a family of few-shot tuned parsers built on four open LLMs, and outline research questions on how model type, size, shot size, and pre-training settings affect parsing accuracy and efficiency compared to state-of-the-art tools. ----- The conclusion reports that LLMParser consistently achieves higher parsing accuracy than traditional log parsers and previous LLM-based approaches while remaining practical to fine-tune with only tens of labeled examples per system. It emphasizes that few-shot tuning is more effective and efficient than in-context learning, that training data diversity matters more than simply increasing shot size, and that medium-size models such as Flan-T5-base can rival much larger ones, whereas pre-training on logs from other systems does not always help. The authors highlight remaining challenges around unseen log templates and hard-to-identify variable types, and call for future work on better sampling strategies, log-tailored LLM architectures, and pre-training schemes that further improve generalization without sacrificing practicality.}
}

@inproceedings{zhu2024can_llm_context,
	title={Can Large Language Models Understand Context?},
	author={Zhu, Yilun and Moniz, Joel Ruben Antony and Bhargava, Shruti and Lu, Jiarui and Piraviperumal, Dhivya and Li, Site and Zhang, Yuan and Yu, Hong and Tseng, Bo-Hsiang},
	booktitle={Findings of the Association for Computational Linguistics: EACL 2024},
	pages={2004--2018},
	year={2024},
	publisher={Association for Computational Linguistics},
	address={St. Julian's, Malta},
	url={https://aclanthology.org/2024.findings-eacl.135/},
	abstract={Understanding context is key to understanding human language, an ability which Large Language Models (LLMs) have been increasingly seen to demonstrate to an impressive extent. However, though the evaluation of LLMs encompasses various domains within the realm of Natural Language Processing, limited attention has been paid to probing their linguistic capability of understanding contextual features. This paper introduces a context understanding benchmark by adapting existing datasets to suit the evaluation of generative models. This benchmark comprises of four distinct tasks and nine datasets, all featuring prompts designed to assess the models’ ability to understand context. First, we evaluate the performance of LLMs under the in-context learning pretraining scenario. Experimental results indicate that pre-trained dense models struggle with understanding more nuanced contextual features when compared to state-of-the-art fine-tuned models. Second, as LLM compression holds growing significance in both research and real-world applications, we assess the context understanding of quantized models under in-context-learning settings. We find that 3-bit post-training quantization leads to varying degrees of performance reduction on our benchmark. We conduct an extensive analysis of these scenarios to substantiate our experimental results. ----- The introduction frames discourse understanding as going beyond single sentences to phenomena like coreference, discourse relations, dialogue state, and ellipsis, and argues that current LLM benchmarks rarely include discourse-focused datasets, so they fail to truly test contextual linguistic competence. It highlights that despite many benchmarks for commonsense, sentiment, NLI, and other tasks, none are tailored to nuanced context understanding, and that large LLMs raise deployment and compression concerns whose impact on contextual ability is poorly studied. Motivated by these gaps, the authors propose a context understanding benchmark built from multiple discourse datasets, use prompts for in-context learning, and systematically evaluate dense and post-training-quantized LLMs of different sizes. ----- The conclusion summarizes that the proposed benchmark, spanning four tasks and nine adapted datasets, systematically probes document- and dialogue-level context understanding for generative LLMs. It reports that under in-context learning, LLMs struggle with fine-grained linguistic features on this benchmark and can behave inconsistently with their performance on more general benchmarks, and that 3-bit post-training quantization further degrades contextual understanding to varying degrees across tasks. The authors position their benchmark and dense-vs-quantized comparison as a new lens on the contextual dimension of language understanding and a valuable complement to existing LLM evaluations, encouraging future work to study where and why models’ context competence breaks down.}
}


@article{karlsen2023benchmarking_llm_log,
	title={Benchmarking Large Language Models for Log Analysis, Security, and Interpretation},
	author={Karlsen, Egil and Luo, Xiao and Zincir-Heywood, Nur and Heywood, Malcolm},
	journal={arXiv:2311.14519},
	year={2023},
	doi={10.48550/arXiv.2311.14519},
	note={Version 1, last revised 24 Nov 2023},
	abstract={This paper benchmarks five large language model architectures (BERT, RoBERTa, DistilRoBERTa, GPT-2, and GPT-Neo) for security-oriented log analysis across six labeled application and system log datasets using a unified sequence-classification setup and the LLM4Sec experimentation pipeline. By fine-tuning 60 models, the authors show that LLM-based semantic feature extraction combined with domain adaptation can achieve near-perfect F1-scores for intrusion detection and substantially outperform prior log-analysis baselines, while also delivering a reusable framework for experimentation, evaluation, and analysis. ----- The introduction describes how modern enterprise infrastructures generate massive, heterogeneous logs that are central to monitoring and forensics but hard to exploit with traditional pipelines that depend on static parsers such as Drain and manually engineered features. It summarizes prior NLP and deep-learning approaches for log analysis, including word2vec+TF-IDF and earlier LLM-based embedding methods, and argues that they often target narrow datasets or require brittle pre-processing, which limits generality and adaptability to new log sources. Motivated by these gaps, the authors propose to systematically explore LLM-based feature extraction for both application and system logs across multiple datasets and architectures, while also studying domain adaptation via fine-tuning, interpretability via t-SNE and SHAP, and an end-to-end pipeline (LLM4Sec) for reproducible benchmarking. ----- The conclusion reports that fine-tuned sequence-classification models based on the evaluated LLMs consistently outperform previous log-analysis techniques on all six datasets, with DistilRoBERTa in particular delivering very high F1-scores and low false-positive rates. It highlights that domain adaptation through fine-tuning is crucial for enabling LLMs trained on generic text to correctly weight log components, and that visualization tools such as t-SNE and SHAP make the resulting anomaly detectors more interpretable for security analysts by exposing which tokens drive NORMAL versus ANOMALOUS decisions. The authors emphasize that the LLM4Sec pipeline supports reproducible experiments and future extensions, and suggest directions such as alternative tokenization schemes, further pre-training, and ultimately training smaller, security-specific LLMs on diverse log corpora to improve efficiency.}
}

@inproceedings{dai2022revisiting_long_doc,
	title={Revisiting Transformer-based Models for Long Document Classification},
	author={Dai, Xiang and Chalkidis, Ilias and Darkner, Sune and Elliott, Desmond},
	booktitle={Findings of the Association for Computational Linguistics: EMNLP 2022},
	year={2022},
	publisher={Association for Computational Linguistics},
	doi={10.48550/arXiv.2204.06683},
	url={https://arxiv.org/abs/2204.06683},
	abstract={Recent work on text classification has largely focused on short sequences, while many real-world applications involve multi-page, multi-paragraph documents that vanilla Transformers cannot process efficiently due to their quadratic self-attention cost and limited pre-training context. This paper revisits Transformer-based long document classification by comparing sparse-attention models such as Longformer and hierarchical Transformers under a range of design choices (e.g., attention window size, number of globally attended tokens, and document splitting strategies) on four datasets from different domains, showing that encoding substantially more tokens brings clear performance gains and enabling practical guidance for applying Transformers to long-document tasks. ----- The introduction contrasts typical short-text benchmarks with genuinely long-document settings, arguing that truncating documents to 512 tokens often discards crucial information and explaining why naive application of pre-trained BERT-style encoders is inadequate. It reviews the computational challenges of standard self-attention, motivates long-document architectures like sparse-attention and hierarchical models, and notes that prior evaluations have relied on datasets where most texts are still relatively short or where Transformers underperform CNN/RNN baselines on truly long clinical notes such as those in MIMIC-III. Building on this, the authors aim to transfer the success of the pre-train--fine-tune paradigm to long documents by systematically comparing sparse and hierarchical Transformer approaches and studying how architectural and training choices affect both effectiveness and efficiency. ----- The conclusion shows that when they are allowed to process longer inputs, Transformer-based models can match or surpass CNN-based baselines for long document classification, rebutting earlier criticisms of their suitability for this setting. Experiments on MIMIC-III, ECtHR, 20News, and Hyperpartisan reveal that Longformer with up to 4096 tokens already delivers competitive performance and that moderate local attention windows plus a small number of global tokens offer a good trade-off between accuracy and efficiency. Hierarchical Transformers achieve the strongest overall results, with the most important design factor being how documents are split into segments; the authors find that relatively short, overlapping segments (around 128 tokens) often work well, though optimal lengths remain dataset-dependent. Overall, the paper provides concrete recommendations for configuring long-document Transformers and highlights that carefully designed architectures and input partitioning are key to unlocking their full potential.}
}



@misc{ibm_moe,
  author = {Bergmann, Dave},
	title = {What is mixture of experts?},
	howpublished = {\url{https://www.ibm.com/think/topics/mixture-of-experts}},
	note = {Accessed: 2025-06-01}
}

@misc{mistral_moe,
  author = {Mistral AI team},
	title = {Mixtral of experts: A high quality Sparse Mixture-of-Experts},
	howpublished = {\url{https://mistral.ai/news/mixtral-of-experts}},
	note = {Accessed: 2025-06-01}
}



%%%%%%%%%% Esempi %%%%%%%%%%
@article{esempio_articolo,
	title={A multi-layered, time-based music description approach based on XML},
	author={Haus, Goffredo and Longari, Maurizio},
	journal={Computer Music Journal},
	volume={29},
	number={1},
	year={2005},
	publisher={MIT Press}
}

@book{esempio_libro,
  title = {Research methods in {Human-Computer Interaction}},
  author = {Lazar, Jonathan and Feng, Jijuan Heidi and Hochheiser, Harry},
  publisher = {Morgan Kaufmann},
  year = {2017},
  edition = {Second},
  address = {Cambridge}
}

@misc{esempio_sito,
  author = {W. H\"am\"al\"ainen},
	title = {Scientific Writing for Computer Science Students},
	howpublished = {\url{http://www.cs.joensuu.fi/\~whamalai/teaching.html}},
	note = {Accessed: 2019-04-10}
}


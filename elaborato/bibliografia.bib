% doi significa Digital Object Identifier, è un identificatore univoco per ogni articolo, libro, documento, ecc.

@article{mu2025moe_survey,
	title={A Comprehensive Survey of Mixture-of-Experts: Algorithms, Theory, and Applications},
	author={Mu, Siyuan and Lin, Sen},
	journal={arXiv:2503.07137},
	year={2025},
	doi={10.48550/arXiv.2503.07137},
	note={Version 3, last revised 18 Apr 2025},
	abstract={Artificial intelligence (AI) has achieved astonishing successes in many domains, especially with the recent breakthroughs in the development of foundational large models. These large models, leveraging their extensive training data, provide versatile solutions for a wide range of downstream tasks. However, as modern datasets become increasingly diverse and complex, the development of large AI models faces two major challenges: (1) the enormous consumption of computational resources and deployment difficulties, and (2) the difficulty in fitting heterogeneous and complex data, which limits the usability of the models. Mixture of Experts (MoE) models has recently attracted much attention in addressing these challenges, by dynamically selecting and activating the most relevant sub-models to process input data. It has been shown that MoEs can significantly improve model performance and efficiency with fewer resources, particularly excelling in handling large-scale, multimodal data. Given the tremendous potential MoE has demonstrated across various domains, it is urgent to provide a comprehensive summary of recent advancements of MoEs in many important fields. Existing surveys on MoE have their limitations, e.g., being outdated or lacking discussion on certain key areas, and we aim to address these gaps. In this paper, we first introduce the basic design of MoE, including gating functions, expert networks, routing mechanisms, training strategies, and system design. We then explore the algorithm design of MoE in important machine learning paradigms such as continual learning, meta-learning, multi-task learning, and reinforcement learning. Additionally, we summarize theoretical studies aimed at understanding MoE and review its applications in computer vision and natural language processing. Finally, we discuss promising future research directions. ----- The introduction further motivates MoE by detailing how growing data heterogeneity and computational costs make dense architectures hard to scale, and explains that sparsely activated experts can specialize on different modalities, tasks, and domains while keeping inference efficient. It reviews representative MoE successes in large language models and other areas, argues that prior surveys are outdated or too narrow, and clearly positions this work as a unified treatment of MoE basics, algorithms, theory, and applications in CV and NLP, with a roadmap of the paper’s structure. ----- The conclusion recaps that the survey synthesizes design patterns, algorithmic uses, and theoretical insights around MoE, and shows how MoE improves specialization, scalability, and adaptability across many learning paradigms. It emphasizes open challenges such as stable training, efficient systems, principled architecture design, deeper theory, and domain-tailored algorithms, and calls for further research to fully exploit MoE’s potential in emerging application areas.}
}

@article{ji2024adapting_llm_log,
	title={Adapting Large Language Models to Log Analysis with Interpretable Domain Knowledge},
	author={Ji, Yuhe and Liu, Yilun and Yao, Feiyu and He, Minggui and Tao, Shimin and Zhao, Xiaofeng and Chang, Su and Yang, Xinhua and Meng, Weibin and others},
	journal={arXiv:2412.01377},
	year={2024},
	doi={10.48550/arXiv.2412.01377},
	note={Accepted by CIKM 2025. Version 2, last revised 26 Aug 2025},
	abstract={Log analysis represents a critical sub-domain within AI applications that facilitates automatic approaches to fault and error management of large-scaled software systems, saving labors of traditional manual methods. While existing solutions using large language models (LLMs) show promise, they are limited by a significant domain gap between natural and log languages (the latter contains rich domain-specific tokens such as status codes, IP addresses, resource pathes), which restricts their effectiveness in real-world applications. However, directly adapting general-purpose LLMs to log analysis using raw logs may degrade their performance due to inconsistent token distribution. In this paper, we present a domain adaptation approach that addresses these limitations by integrating interpretable domain knowledge into open-source LLMs through continual pre-training (CPT), which bridges this domain gap by adapting LLMs on interpretable natural texts with log knowledge (instead of raw logs) to reduce distribution discrepancy. To achieve this, we developed NLPLog, a comprehensive dataset containing over 250,000 question-answer pairs on log-related knowledge. Our resulting model, SuperLog, achieves the best performance across four log analysis tasks, with an average accuracy improvement of 12.01% over the second-best model. Ablation study also suggests advantages of domain adaption using interpretable log knowledge over using raw logs. ----- The introduction further motivates the problem by emphasizing that manual inspection of large, complex log streams is no longer scalable, and shows that prompting general-purpose or proprietary LLMs directly on raw logs often yields suboptimal results due to the linguistic gap between terse, irregular logs and natural language. It reviews prior efforts that fine-tune or continually pre-train smaller models on raw logs, highlighting issues such as catastrophic forgetting, reduced interpretability of model predictions, and deployment constraints when relying on API-based proprietary models. These limitations motivate their proposal to adapt open-source LLMs with interpretable log-domain knowledge expressed as natural-language question-answer pairs, instantiated through the NLPLog dataset and the SuperLog model. ----- The conclusion summarizes that continually pre-training open-source LLMs on interpretable log knowledge effectively bridges the gap between natural language and log language, enabling SuperLog to deliver state-of-the-art accuracy and human-understandable explanations across diverse log analysis tasks, including unseen domains. It stresses that ablation studies confirm the superiority of using interpretable log knowledge over raw logs for domain adaptation, and that releasing the NLPLog dataset is intended to catalyze further work on domain-specific CPT and practical, interpretable LLM-based log analysis.}
}

@article{beck2025system_log_parsing,
	title={System Log Parsing with Large Language Models: A Review},
	author={Beck, Viktor and Landauer, Max and Wurzenberger, Markus and Skopik, Florian and Rauber, Andreas},
	journal={arXiv:2504.04877},
	year={2025},
	doi={10.48550/arXiv.2504.04877},
	note={Version 2, last revised 15 May 2025},
	abstract={Log data provides crucial insights for tasks like monitoring, root cause analysis, and anomaly detection. Due to the vast volume of logs, automated log parsing is essential to transform semi-structured log messages into structured representations. Recent advances in large language models (LLMs) have introduced the new research field of LLM-based log parsing. Despite promising results, there is no structured overview of the approaches in this relatively new research field with the earliest advances published in late 2023. This work systematically reviews 29 LLM-based log parsing methods. We benchmark seven of them on public datasets and critically assess their comparability and the reproducibility of their reported results. Our findings summarize the advances of this new research field, with insights on how to report results, which data sets, metrics and which terminology to use, and which inconsistencies to avoid, with code and results made publicly available for transparency. ----- The introduction situates log parsing as a prerequisite for scalable log analytics, explaining how massive, semi-structured logs from modern systems overwhelm manual inspection and require automated template extraction. It reviews conventional parsers, highlighting their reliance on dataset-specific formats, regular expressions, and tuning, and their tendency to work well on curated benchmarks like LogHub while struggling to generalize to new log sources. It then motivates LLM-based log parsing as a way to combine syntactic and semantic understanding via in-context learning, fine-tuning, and caching, and outlines the paper's research questions and contributions: a feature-based taxonomy of 29 approaches and a benchmark of representative frameworks. ----- The conclusion synthesizes evidence that LLM-based parsers can better adapt to diverse log formats and unseen templates than traditional methods, particularly when combined with techniques such as in-context learning, retrieval-augmented prompts, caching, and template revision. At the same time, it stresses ongoing challenges, including high computational cost, susceptibility to hallucinations, uneven code quality, and inconsistent experimental setups that hamper reproducibility and fair comparison. The authors argue for more standardized benchmarks and reporting practices, position their GitHub repository and evaluation as a step toward that goal, and call for future work on more efficient, transparent, and user-friendly LLM-based log parsing systems.}
}

@article{kostina2025llm_text_classification,
	title={Large Language Models For Text Classification: Case Study And Comprehensive Review},
	author={Kostina, Arina and Dikaiakos, Marios D. and Stefanidis, Dimosthenis and Pallis, George},
	journal={arXiv:2501.08457},
	year={2025},
	doi={10.48550/arXiv.2501.08457},
	note={Version 1, last revised 14 Jan 2025},
	abstract={Unlocking the potential of Large Language Models (LLMs) in data classification represents a promising frontier in natural language processing. In this work, we evaluate the performance of different LLMs in comparison with state-of-the-art deep-learning and machine-learning models, in two different classification scenarios: i) the classification of employees' working locations based on job reviews posted online (multiclass classification), and 2) the classification of news articles as fake or not (binary classification). Our analysis encompasses a diverse range of language models differentiating in size, quantization, and architecture. We explore the impact of alternative prompting techniques and evaluate the models based on the weighted F1-score. Also, we examine the trade-off between performance (F1-score) and time (inference response time) for each language model to provide a more nuanced understanding of each model's practical applicability. Our work reveals significant variations in model responses based on the prompting strategies. We find that LLMs, particularly Llama3 and GPT-4, can outperform traditional methods in complex classification tasks, such as multiclass classification, though at the cost of longer inference times. In contrast, simpler ML models offer better performance-to-time trade-offs in simpler binary classification tasks. ----- The introduction frames text classification as a core NLP task whose importance grows with the volume of digital text, and motivates studying LLMs alongside traditional deep and classical models on two realistic scenarios: fake news detection and employee-review classification by work-location status. It explains that the work systematically compares many LLMs, a strong encoder-based RoBERTa baseline, and Naive Bayes/SVM, while probing how model size, quantization, and a spectrum of prompting strategies (zero- and few-shot, chain-of-thought, emotional prompting, role-playing, naming the assistant) jointly affect F1-score and inference time, with the goal of clarifying when LLMs provide practical advantages over established methods. ----- The conclusion finds that large LLMs can match or surpass traditional approaches on more complex multiclass problems, but that their accuracy gains usually come with substantially higher latency and computational cost. It highlights that Naive Bayes and SVM remain very competitive for simpler binary tasks, and that RoBERTa offers a strong accuracy-efficiency compromise, so model choice should depend on task difficulty and resource constraints. The authors also stress that prompt design significantly influences performance—techniques like chain-of-thought and few-shot examples often help, whereas some persona-style prompts can hurt—and outline future work expanding to more datasets, domains, and model families to better understand how scale, architecture, training data, and price-performance trade-offs shape LLMs' value in text classification.}
}

@article{elkhatiba2024performance_benchmarking,
	title={Performance Benchmarking of Traditional Machine Learning and Transformer Models for Multi-Class Text Classification},
	author={El Khatiba, Omar and Alkhatib, Nabeel},
	journal={International Journal of Computer (IJC)},
	volume={55},
	number={1},
	pages={102--116},
	year={2025},
	url={https://ijcjournal.org/InternationalJournalOfComputer/article/view/2404},
	note={Published: 2025-07-14. Loyola University New Orleans},
	abstract={Text classification is a fundamental task in natural language processing (NLP), widely applied in areas such as spam detection, sentiment analysis, and text categorization. This study presents a comparative analysis of three distinct machine learning paradigms—traditional machine learning algorithms (like Random Forest, XGBoost, support vector machine and Naive Bayes), a custom-built transformer architecture, and transfer learning or pre-trained transformer models (BERT, DistilBERT, RoBERTa, ELECTRA)—on the multi-class news classification dataset. While traditional models provided competitive baselines with up to 90.47% accuracy, modern transformer architecture surpassed them, achieving 91% accuracy when trained from scratch. The highest performance was observed with transfer learning using pre-trained models, where RoBERTa achieved 94.54% accuracy, DistillBERT achieved 94.32% accuracy, BERT achieved 94.07% accuracy and ELECTRA achieved 93.66%. These findings highlight the significance of contextual embeddings and large-scale pretraining in advancing text classification performance. ----- The introduction motivates news classification as an important instance of text classification for organizing digital content, powering personalized feeds, and filtering misinformation, and notes that classical ML models with bag-of-words or TF-IDF features are efficient and interpretable but struggle with long-range dependencies and semantic nuance. It then highlights how transformer-based architectures and pre-trained language models (e.g., BERT, DistilBERT, RoBERTa, ELECTRA) better capture contextual meaning, and sets up an empirical comparison on the AG News dataset between traditional classifiers and transformers trained from scratch or fine-tuned from checkpoints, evaluating accuracy, training time, and resource efficiency within a unified experimental framework. ----- The conclusion confirms that transformer-based models clearly outperform traditional classifiers on AG News, with all transformer variants surpassing the best traditional method and RoBERTa achieving the top accuracy and F1-score. It emphasizes that DistilBERT offers a strong balance between performance and efficiency, that class difficulty varies (with Sports easiest and Business/Technology more challenging), and that the choice between traditional and transformer-based approaches should weigh accuracy requirements against training time and computational resources. Overall, the study validates the impact of attention-based architectures for text classification and provides practical guidance for practitioners selecting models for real-world news categorization tasks.}
}

@article{alassan2024comparison_opensource_proprietary,
	title={Comparison of Open-Source and Proprietary LLMs for Machine Reading Comprehension: A Practical Analysis for Industrial Applications},
	author={Alassan, Mahaman Sanoussi Yahaya and Espejel, Jessica López and Bouhandi, Merieme and Dahhane, Walid and Ettifouri, El Hassane},
	journal={arXiv:2406.13713},
	year={2024},
	doi={10.48550/arXiv.2406.13713},
	note={Version 2, last revised 6 Dec 2024},
	abstract={Large Language Models (LLMs) have recently demonstrated remarkable performance in various Natural Language Processing (NLP) applications, such as sentiment analysis, content generation, and personalized recommendations. Despite their impressive capabilities, there remains a significant need for systematic studies concerning the practical application of LLMs in industrial settings, as well as the specific requirements and challenges related to their deployment in these contexts. This need is particularly critical for Machine Reading Comprehension (MCR), where factual, concise, and accurate responses are required. To date, most MCR rely on Small Language Models (SLMs) or Recurrent Neural Networks (RNNs) such as Long Short-Term Memory (LSTM). This article presents a comparative analysis between open-source LLMs and proprietary models on this task, aiming to identify light and open-source alternatives that offer comparable performance to proprietary models. ----- The introduction situates the work in the broader rise of LLMs in NLP and focuses on machine reading comprehension as a key capability for industrial applications such as document analysis, customer support, and knowledge management. It highlights the practical trade-offs between proprietary models, which often deliver state-of-the-art accuracy but at high financial and computational cost with opacity and data-governance concerns, and open-source models, which are more flexible and affordable but may lag in performance. The authors argue that there is a lack of systematic comparisons for MRC in industrial settings and outline their contribution: benchmarking proprietary and open-source LLMs on accuracy, efficiency, scalability, and resource usage to guide practitioners in model selection. ----- The conclusion shows that proprietary LLMs like GPT-3.5 and GPT-4 still achieve the strongest overall accuracy and speed for MRC, but that well-configured open-source models such as Mistral-7B-OpenOrca and LLaMA-2 can reach competitive performance while enabling on-premise deployment, customization, and better control over sensitive data. It emphasizes how quantization and architectural optimizations allow open-source models to offer attractive cost--performance trade-offs, and notes that secure hosting options for proprietary APIs (e.g., Azure OpenAI) partially mitigate confidentiality issues. The authors foresee continued improvements in open-source LLMs and task-specific instruction tuning, arguing that these trends will further narrow the performance gap and support broader industrial adoption where flexibility, privacy, and local data management are priorities.}
}

@article{bendiouis2024deploying_opensource,
	title={Deploying Open-Source Large Language Models: A performance Analysis},
	author={Bendi-Ouis, Yannis and Dutartre, Dan and Hinaut, Xavier},
	journal={arXiv:2409.14887},
	year={2024},
	doi={10.48550/arXiv.2409.14887},
	note={Version 4, last revised 12 Jun 2025},
	abstract={Since the release of ChatGPT in November 2022, large language models (LLMs) have seen considerable success, including in the open-source community, with many open-weight models available. However, the requirements to deploy such a service are often unknown and difficult to evaluate in advance. To facilitate this process, we conducted numerous tests at the Centre Inria de l'Université de Bordeaux. In this article, we propose a comparison of the performance of several models of different sizes (mainly Mistral and LLaMa) depending on the available GPUs, using vLLM, a Python library designed to optimize the inference of these models. Our results provide valuable information for private and public groups wishing to deploy LLMs, allowing them to evaluate the performance of different models based on their available hardware. This study thus contributes to facilitating the adoption and use of these large language models in various application domains. ----- The introduction frames the work in the rapid industrial adoption of LLM-based services and the concentration of compute and control in a few proprietary providers. It argues that open-weight models from actors like Meta, Mistral AI, and DeepSeek are essential for transparency and digital sovereignty, but that even "small" models still require careful engineering, quantization, and hardware planning to be served efficiently to many users. The authors explain how vLLM and related toolchains make high-throughput, multi-user serving feasible on realistic GPU setups, motivating their empirical study of deployment trade-offs for different open-source models and GPUs. ----- The discussion and conclusion show that open-source LLMs such as Mistral-family models and LLaMA-3 can be deployed locally on V100 and A100 GPUs with strong performance and scalability, especially when combined with quantization and architectures like Mixture-of-Experts. They highlight that latency grows sublinearly with concurrent users up to a threshold, that context length and VRAM impose practical limits, and that a modest number of high-end GPUs is enough to offer competitive alternatives to proprietary APIs. The paper emphasizes the strategic benefits of local deployment for data confidentiality and organizational autonomy, and anticipates that newer hardware, larger open-weight models, and improved compression techniques will further democratize high-quality LLM serving in research centers, universities, and industry.}
}

@article{white2024livebench,
	title={LiveBench: A Challenging, Contamination-Limited LLM Benchmark},
	author={White, Colin and Dooley, Samuel and Roberts, Manley and Pal, Arka and Feuer, Ben and Jain, Siddhartha and Shwartz-Ziv, Ravid and Jain, Neel and Saifullah, Khalid and Pandas, Siddartha and Hay, Neel and Hegde, Chinmay and Tenenbaum, Jonas and Frankle, Jonathan and Goldblum, Micah and Goldstein, Tom},
	journal={arXiv:2406.19314},
	year={2024},
	doi={10.48550/arXiv.2406.19314},
	note={Version 2, last revised 18 Apr 2025. ICLR 2025 Spotlight},
	abstract={Test set contamination, wherein test data from a benchmark ends up in a newer model's training set, is a well-documented obstacle for fair LLM evaluation and can quickly render benchmarks obsolete. To mitigate this, many recent benchmarks crowdsource new prompts and evaluations from human or LLM judges; however, these can introduce significant biases, and break down when scoring hard questions. In this work, we introduce a new benchmark for LLMs designed to be resistant to both test set contamination and the pitfalls of LLM judging and human crowdsourcing. We release LiveBench, the first benchmark that (1) contains frequently-updated questions from recent information sources, (2) scores answers automatically according to objective ground-truth values, and (3) contains a wide variety of challenging tasks, spanning math, coding, reasoning, language, instruction following, and data analysis. To achieve this, LiveBench contains questions that are based on recently-released math competitions, arXiv papers, news articles, and datasets, and it contains harder, contamination-limited versions of tasks from previous benchmarks such as Big-Bench Hard, AMPS, and IFEval. We evaluate many prominent closed-source models, as well as dozens of open-source models ranging from 0.5B to 405B in size. LiveBench is difficult, with top models achieving below 70% accuracy. We release all questions, code, and model answers. Questions are added and updated on a monthly basis, and we release new tasks and harder versions of tasks over time so that LiveBench can distinguish between the capabilities of LLMs as they improve in the future. We welcome community engagement and collaboration for expanding the benchmark tasks and models. ----- The introduction explains that standard ML benchmarks are increasingly unreliable for LLMs because many test questions are already present in web-scale training corpora, leading to severe test set contamination and overfitting signals on datasets like Codeforces and GSM8K. It reviews how newer benchmarks rely on LLM or human judges and crowdsourced questions to reduce contamination, but highlights their drawbacks: biased and inconsistent judgments, preference for verbose or self-generated answers, and limited, skewed question diversity from human annotators. Against this backdrop, the authors motivate LiveBench as a framework that jointly minimizes contamination and judging/crowdsourcing biases by using automatically scored, objectively answerable questions drawn from frequently updated real-world sources. ----- The conclusion reiterates that LiveBench operationalizes this framework by combining three key properties: continuously refreshed questions from recent math contests, arXiv papers, and datasets; automatic scoring against ground-truth labels instead of subjective LLM judges; and a broad suite of challenging tasks spanning math, coding, reasoning, language, instruction following, and data analysis. It emphasizes that questions become harder and are updated over time, include contamination-limited variants of existing benchmarks, and that all questions, code, and model outputs are openly released and extended monthly. The authors invite community contributions to expand tasks and models, positioning LiveBench as an evolving benchmark that can more reliably differentiate LLM capabilities as the field progresses.}
}

@article{ojo2025apt_cloudtrail,
	title={Investigating Advanced Persistent Threat Tactics in Cloud Environments: A Forensic Study of AWS CloudTrail Log Data},
	author={Ojo, Adeolu Opeyemi and Benmubarak, Mohammed},
	journal={International Journal of Innovative Science and Research Technology (IJISRT)},
	volume={10},
	number={7},
	year={2025},
	doi={10.38124/ijisrt/25jul1786},
	abstract={The focus of this study is to identify and reduce Advanced Persistent Threats (APTs) in the cloud environment of Amazon Web Services (AWS). Popular security frameworks like MITRE ATT\&CK, Cyber-Kill Chain and Pyramid of Pain were employed to improve effectiveness of forensic investigation in cloud environments. Tactics, techniques and procedures (TTPs) using Cloud Trail log data were analyzed in order to discover the efficiency of these frameworks in attack patterns identification. Findings from the study reveals that logs are crucial for identifying attack trends such as lateral movement, exfiltration of data, escalation of privileges in order to help improve understanding and analysis of APT activities in AWS environment, and the integration of frameworks such as MITRE ATT\&CK, Cyber-Kill Chain and Pyramid of Pain provides strategies that are proactive to quelling advanced cyber adversaries. ----- The introduction highlights that global cloud computing adoption has redefined how businesses manage IT resources, but brings significant security risks including data breaches and operational disruptions. It notes that traditional forensic tools were not designed for cloud environments, creating difficulties with evidence chronology and architectural complexity. The authors identify critical gaps in existing research, particularly the lack of integration between APT detection techniques and cloud forensics methodologies, and limited research on optimal application of threat detection methods within cloud infrastructures. Building on prior work that used only Cyber Kill Chain and MITRE ATT\&CK, this study aims to examine APT behavior in AWS by incorporating three security frameworks (adding Pyramid of Pain) through APT emulation scenarios and integrated framework implementation. ----- The conclusion advocates for efficient defense techniques against advanced cyber-attacks by examining APT methods within AWS and mapping observed patterns to established security frameworks. It acknowledges experimental limitations but emphasizes the importance of understanding APT actor behavior for security experts to modify their defense techniques accordingly. Future research directions include examining other cloud architectures beyond AWS to determine cross-platform applicability, and investigating advanced methods such as machine learning-based anomaly detection and automated response mechanisms to improve APT detection and mitigation in cloud environments.}
}

@article{singh2024mesa_security,
	title={The MESA Security Model 2.0: A Dynamic Framework for Mitigating Stealth Data Exfiltration},
	author={Singh, Sanjeev Pratap and Afzal, Naveed},
	journal={International Journal of Network Security \& Its Applications (IJNSA)},
	year={2024},
	doi={10.48550/arXiv.2405.10880},
	note={arXiv:2405.10880, Version 1, last revised 17 May 2024},
	abstract={The rising complexity of cyber threats calls for a comprehensive reassessment of current security frameworks in business environments. This research focuses on Stealth Data Exfiltration, a significant cyber threat characterized by covert infiltration, extended undetectability, and unauthorized dissemination of confidential data. Our findings reveal that conventional defense-in-depth strategies often fall short in combating these sophisticated threats, highlighting the immediate need for a shift in information risk management across businesses. The evolving nature of cyber threats, driven by advancements in techniques such as social engineering, multi-vector attacks, and Generative AI, underscores the need for robust, adaptable, and comprehensive security strategies. As we navigate this complex landscape, it is crucial to anticipate potential threats and continually update our defenses. We propose a shift from traditional perimeter-based, prevention-focused models, which depend on a static attack surface, to a more dynamic framework that prepares for inevitable breaches. This suggested model, known as MESA 2.0 Security Model, prioritizes swift detection, immediate response, and ongoing resilience, thereby enhancing an organization's ability to promptly identify and neutralize threats, significantly reducing the consequences of security breaches. This study suggests that businesses adopt a forward-thinking and adaptable approach to security management to stay ahead of the ever-changing cyber threat landscape. ----- The introduction describes how today's malware and exploits are increasingly sophisticated, challenging traditional security models as perimeter defenses become less effective. The threats exhibit three key characteristics: using social engineering to infiltrate infrastructure, remaining hidden for extended periods to monitor and gather data, and eventually exfiltrating information externally. Organizations face advanced, well-funded threats from nation-states or criminal enterprises that target human vulnerabilities, remain undetected, and exploit zero-day vulnerabilities. The authors argue that the rising sophistication requires a shift from traditional measures to dynamic strategies incorporating cutting-edge technologies and APT models. The study aims to develop a novel framework by characterizing threat models, identifying gaps in current controls, and introducing additional protective mechanisms to strengthen existing security approaches. ----- The conclusion emphasizes that the swiftly changing cyber threat landscape, characterized by APTs and stealth data exfiltration, calls for a substantial revamp of conventional enterprise security models. The MESA 2.0 security model embodies a proactive approach integrating prevention, detection, response, continuous learning, and security design updates. The paper stresses the importance of continuous security training, robust incident response plans, and ongoing adaptation of security controls. It advocates for transitioning from purely prevention-focused defenses to dynamic approaches that prepare for inevitable breaches, incorporating advanced technologies such as Machine Learning and AI. The study provides a strategic framework to steer future security practices, ensuring enterprises are better prepared to confront APTs while anticipating future challenges in the rapidly evolving digital landscape.}
}

@article{rastogi2021dante_insider,
	title={DANTE: Predicting Insider Threat using LSTM on system logs},
	author={Rastogi, Nidhi and Ma, Qicheng},
	journal={arXiv:2102.05600},
	year={2021},
	doi={10.48550/arXiv.2102.05600},
	note={Version 1, last revised 10 Feb 2021},
	abstract={Insider threat is one of the most pernicious threat vectors to information and communication technologies (ICT) across the world due to the elevated level of trust and access that an insider is afforded. This type of threat can stem from both malicious users with a motive as well as negligent users who inadvertently reveal details about trade secrets, company information, or even access information to malignant players. In this paper, we propose a novel approach that uses system logs to detect insider behavior using a special recurrent neural network (RNN) model. Ground truth is established using DANTE and used as the baseline for identifying anomalous behavior. For this, system logs are modeled as a natural language sequence and patterns are extracted from these sequences. We create workflows of sequences of actions that follow a natural language logic and control flow. These flows are assigned various categories of behaviors - malignant or benign. Any deviation from these sequences indicates the presence of a threat. We further classify threats into one of the five categories provided in the CERT insider threat dataset. Through experimental evaluation, we show that the proposed model can achieve 99\% prediction accuracy. ----- The introduction motivates insider threat detection by noting substantial federal spending on the issue and explaining that insiders already have access, motive, and means to sabotage systems, making firewalls and authentication ineffective against them. It highlights how COVID-19 worsened the situation by shifting to VPNs and remote work before security postures could adapt, increasing vulnerability to social engineering. The authors argue that system and network logs record detailed time-sequenced events that follow rules and control flows like structured natural language, making them suitable for detecting intrusions, abnormalities, and data theft without requiring additional data extraction. DANTE uses this natural language-based model with LSTM to learn long-range dependencies over log sequences, create workflow models for separate tasks, and detect anomalous patterns across five insider threat categories, achieving 93\% accuracy in initial experiments though limited to known anomaly types. ----- The conclusion acknowledges this is work in progress within a larger AI-based insider threat detection research roadmap, presenting DANTE's exploratory approach of treating log sequences as natural language due to their inherent control logic and flow. It identifies several challenges: anomaly detection's reliance on fully modeled baselines causing high false positives, the need for clustering techniques without a-priori knowledge of benign behavior, handling loops where the same key-value reoccurs, and log granularity issues that may overlook new patterns from users with new roles. Additional challenges include assumptions of unchanged source code for log generation, single-user operation (not considering collusion), and the possibility of insiders poisoning both models and training data. Future improvements include online feedback mechanisms to reduce false positives and negatives.}
}

@article{boffa2024logprecis,
	title={LogPr\'{e}cis: Unleashing Language Models for Automated Malicious Log Analysis},
	author={Boffa, Matteo and Valentim, Rodolfo Vieira and Vassio, Luca and Giordano, Danilo and Drago, Idilio and Mellia, Marco and Ben Houidi, Zied},
	journal={Computers \& Security},
	year={2024},
	pages={103805},
	doi={10.1016/j.cose.2024.103805},
	note={arXiv:2307.08309v3, last revised 22 Mar 2024},
	abstract={The collection of security-related logs holds the key to understanding attack behaviors and diagnosing vulnerabilities. Still, their analysis remains a daunting challenge. Recently, Language Models (LMs) have demonstrated unmatched potential in understanding natural and programming languages. The question arises whether and how LMs could be also useful for security experts since their logs contain intrinsically confused and obfuscated information. In this paper, we systematically study how to benefit from the state-of-the-art in LM to automatically analyze text-like Unix shell attack logs. We present a thorough design methodology that leads to LogPr\'{e}cis. It receives as input raw shell sessions and automatically identifies and assigns the attacker tactic to each portion of the session, i.e., unveiling the sequence of the attacker's goals. We demonstrate LogPr\'{e}cis capability to support the analysis of two large datasets containing about 400,000 unique Unix shell attacks. LogPr\'{e}cis reduces them into about 3,000 fingerprints, each grouping sessions with the same sequence of tactics. The abstraction it provides lets the analyst better understand attacks, identify fingerprints, detect novelty, link similar attacks, and track families and mutations. Overall, LogPr\'{e}cis, released as open source, paves the way for better and more responsive defense against cyberattacks. ----- The introduction motivates log analysis as a critical but daunting challenge for security analysts, noting that while data collection can be automated, parsing unclear and malformed logs remains time-consuming and error-prone. It highlights how attackers employ evasion tactics to confuse conventional pattern-matching and blocklisting measures, making static rules expensive to maintain. The authors investigate whether PLMs pre-trained on natural language and legitimate code can be successfully applied to malicious logs, whether fine-tuning brings benefits or degrades original knowledge, and whether smaller models can match large costly GPT-family models. They propose LogPr\'{e}cis to map raw shell scripts into intermediate representations using MITRE ATT\&CK Tactics, fine-tuning PLMs with only 360 labelled sessions to generate attack fingerprints that reduce nearly 400,000 unique scripts to fewer than 3,000 distinct fingerprints, aiding forensic analysis, novelty detection, and attack family tracking. ----- The conclusion presents LogPr\'{e}cis as a novel tool leveraging PLMs for automated Unix shell log analysis, mapping raw scripts into intermediate representations that encapsulate attacker goals and enabling powerful threat detection and attack understanding. It notes that fingerprints reduce thousands of unique samples into tens of new fingerprints per day, streamlining forensic analysis and uncovering evolving attack patterns. However, limitations remain: identical fingerprints for intrinsically different attacks can lead to misclassification, especially if adversaries deliberately mimic fingerprints to bypass the system. Future work will address this by considering internal model representations or more expressive classes than MITRE tactics, and the methodology can be extended to other log types since few-shot tuning requires only minimal labelled samples.}
}

@article{koisser2023accountability_iot_logging,
	title={Accountability of Things: Large-Scale Tamper-Evident Logging for Smart Devices},
	author={Koisser, David and Sadeghi, Ahmad-Reza},
	journal={arXiv:2308.05557},
	year={2023},
	doi={10.48550/arXiv.2308.05557},
	note={Version 1, last revised 10 Aug 2023},
	abstract={Our modern world relies on a growing number of interconnected and interacting devices, leading to a plethora of logs establishing audit trails for all kinds of events. Simultaneously, logs become increasingly important for forensic investigations, and thus, an adversary will aim to alter logs to avoid culpability, e.g., by compromising devices that generate and store logs. Thus, it is essential to ensure that no one can tamper with any logs without going undetected. However, existing approaches to establish tamper evidence of logs do not scale and cannot protect the increasingly large number of devices found today, as they impose large storage or network overheads. Additionally, most schemes do not provide an efficient mechanism to prove that individual events have been logged to establish accountability when different devices interact. This paper introduces a novel scheme for practical large-scale tamper-evident logging with the help of a trusted third party. To achieve this, we present a new binary hash tree construction designed around timestamps to achieve constant storage overhead with a configured temporal resolution. Additionally, our design enables the efficient construction of shareable proofs, proving that an event was indeed logged. Our evaluation shows that—using practical parameters—our scheme can localize any tampering of logs with a sub-second resolution, with a constant overhead of ~8KB per hour per device. ----- The introduction addresses the growing problem of secure logging in smart device ecosystems and IoT systems that generate massive volumes of logs essential for forensic analysis, accountability attribution, and event reconstruction. It describes how adversaries who compromise devices can modify, delete, or inject logs to evade investigations, highlighting limitations of existing approaches (remote logging, blockchain, dedicated hardware, hash structures, forward-secure signatures) regarding scalability, overhead, and support for log receipt proofs. The section then defines the system and adversary models, and synthesizes solution requirements: detecting tampering with configurable temporal resolution, generating efficient receipts for individual events, and maintaining minimal device overhead by delegating cryptographic operations to a trusted log notary. ----- The conclusion reiterates that the proposed scheme, based on a novel temporal hash tree (PITS) and log notary, provides scalable tamper-evident logging with constant storage overhead and fine-grained tampering localization. It summarizes experimental results showing approximately 8 KB/hour per device consumption and a single server's capacity to handle hundreds of thousands of events per second, thereby satisfying scalability requirements for smart device fleets. The authors discuss how forward integrity properties and log receipts create strong constraints on attackers from a forensics perspective, and indicate future work directions such as applying the model to new IoT scenarios and optimizing parameters for different risk profiles.}
}

@article{aghili2024protecting_privacy_logs,
	title={Protecting Privacy in Software Logs: What Should Be Anonymized?},
	author={Aghili, Roozbeh and Li, Heng and Khomh, Foutse},
	journal={arXiv:2409.11313},
	year={2024},
	doi={10.48550/arXiv.2409.11313},
	note={Version 2, last revised 7 May 2025},
	abstract={Software logs, generated during the runtime of software systems, are essential for various development and analysis activities, such as anomaly detection and failure diagnosis. However, the presence of sensitive information in these logs poses significant privacy concerns, particularly regarding Personally Identifiable Information (PII) and quasi-identifiers that could lead to re-identification risks. While general data privacy has been extensively studied, the specific domain of privacy in software logs remains underexplored, with inconsistent definitions of sensitivity and a lack of standardized guidelines for anonymization. To mitigate this gap, this study offers a comprehensive analysis of privacy in software logs from multiple perspectives. We start by performing an analysis of 25 publicly available log datasets to identify potentially sensitive attributes. Based on the result of this step, we focus on three perspectives: privacy regulations, research literature, and industry practices. We first analyze key data privacy regulations, such as the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA), to understand the legal requirements concerning sensitive information in logs. Second, we conduct a systematic literature review to identify common privacy attributes and practices in log anonymization, revealing gaps in existing approaches. Finally, we survey 45 industry professionals to capture practical insights on log anonymization practices. Our findings shed light on various perspectives of log privacy and reveal industry challenges, such as technical and efficiency issues while highlighting the need for standardized guidelines. By combining insights from regulatory, academic, and industry perspectives, our study aims to provide a clearer framework for identifying and protecting sensitive information in software logs. ----- The introduction frames software logs as a central resource for maintenance, failure diagnosis, and anomaly detection, yet emphasizes that the presence of PII and quasi-identifiers introduces serious re-identification and data leakage risks. The authors show that, unlike privacy in general data contexts, privacy in logs is understudied: there are no shared definitions of which attributes should be considered sensitive and no standardized guidelines for anonymization exist, while organizations are often reluctant to share datasets due to these concerns. The introduction thus motivates a systematic study combining analysis of 25 log datasets, mapping of regulations (GDPR, CCPA, HIPAA, etc.), review of log anonymization literature, and industry survey, structuring the work around four research questions on: most common attributes, what regulations consider sensitive, what emerges from research literature, and which attributes/practices professionals deem critical. ----- The conclusion summarizes that the joint analysis of real datasets, regulations, literature, and surveys has enabled a much clearer picture of which log attributes should be treated as sensitive and reveals that current practices are fragmented. The authors highlight convergences (e.g., importance of IP addresses, timestamps, identifiers, and paths) but also gaps, such as underestimation of certain configuration or context fields, and report that professionals face technical and performance challenges when anonymizing while maintaining log utility. The conclusion proposes a series of recommendations and attribute categories that can serve as a foundation for standardized guidelines, indicating future work on developing tools and benchmarks that implement and evaluate such guidelines in industrial scenarios.}
}

@article{jena2012audit_log_hash,
	title={Ensuring Audit Log Accountability through Hash Based Techniques},
	author={Jena, Rashmita and Aparna, M. and Sahu, Chinmaya and Ranjan, Rajeev and Atmakuri, Rajesh},
	journal={International Journal of Future Computer and Communication},
	volume={1},
	number={4},
	pages={327--329},
	year={2012},
	doi={10.7763/IJFCC.2012.V1.88},
	abstract={Audit logs are now considered good practice and a standard approach for business systems. The integrity of the auditing records themselves is critical. By simply storing all the interactions in a separate audit log does not guarantee the integrity of the log. Data tampering can be done through unauthorized access and in some cases through authorized users. Results of such action can be unpleasant for business and their clients. Therefore, a demand for audit log security is needed more than ever. This paper describes a mechanism based on cryptographic hash functions and trusted time stamping that prevents an outsider or inside intruder from silently corrupting the audit log. In addition it is shown that the proposed mechanism can be realized in database systems with little overhead and that the hash based techniques and trusted time stamping can be used efficiently and correctly to determine if the audit log has been compromised. ----- The introduction emphasizes that data security encompasses three interrelated areas—secrecy, integrity, and availability—with data tampering posing distinct threats from both unauthorized and authorized users. Through real-world examples (fraudulent transaction dating in sales companies, grade falsification in school databases), the section illustrates why audit logs are vulnerable and why their integrity must be protected. It highlights the gap between current regulatory requirements for long-term document retention and the inadequacy of existing commercial forensics tools, which cannot detect tampering by intruders with full access to audit logs. The introduction positions trusted timestamping and cryptographic hash functions as essential techniques for detecting unauthorized modifications and establishes the need for a low-overhead mechanism to certify audit log integrity in database systems. ----- The conclusion summarizes the paper's contributions to transaction processing systems for effective and efficient tampering detection based on cryptographic techniques (strong hashing, partial result authentication codes, and offsite digital timestamping). Key achievements include demonstrating how hash-based techniques enable forensic analysis transparent to applications, how trusted timestamping prevents corruption by insiders or outsiders, and how linked hashing minimizes interaction costs with timestamping authorities. The experimental evaluation shows hashing overhead never exceeds 15%, with projected overhead below 10% in commercial DBMS implementations. The conclusion identifies future directions: determining optimal timestamping service granularity, leveraging tamper detection mechanisms to produce tamper-resistant audit logs, and developing forensic analysis algorithms to determine the who, when, what, and where components of detected tampering.}
}

@article{karlsen2023benchmarking_llm_log,
	title={Benchmarking Large Language Models for Log Analysis, Security, and Interpretation},
	author={Karlsen, Egil and Luo, Xiao and Zincir-Heywood, Nur and Heywood, Malcolm},
	journal={arXiv:2311.14519},
	year={2023},
	doi={10.48550/arXiv.2311.14519},
	note={Version 1, last revised 24 Nov 2023},
	abstract={This paper benchmarks five large language model architectures (BERT, RoBERTa, DistilRoBERTa, GPT-2, and GPT-Neo) for security-oriented log analysis across six labeled application and system log datasets using a unified sequence-classification setup and the LLM4Sec experimentation pipeline. By fine-tuning 60 models, the authors show that LLM-based semantic feature extraction combined with domain adaptation can achieve near-perfect F1-scores for intrusion detection and substantially outperform prior log-analysis baselines, while also delivering a reusable framework for experimentation, evaluation, and analysis. ----- The introduction describes how modern enterprise infrastructures generate massive, heterogeneous logs that are central to monitoring and forensics but hard to exploit with traditional pipelines that depend on static parsers such as Drain and manually engineered features. It summarizes prior NLP and deep-learning approaches for log analysis, including word2vec+TF-IDF and earlier LLM-based embedding methods, and argues that they often target narrow datasets or require brittle pre-processing, which limits generality and adaptability to new log sources. Motivated by these gaps, the authors propose to systematically explore LLM-based feature extraction for both application and system logs across multiple datasets and architectures, while also studying domain adaptation via fine-tuning, interpretability via t-SNE and SHAP, and an end-to-end pipeline (LLM4Sec) for reproducible benchmarking. ----- The conclusion reports that fine-tuned sequence-classification models based on the evaluated LLMs consistently outperform previous log-analysis techniques on all six datasets, with DistilRoBERTa in particular delivering very high F1-scores and low false-positive rates. It highlights that domain adaptation through fine-tuning is crucial for enabling LLMs trained on generic text to correctly weight log components, and that visualization tools such as t-SNE and SHAP make the resulting anomaly detectors more interpretable for security analysts by exposing which tokens drive NORMAL versus ANOMALOUS decisions. The authors emphasize that the LLM4Sec pipeline supports reproducible experiments and future extensions, and suggest directions such as alternative tokenization schemes, further pre-training, and ultimately training smaller, security-specific LLMs on diverse log corpora to improve efficiency.}
}



@inproceedings{shinde2018review_ml_dl,
	title={A review of machine learning and deep learning applications},
	author={Shinde, Pramila P and Shah, Seema},
	booktitle={2018 Fourth International Conference on Computing Communication Control and Automation (ICCUBEA)},
	year={2018},
	organization={IEEE},
	doi={10.1109/ICCUBEA.2018.8697857},
	abstract={Machine learning is one of the fields in the modern computing world. A plenty of research has been undertaken to make machines intelligent. Learning is a natural human behavior which has been made an essential aspect of the machines as well. There are various techniques devised for the same. Traditional machine learning algorithms have been applied in many application areas. Researchers have put many efforts to improve the accuracy of that machinelearning algorithms. Another dimension was given thought which leads to deep learning concept. Deep learning is a subset of machine learning. So far few applications of deep learning have been explored. This is definitely going to cater to solving issues in several new application domains, sub-domains using deep learning. A review of these past and future application domains, sub-domains, and applications of machine learning and deep learning are illustrated in this paper.}
}

@inproceedings{ma2024llmparser,
	title={LLMParser: An Exploratory Study on Using Large Language Models for Log Parsing},
	author={Ma, Zeyang and Chen, An Ran and Kim, Dong Jae and Chen, Tse-Hsun and Wang, Shaowei},
	booktitle={Proceedings of the 46th International Conference on Software Engineering},
	year={2024},
	publisher={ACM},
	doi={10.48550/arXiv.2404.18001},
	abstract={Logs are important in modern software development with runtime information. Log parsing is the first step in many log-based analyses, that involve extracting structured information from unstructured log data. Traditional log parsers face challenges in accurately parsing logs due to the diversity of log formats, which directly impacts the performance of downstream log-analysis tasks. In this paper, we explore the potential of using Large Language Models (LLMs) for log parsing and propose LLMParser, an LLM-based log parser based on generative LLMs and few-shot tuning. We leverage four LLMs, Flan-T5-small, Flan-T5-base, LLaMA-7B, and ChatGLM-6B in LLMParsers. Our evaluation of 16 open-source systems shows that LLMParser achieves statistically significantly higher parsing accuracy than state-of-the-art parsers (a 96% average parsing accuracy). We further conduct a comprehensive empirical analysis on the effect of training size, model size, and pre-training LLM on log parsing accuracy. We find that smaller LLMs may be more effective than more complex LLMs; for instance where Flan-T5-base achieves comparable results as LLaMA-7B with a shorter inference time. We also find that using LLMs pre-trained using logs from other systems does not always improve parsing accuracy. While using pre-trained Flan-T5-base shows an improvement in accuracy, pre-trained LLaMA results in a decrease (decrease by almost 55% in group accuracy). In short, our study provides empirical evidence for using LLMs for log parsing and highlights the limitations and future research direction of LLM-based log parsers. ----- The introduction situates log parsing as an essential but challenging first step for large-scale log analysis, explaining how massive and heterogeneous logs overwhelm manual inspection and how existing parsers based on pattern mining, clustering, and parsing trees often miss variables and hurt downstream anomaly detection and diagnosis. It motivates viewing log parsing as a translation task from raw logs to log templates and argues that generative LLMs can better capture both natural-language text and code-like tokens. The authors introduce LLMParser, a family of few-shot tuned parsers built on four open LLMs, and outline research questions on how model type, size, shot size, and pre-training settings affect parsing accuracy and efficiency compared to state-of-the-art tools. ----- The conclusion reports that LLMParser consistently achieves higher parsing accuracy than traditional log parsers and previous LLM-based approaches while remaining practical to fine-tune with only tens of labeled examples per system. It emphasizes that few-shot tuning is more effective and efficient than in-context learning, that training data diversity matters more than simply increasing shot size, and that medium-size models such as Flan-T5-base can rival much larger ones, whereas pre-training on logs from other systems does not always help. The authors highlight remaining challenges around unseen log templates and hard-to-identify variable types, and call for future work on better sampling strategies, log-tailored LLM architectures, and pre-training schemes that further improve generalization without sacrificing practicality.}
}

@inproceedings{zhu2024can_llm_context,
	title={Can Large Language Models Understand Context?},
	author={Zhu, Yilun and Moniz, Joel Ruben Antony and Bhargava, Shruti and Lu, Jiarui and Piraviperumal, Dhivya and Li, Site and Zhang, Yuan and Yu, Hong and Tseng, Bo-Hsiang},
	booktitle={Findings of the Association for Computational Linguistics: EACL 2024},
	pages={2004--2018},
	year={2024},
	publisher={Association for Computational Linguistics},
	address={St. Julian's, Malta},
	url={https://aclanthology.org/2024.findings-eacl.135/},
	abstract={Understanding context is key to understanding human language, an ability which Large Language Models (LLMs) have been increasingly seen to demonstrate to an impressive extent. However, though the evaluation of LLMs encompasses various domains within the realm of Natural Language Processing, limited attention has been paid to probing their linguistic capability of understanding contextual features. This paper introduces a context understanding benchmark by adapting existing datasets to suit the evaluation of generative models. This benchmark comprises of four distinct tasks and nine datasets, all featuring prompts designed to assess the models’ ability to understand context. First, we evaluate the performance of LLMs under the in-context learning pretraining scenario. Experimental results indicate that pre-trained dense models struggle with understanding more nuanced contextual features when compared to state-of-the-art fine-tuned models. Second, as LLM compression holds growing significance in both research and real-world applications, we assess the context understanding of quantized models under in-context-learning settings. We find that 3-bit post-training quantization leads to varying degrees of performance reduction on our benchmark. We conduct an extensive analysis of these scenarios to substantiate our experimental results. ----- The introduction frames discourse understanding as going beyond single sentences to phenomena like coreference, discourse relations, dialogue state, and ellipsis, and argues that current LLM benchmarks rarely include discourse-focused datasets, so they fail to truly test contextual linguistic competence. It highlights that despite many benchmarks for commonsense, sentiment, NLI, and other tasks, none are tailored to nuanced context understanding, and that large LLMs raise deployment and compression concerns whose impact on contextual ability is poorly studied. Motivated by these gaps, the authors propose a context understanding benchmark built from multiple discourse datasets, use prompts for in-context learning, and systematically evaluate dense and post-training-quantized LLMs of different sizes. ----- The conclusion summarizes that the proposed benchmark, spanning four tasks and nine adapted datasets, systematically probes document- and dialogue-level context understanding for generative LLMs. It reports that under in-context learning, LLMs struggle with fine-grained linguistic features on this benchmark and can behave inconsistently with their performance on more general benchmarks, and that 3-bit post-training quantization further degrades contextual understanding to varying degrees across tasks. The authors position their benchmark and dense-vs-quantized comparison as a new lens on the contextual dimension of language understanding and a valuable complement to existing LLM evaluations, encouraging future work to study where and why models’ context competence breaks down.}
}

@inproceedings{dai2022revisiting_long_doc,
	title={Revisiting Transformer-based Models for Long Document Classification},
	author={Dai, Xiang and Chalkidis, Ilias and Darkner, Sune and Elliott, Desmond},
	booktitle={Findings of the Association for Computational Linguistics: EMNLP 2022},
	year={2022},
	publisher={Association for Computational Linguistics},
	doi={10.48550/arXiv.2204.06683},
	url={https://arxiv.org/abs/2204.06683},
	abstract={Recent work on text classification has largely focused on short sequences, while many real-world applications involve multi-page, multi-paragraph documents that vanilla Transformers cannot process efficiently due to their quadratic self-attention cost and limited pre-training context. This paper revisits Transformer-based long document classification by comparing sparse-attention models such as Longformer and hierarchical Transformers under a range of design choices (e.g., attention window size, number of globally attended tokens, and document splitting strategies) on four datasets from different domains, showing that encoding substantially more tokens brings clear performance gains and enabling practical guidance for applying Transformers to long-document tasks. ----- The introduction contrasts typical short-text benchmarks with genuinely long-document settings, arguing that truncating documents to 512 tokens often discards crucial information and explaining why naive application of pre-trained BERT-style encoders is inadequate. It reviews the computational challenges of standard self-attention, motivates long-document architectures like sparse-attention and hierarchical models, and notes that prior evaluations have relied on datasets where most texts are still relatively short or where Transformers underperform CNN/RNN baselines on truly long clinical notes such as those in MIMIC-III. Building on this, the authors aim to transfer the success of the pre-train--fine-tune paradigm to long documents by systematically comparing sparse and hierarchical Transformer approaches and studying how architectural and training choices affect both effectiveness and efficiency. ----- The conclusion shows that when they are allowed to process longer inputs, Transformer-based models can match or surpass CNN-based baselines for long document classification, rebutting earlier criticisms of their suitability for this setting. Experiments on MIMIC-III, ECtHR, 20News, and Hyperpartisan reveal that Longformer with up to 4096 tokens already delivers competitive performance and that moderate local attention windows plus a small number of global tokens offer a good trade-off between accuracy and efficiency. Hierarchical Transformers achieve the strongest overall results, with the most important design factor being how documents are split into segments; the authors find that relatively short, overlapping segments (around 128 tokens) often work well, though optimal lengths remain dataset-dependent. Overall, the paper provides concrete recommendations for configuring long-document Transformers and highlights that carefully designed architectures and input partitioning are key to unlocking their full potential.}
}

@inproceedings{liu2020apt_cloud_forensics,
	title={Forensic Analysis of Advanced Persistent Threat Attacks in Cloud Environments},
	author={Liu, Changwei and Singhal, Anoop and Wijesekera, Duminda},
	booktitle={Advances in Digital Forensics XVI: 16th IFIP WG 11.9 International Conference},
	pages={161--180},
	year={2020},
	publisher={Springer},
	address={New Delhi, India},
	doi={10.1007/978-3-030-56223-6_9},
	abstract={Cloud forensic investigations involve large volumes of diverse devices and data. Investigations involving advanced persistent threat attacks involve filtering noisy data and using expert knowledge to identify the missing steps in the attacks that typically have long time spans. Under such circumstances, obtaining timely and credible forensic results is a challenge. This chapter engages a case study to demonstrate how MITRE's ATT\&CK knowledge base and Lockheed Martin's Cyber Kill Chain methodology can be used in conjunction to perform forensic analyses of advanced persistent threat attacks in cloud environments. ATT\&CK is a globally-accessible knowledge base of adversary tactics and techniques developed from real-world observations of attacks. The Cyber Kill Chain methodology describes a series of steps that trace a cyber attack from its early reconnaissance stage to the later data exfiltration stage. Because advanced persistent threat attacks on cloud systems involve the key Cyber Kill Chain phases of reconnaissance, command and control communications, privilege escalation, lateral movement through a network and exfiltration of confidential information, it is beneficial to combine the ATT\&CK knowledge base and Cyber Kill Chain methodology to identify and aggregate evidence, and automate the construction of the attack steps. ----- The introduction defines digital forensics as applying scientific methodologies to identify, collect, examine and analyze evidentiary data while preserving integrity and chain of custody. It notes that cloud forensic investigations have expanded in scope due to unchecked attack surfaces and diverse data, with hypervisors lacking forensic tools and current techniques not designed for cloud environments. For APT attacks spanning long periods, timestamps from different sources may not indicate a single attack, requiring filtering noisy data and expert speculation about attack steps. Prior research proposes collecting evidence from hypervisors and VMs or using graphical frameworks to reconstruct attack scenarios, but assumes forensic data can be manually aggregated and investigators can construct steps when evidence is incomplete. The main contribution is combining ATT\&CK and Cyber Kill Chain frameworks---previously used independently---to identify and correlate attack evidence, using sample APT attacks on an experimental cloud environment processed by a Prolog-based forensic tool to automatically construct attack steps. ----- The conclusion notes that justifying pre-attack, attack and post-attack phases requires evidence of activities related to those phases, and constructing APT attack steps is difficult because timestamps cannot serve as reliable indicators and recognizing phases may require statistical correlation across multiple sources. The ATT\&CK knowledge base is readily leveraged to identify evidence and build attack steps by mapping available evidence to Cyber Kill Chain phases. The experimental cloud case study validates the benefits of combining both frameworks to identify and aggregate evidence, feeding it to a Prolog-based tool for automated attack step construction. Future research will extend the relationships between the Cyber Kill Chain and evidence gathering and attack-attribution tasks.}
}

@inproceedings{paccagnella2020custos,
	title={Custos: Practical Tamper-Evident Auditing of Operating Systems Using Trusted Execution},
	author={Paccagnella, Riccardo and Datta, Pubali and Hassan, Wajih Ul and Bates, Adam and Fletcher, Christopher W. and Miller, Andrew and Tian, Dave},
	booktitle={Network and Distributed Systems Security (NDSS) Symposium 2020},
	year={2020},
	publisher={Internet Society},
	address={San Diego, CA, USA},
	doi={10.14722/ndss.2020.24065},
	abstract={System auditing is a central concern when investigating and responding to security incidents. Unfortunately, attackers regularly engage in anti-forensic activities after a break-in, covering their tracks from the system logs in order to frustrate the efforts of investigators. While a variety of tamper-evident logging solutions have appeared throughout the industry and the literature, these techniques do not meet the operational and scalability requirements of system-layer audit frameworks. In this work, we introduce Custos, a practical framework for the detection of tampering in system logs. Custos consists of a tamper-evident logging layer and a decentralized auditing protocol. The former enables the verification of log integrity with minimal changes to the underlying logging framework, while the latter enables near real-time detection of log integrity violations within an enterprise-class network. Custos is made practical by the observation that we can decouple the costs of cryptographic log commitments from the act of creating and storing log events, without trading off security, leveraging features of off-the-shelf trusted execution environments. Supporting over one million events per second, we show that Custos' tamper-evident logging protocol is three orders of magnitude (1000x) faster than prior solutions and incurs only between 2% and 7% runtime overhead over insecure logging on intensive workloads. Further, we show that Custos' auditing protocol can detect violations in near real-time even in the presence of a powerful distributed adversary and with minimal (3%) network overhead. Our case study on a real-world APT attack scenario demonstrates that Custos forces anti-forensic attackers into a "lose-lose" situation, where they can either be covert and not tamper with logs (which can be used for forensics), or erase logs but then be detected by Custos. ----- The introduction establishes that system auditing is essential for incident investigation and response, as logs serve as the definitive ground truth for understanding system activities, post-mortem attack reconstruction, access control, execution integrity, and intrusion detection. However, attackers understand the forensic value of logs and actively engage in anti-forensic countermeasures to erase, edit, or insert events to confuse investigators—a practice so prevalent that 72% of incident response specialists report encountering tampered logs. The introduction critiques existing solutions: commodity operating systems provide no special protections (root access is often sufficient for covert log manipulation), commercial WORM devices and remote servers are costly and impractical, and cryptographic approaches incur excessive computational and storage overhead while being too slow for modern operating systems generating hundreds of thousands of system calls per second. The section concludes by positioning Custos as a practical solution that must be scalable, efficient, and minimally invasive to existing audit frameworks. ----- The conclusion reiterates that despite logs' centrality in responding to modern security incidents like Advanced Persistent Threats, commodity operating systems fail to assure log integrity beyond basic access controls. Custos is presented as the first tamper-evident logging solution meeting practical operating system constraints by decoupling event logging from cryptographic commitment without sacrificing security, leveraging readily available Trusted Execution Environments (TEEs). The conclusion summarizes key achievements: the log commitment protocol is three orders of magnitude faster than prior secure logging solutions, incurring only 2-7% runtime overhead over insecure logging, while the auditing protocol detects integrity violations with less than 3% network overhead. The authors conclude that Custos demonstrates a realistic and practical path forward for achieving tamper-evident auditing of operating systems.}
}

@inproceedings{hou2025catamaran,
	title={Catamaran: User Privacy Violation Detection in Mobile Logging},
	author={Hou, Chenxi and Chong, Chun Jie and Yao, Zhihao and Peng, Hui},
	booktitle={2025 IEEE Secure Development Conference (SecDev)},
	year={2025},
	publisher={IEEE},
	address={Indianapolis, IN, USA},
	doi={10.1109/SecDev66745.2025.00013},
	note={Date of Conference: 14-16 October 2025. Date Added to IEEE Xplore: 12 November 2025},
	abstract={Logs are widely used in mobile apps for debugging and diagnosis. Unfortunately, they frequently expose personally identifiable information (PII) due to inattentive logging practices, leading to privacy hazards, which has been categorized as a Common Weakness Enumeration, CWE-532. Existing manual redaction and anonymization techniques are often incomplete and ineffective, as evidenced by the rising number of CWE-532 vulnerabilities. To address this, we introduce Catamaran, a framework to proactively detect PII violations in Android app logs. Catamaran takes a comprehensive approach combining dynamic and static analyses: dynamic analysis captures PII leaks directly at runtime in logcat and other log files, while static analysis expands detection to untriggered code paths by reconstructing contextualized log statements. The static analysis leverages call graph analysis, constant propagation, and Large Language Model (LLM) to intelligently identify potential PII issues. Our extensive evaluation on 3,885 Android apps uncovered runtime PII leakage in 233 apps (approximately 6%), comprising a total of 7,485 violations. The static analysis of 300,073 log statements across 3,994 apps identified 1,788 potential log-based PII leaks in 932 apps and 97 distinct libraries. We have responsibly disclosed our findings, leading to the confirmation of nine vulnerabilities and four patches in the Android Open Source Project (AOSP). ----- The introduction addresses the critical problem of PII exposure in mobile app logs, a widespread issue in Android development where logging frameworks (logcat) create shared system buffers accessible to malicious apps and connected devices. The paper demonstrates that 81% of pre-installed mobile apps read system-wide logs, and despite Google's recommendations against including PII in logs, developers continue to expose sensitive information such as email addresses, device identifiers, and login credentials. The introduction emphasizes that previous research focused primarily on logcat via static or dynamic analysis but overlooked app-specific log files and library code, leaving significant gaps in coverage. It motivates the need for a comprehensive framework integrated into the development and deployment cycle capable of detecting PII in all logging channels and attributing leakage sources to either app or library code. ----- The conclusion summarizes that Catamaran successfully combines dynamic and static analysis to detect log information disclosure vulnerabilities across both logcat and log files in Android apps. The dynamic study identified 233 apps with PII disclosures among 3,885 applications (including 3,864 third-party apps and 15 platform apps), while static analysis of 3,994 apps revealed 932 apps and 97 libraries logging PII to logcat. The work demonstrates the effectiveness of using LLM in conjunction with call graph and constant propagation analysis to reconstruct contextualized log statements and identify potential PII leaks in untriggered code paths. The authors highlight the practical impact of their responsible disclosure process, which resulted in nine confirmed vulnerabilities and four patches merged into AOSP, underscoring the real-world significance of detecting such privacy violations in mobile logging infrastructure.}
}



@misc{ibm_moe,
  author = {Bergmann, Dave},
	title = {What is mixture of experts?},
	howpublished = {\url{https://www.ibm.com/think/topics/mixture-of-experts}},
	note = {Accessed: 2025-06-01}
}

@misc{mistral_moe,
  author = {Mistral AI team},
	title = {Mixtral of experts: A high quality Sparse Mixture-of-Experts},
	howpublished = {\url{https://mistral.ai/news/mixtral-of-experts}},
	note = {Accessed: 2025-06-01}
}



%%%%%%%%%% Esempi %%%%%%%%%%
@article{esempio_articolo,
	title={A multi-layered, time-based music description approach based on XML},
	author={Haus, Goffredo and Longari, Maurizio},
	journal={Computer Music Journal},
	volume={29},
	number={1},
	year={2005},
	publisher={MIT Press}
}

@book{esempio_libro,
  title = {Research methods in {Human-Computer Interaction}},
  author = {Lazar, Jonathan and Feng, Jijuan Heidi and Hochheiser, Harry},
  publisher = {Morgan Kaufmann},
  year = {2017},
  edition = {Second},
  address = {Cambridge}
}

@misc{esempio_sito,
  author = {W. H\"am\"al\"ainen},
	title = {Scientific Writing for Computer Science Students},
	howpublished = {\url{http://www.cs.joensuu.fi/\~whamalai/teaching.html}},
	note = {Accessed: 2019-04-10}
}


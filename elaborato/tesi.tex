%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                             %
%           TEMPLATE LATEX PER TESI                                           %
%           ______________                                                    %
%                                                                             %
%           Ultima revisione: 28 Novembre 2024                                %
%           Revisori: G.Presti; L.A.Ludovico; F. Avanzini; M. Tiraboschi      %
%                                                                             %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[12pt]{report}

% --- PREAMBOLO ---------------------------------------------------------------
% Inserire qui eventuali package da includere o
% definizioni di comandi personalizzati

% Selezione lingua
\usepackage[italian]{babel}

\usepackage{tesi}
% Puoi usare il font di default di LaTeX con la relativa opzione del package
% \usepackage[defaultfont]{tesi}
% Esiste anche un'opzione per il formato 17x24 per le tesi di dottorato
% \usepackage[phd]{tesi}

% Mostra i bounding box per visualizzare il layout (TODO rimuovere per versione finale)
\geometry{showframe}

% Per disabilitare i todo:
% \usepackage[disable]{todonotes}
% \usepackage{todonotes}

% Inclusione comandi personalizzati per TODO automatici
\usepackage{comandi-todo}

% Pacchetto per grafici con dati integrati
\usepackage{pgfplots}
\pgfplotsset{compat=1.18} % Compatibilità con la versione 1.18 di pgfplots

% In caso il copia-incolla del PDF generato perda gli spazi,
% provare a decommentare la seguente riga
% \pdfinterwordspaceon

% !!! INFORMAZIONI SULLA TESI DA COMPILARE !!!

%   UNIVERSITA' E CORSO DI LAUREA:
\university{Università degli Studi di Milano}
\unilogo{immagini/loghi/unimi}
\faculty{Facoltà di Scienze e Tecnologie}
\department{Dipartimento di Informatica\\Giovanni Degli Antoni}
\cdl{Corso di Laurea Triennale in\\Sicurezza dei Sistemi e delle Reti Informatiche}

%   TITOLO TESI:
\title{Tesi}
% Questo comando (opzionale) sovrascrive \title per quanto riguarda la copertina
% Può essere usato per stampare caratteri speciali, tenendo i metadati puliti
\printedtitle{Titolo tesi}

%   AUTORE:
\author{Emanuele Magon}
\matricola{909482}
% "Elaborato Finale" per i CdL triennali
% "Tesi di Laurea" per i CdL magistrali
\typeofthesis{Elaborato Finale}

%   RELATORE E CORRELATORE:
\relatore{Prof. Marco Anisetti}
\correlatore{Antongiacomo Polimeno}

%   LABORATORIO:
% Questa sezione crea una pagina di chiusura della tesi con
% il logo dell'ente/laboratorio presso cui si è svolto il tirocinio.
% Più afferenze/url/loghi sono supportate,
% e la frase può essere personalizzata.
% Qui trovate alcuni predefiniti del nostro dipartimento
% \adaptlab
% \aislab
% \anacletolab
% \bisplab
% \connetslab
% \everywarelab
% \falselab
% \iebilab
% \islab
% \lailalab
% \lalalab
% \lawlab
% \laserlab
% \limlab
% \mipslab
% \optlab
% \phuselab
% \ponglab
% \sesarlab
% \spdplab

% Esempio di personalizzazione della pagina di chiusura
% (non consegnate con questo esempio!)
% (da commentare in caso sia sufficiente una delle macro precedenti)
% \lab{Laboratorio di Ricerca}
% \lab[in collaborazione con l']{Azienda Specifica}
% \laburl{https://di.unimi.it/it/ricerca/risorse-e-luoghi-della-ricerca/laboratori-di-ricerca}
% \lablogo{immagini/redqmark}

% Con questo comando si può cambiare la dimensione (massima
% altezza e larghezza consentite) dei loghi
% \setlength\lablogosize{25mm}

%   ANNO ACCADEMICO
% \the\year inserisce l'anno corrente
% per specificare manualmente un anno accademico
% NON inserire nel formato 1970-1971, ma
% inserire solo 1970
\academicyear{2024}

%   INDICI:
% elenco delle figure (facoltativo)
% \figurespagetrue
% elenco delle tabelle (facoltativo)
% \tablespagetrue
% prefazioni nell'indice (facoltativo)
% \prefaceintoctrue
% indice nell'indice (facoltativo)
% \tocintoctrue

\setlength {\marginparwidth }{2cm}

% Stile dei blocchi di codice
\usepackage{xcolor}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.97,0.97,0.95}

\lstdefinestyle{codelistingstyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\scriptsize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=t,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=codelistingstyle}

% Cambia il nome usato nelle caption del codice
\renewcommand{\lstlistingname}{Sorgente}
\renewcommand{\lstlistlistingname}{Elenco dei sorgenti}

% --- FINE PREAMBOLO ----------------------------------------------------------

\begin{document}

% Creazione automatica della copertina
% Centra la copertina nel foglio: usa questo comando per la copertina esterna
\makecenteredfrontpage
\todo{TODO titolo tesi}
\todo{TODO Antongiacomo è correlatore?}
% Copertina allineata alle altre pagine: usa questo comando per la copertina interna
% \makefrontpage

%
%			PAGINA DI DEDICA E/O CITAZIONE
%			facoltativa, questa è l'unica cosa che dovete formattare a mano, un po' come vi pare
%
\todo{TODO dediche}

{\raggedleft \large \sl Dedica 1\\

    \vspace{2cm}

    ``Citazione 1''

    \bigskip

    \--- Autore\\

    \vspace{2cm}

    ``Citazione 2''

    \bigskip

    \--- Autore\\}

\clearpage
\beforepreface

%
%			PREFAZIONE (facoltativa)
%

% \prefacesection{Prefazione}
% Le prefazioni non sono molto comuni, tuttavia a volte capita che qualcuno voglia dire qualcosa che esuli dal lavoro in sé (come un meta-commento sull'elaborato), o voglia fornire informazioni riguardanti l'eventuale progetto entro cui la tesi si colloca (in questo caso è probabile che sia il relatore a scrivere questa parte).

%
%			RINGRAZIAMENTI (facoltativi)
%

\todoprefacesection{Ringraziamenti}{ringraziamenti}
Questa sezione, facoltativa, contiene i ringraziamenti.

%
%			Creazione automatica dell'indice
%

\afterpreface



%
%			CAPITOLO: Introduzione o Abstract
%

\todochapter{Introduzione}{chap:introduzione}{aumentando piattaforme di microservizi e più importanza sulla sicurezza abbiamo fatto x y z nei capitoli a b c}

L'obiettivo di questo lavoro è sviluppare un sistema automatico per l'identificazione e la notificazione della presenza di dati sensibili all'interno di log di grandi dimensioni, fornendo agli amministratori di sistema una soluzione efficace per il monitoraggio della conformità alla privacy. \\
Sebbene il sistema sia progettato per essere generico e applicabile a diverse tipologie di log, il presente lavoro è stato sviluppato e testato specificamente per l'integrazione con una piattaforma di gestione \textit{smart city}.

Inizialmente, l'approccio progettuale prevedeva l'utilizzo esclusivo di analisi statica basata su espressioni regolari per l'identificazione dei dati sensibili, con il supporto di Large Language Models (LLM) limitato alla fase di sviluppo per la generazione di regex sempre più esaustive e precise. Una volta completato il progetto, il sistema avrebbe dovuto operare esclusivamente con le espressioni regolari generate, senza ulteriore dipendenza dagli LLM. Tuttavia, durante la fase di sviluppo e test, l'osservazione delle potenzialità degli LLM nell'analisi semantica del contenuto ha portato a una evoluzione dell'architettura del sistema.

La soluzione finale integra infatti un approccio ibrido che combina l'efficienza delle espressioni regolari per l'identificazione di dati sensibili strutturati (come codici fiscali, numeri di telefono, indirizzi email) con la capacità di comprensione contestuale degli LLM per il rilevamento di informazioni sensibili non strutturate o espresse in linguaggio naturale.



\todo{TODO inserire riassunto che spiega velocemente ogni capitolo cosa contiene}



%
%			CAPITOLO: Stato dell'arte
%

\todochapter{Stato dell'arte}{chap:stato_arte}{stato dell'arte}
\todo{TODO provare a chiedere a chatGPT aiuto su come cercare paper in google scholar e simili (che keyword usare ecc.)}

\begin{itemize}
    \item Log deformati, fare ricerca su soluzioni attuali su fonti autorevoli (paper con scholar.google.com). Se proprio non si trova si possono usare siti (se autoritevoli).
    \item Parlare di llm orientati nel nostro caso specifico.
    \item Stato dell'arte su analisi sicurezza e lettura dei log etc.
    \item Stato dell'arte dei log in generale: perché fare raccolta di log, raccorta centralizzata etc.
    \item Vedere se si trova nello stato dell'arte soluzioni già presenti, simili ma comunque un po' diverse
\end{itemize}

\todosection{Sicurezza nei log}{sec:sicurezza_log}{finire}

\todosection{Identificazione di dati sensibili nei log}{sec:identificazione_dati_sensibili}{finire}

\todosection{Soluzioni esistenti per l'analisi dei log}{sec:soluzioni_esistenti}{finire}

\todosection{Large Language Models nell'analisi dei testi}{sec:llm_analisi_testi}{finire}



%
%			CAPITOLO: Metodologia teorica
%

\todochapter{Metodologia}{chap:metodologia_teorica}{metodologia teorica}

\todosection{Architettura proposta}{sec:architettura_proposta}{grafico di flusso dell'architettura}

\todosection{Approccio all'identificazione dei dati sensibili}{sec:approccio_identificazione}{approccio all'identificazione dei dati sensibili}

\todosection{Metriche di valutazione (forse?)}{sec:metriche_valutazione}{metriche di valutazione}



%
%			CAPITOLO: Tecnologie utilizzate
%			(omettere questo capitolo se non necessario)
%

\chapter{Tecnologie utilizzate}
\label{chap:tecnologie_utilizzate}

Il progetto \textit{Sensitive Data Detector} integra diverse tecnologie per creare una soluzione completa di analisi automatica dei log. L'architettura si basa su un approccio multi-componente che combina strumenti di gestione dei log enterprise, tecnologie di intelligenza artificiale e linguaggi di programmazione consolidati.

Questa sezione descrive nel dettaglio le tecnologie scelte, motivando le decisioni architetturali e spiegando come ogni componente contribuisce al funzionamento complessivo del sistema.

\section{Elaborazione}
\label{sec:elaborazione}

Il livello di elaborazione costituisce il cuore del sistema, responsabile della raccolta, gestione e analisi intelligente dei log. Le tecnologie in questo layer sono state selezionate per garantire scalabilità, affidabilità e precisione nell'identificazione dei dati sensibili.

\subsection{GrayLog}
\label{subsec:graylog}

GrayLog rappresenta la piattaforma centrale per la gestione dei log del sistema. Si tratta di una soluzione enterprise open-source che fornisce funzionalità avanzate di raccolta, indicizzazione, ricerca e analisi dei log in tempo reale.
\todo{TODO aggiungere link a graylog}

La scelta di GrayLog è stata motivata principalmente dal fatto che questa piattaforma viene già utilizzata nell'ambiente di produzione target del progetto, rendendo l'integrazione del \textit{Sensitive Data Detector} un'estensione naturale dell'infrastruttura esistente.

\subsubsection{Caratteristiche principali}
Oltre al vantaggio dell'integrazione con l'ambiente esistente, le caratteristiche tecniche che rendono GrayLog ideale per questo progetto includono:

\begin{itemize}
    \item \textbf{Raccolta centralizzata}: Supporto nativo per diversi protocolli di logging (GELF\footnote{GELF (GrayLog Extended Log Format) è un formato strutturato progettato specificamente per il trasporto di log su reti TCP/UDP, che estende il formato syslog standard con campi aggiuntivi e supporto nativo per metadati.}, Syslog\footnote{Syslog è un protocollo standard per il logging di sistema definito dall RFC 3164, ampiamente utilizzato per la trasmissione di messaggi di log su reti IP.}) che permette l'integrazione con sistemi eterogenei
    \item \textbf{Indicizzazione real-time}: Capacità di indicizzare e rendere ricercabili i log immediatamente dopo la ricezione
    \item \textbf{Pipeline di processing}: Sistema di regole configurabili per trasformare, filtrare e arricchire i log durante l'ingestion
    \item \textbf{API REST complete}: Interfacce programmatiche per l'automazione della configurazione e l'integrazione con sistemi esterni
    \item \textbf{Scalabilità orizzontale}: Architettura distribuita che supporta cluster multi-nodo per gestire volumi elevati di log
\end{itemize}

\subsubsection{Integrazione nel progetto}
Nel contesto del \textit{Sensitive Data Detector}, GrayLog svolge un ruolo duplice:

\begin{enumerate}
    \item \textbf{Sorgente di dati}: Riceve log da sistemi di produzione tramite input configurabili sulla porta TCP 5555
    \item \textbf{Destinazione per notifiche}: Riceve le segnalazioni di dati sensibili identificati dal sistema tramite la porta TCP 5556
\end{enumerate}

La configurazione utilizza stream e output GELF per creare un pipeline che:
\begin{itemize}
    \item Instrada i log in ingresso verso il \textit{Sensitive Data Detector} tramite output GELF sulla porta 24367
    \item Raccoglie le notifiche di ritorno per visualizzazione e alerting
    \item Applica regole regex aggiuntive per identificazione rapida di pattern comuni (email, JWT, coordinate geografiche)
\end{itemize}

\subsection{Ollama}
\label{subsec:ollama}

Ollama è un framework open-source progettato per semplificare l'esecuzione locale di Large Language Models.\\
Rappresenta il ponte che permette al sistema di interfacciarsi con diversi modelli LLM in modo unificato ed efficiente.

\subsubsection{Architettura e funzionalità}
Ollama offre un'architettura client-server che include:

\begin{itemize}
    \item \textbf{Server locale}: Processo daemon che gestisce il caricamento e l'esecuzione dei modelli LLM
    \item \textbf{API REST}: Interfaccia HTTP standardizzata per l'interazione con i modelli caricati
    \item \textbf{Gestione dei modelli}: Sistema per il download, l'aggiornamento e la configurazione di modelli da repository pubblici
    \item \textit{\textbf{Modelfile}}: Formato di configurazione che permette la creazione di modelli personalizzati con prompt engineering incorporato
    \item \textbf{Ottimizzazioni hardware}: Supporto per accelerazione GPU e ottimizzazioni specifiche per diverse architetture (CPU, Metal\footnote{Metal è l'API grafica e di calcolo di Apple per macOS e iOS.} su macOS, CUDA\footnote{CUDA è la piattaforma di calcolo parallelo di NVIDIA per GPU.}/ROCM\footnote{ROCM è la piattaforma open-source di AMD per calcolo su GPU.} su Linux)
\end{itemize}

\subsubsection{Vantaggi per il progetto}
La scelta di Ollama ha portato diversi vantaggi architetturali:

\begin{itemize}
    \item \textbf{Deployment locale}: Eliminazione della dipendenza da servizi cloud esterni, garantendo privacy e controllo completo sui dati
    \item \textbf{Costi operativi ridotti}: Assenza di costi per token o chiamate API, permettendo elaborazioni intensive
    \item \textbf{Latenza ottimizzata}: Comunicazione locale che riduce significativamente i tempi di risposta
    \item \textbf{Flessibilità dei modelli}: Possibilità di sperimentare con diversi modelli senza vincoli commerciali
\end{itemize}

\subsection{Large Language Models}
\label{subsec:llm}

Il sistema utilizza un approccio multi-modello per massimizzare l'accuratezza nell'identificazione di dati sensibili. La strategia prevede l'utilizzo di modelli personalizzati ottimizzati specificamente per il dominio dei log di sistema.

\subsubsection{Modelli supportati}
Il progetto supporta nove diversi modelli LLM, selezionati per bilanciare performance, accuratezza e requisiti computazionali, come mostrato nella tabella \ref{tab:modelli_llm}.

\begin{table}[h!]
    \centering
    \begin{tabular}{|l|c|l|}
        \hline
        \textbf{Nome} & \textbf{Num. di token} & \textbf{Caratteristiche principali} \\ \hline
        Llama 3.1 & 8B & Modello generale con ottimo rapporto qualità/perf. \\ \hline
        Llama 3.2 & 3B & Versione compatta per ambienti con risorse limitate \\ \hline
        Mistral & 7B, 12B & Modelli europei con architettura Mixture of Experts \\ \hline
        Qwen 2.5 & 7B & Modello multilingue trainato su dati di tutti i tipi \\ \hline
        Gemma 3 & 4B, 12B & Modelli Google con ottimizzazioni per task specifici \\ \hline
        DeepSeek R1 & 7B, 8B & Modelli con capacità di reasoning avanzate \\ \hline
    \end{tabular}
    \caption{Modelli LLM supportati dal sistema}
    \label{tab:modelli_llm}
\end{table}
\todo{TODO bibliografia a mixture of experts}

\subsubsection{Personalizzazione tramite \textit{Modelfile}}
Ogni modello viene personalizzato attraverso file di configurazione \textit{Modelfile} che includono:

\begin{itemize}
    \item \textbf{Prompt engineering}: Istruzioni specifiche per l'identificazione di dati sensibili nei log
    \item \textbf{Parametri ottimizzati}: Configurazione di temperatura e context window per massimizzare la precisione
    \item \textbf{Esempi di training}: Messaggi di esempio che guidano il modello nel formato di output atteso
    \item \textbf{Definizioni precise}: Specifiche dettagliate su cosa costituisce un dato sensibile e cosa escludere dall'analisi
\end{itemize}

Il prompt engineering incorpora linee guida specifiche per:
\begin{itemize}
    \item Identificazione di email, token di autenticazione, coordinate geografiche
    \item Esclusione di dati non sensibili come username, ID, timestamp
    \item Gestione di oggetti JSON complessi con estrazione selettiva
    \item Formato di output standardizzato per facilità di parsing
\end{itemize}

Per ulteriori dettagli riguardanti i \textit{Modelfile} e la configurazione dei LLM si rimanda alla sezione \ref{subsec:ver2_prompt_engineering}.


\section{Codice}
\label{sec:codice}

Il livello di implementazione si basa su tecnologie consolidate che garantiscono affidabilità, manutenibilità e portabilità del sistema.

\subsection{Python}
\label{subsec:python}

Python rappresenta il linguaggio di programmazione principale del progetto, scelto per la sua versatilità e l'ampio ecosistema di librerie specializzate per l'integrazione con sistemi di AI.

\subsubsection{Versione e compatibilità}
Il progetto utilizza Python 3.12, che garantisce:
\begin{itemize}
    \item Supporto completo per programmazione asincrona con \texttt{asyncio}
    \item Ottimizzazioni di performance per applicazioni I/O intensive
    \item Compatibilità con le librerie più recenti dell'ecosistema AI
    \item Funzionalità avanzate di type hinting per migliorare la manutenibilità del codice
\end{itemize}

\subsubsection{Dipendenze principali}
Le librerie Python utilizzate includono:

\begin{itemize}
    \item \textbf{ollama}: Libreria ufficiale per l'interazione con il server Ollama tramite API REST
    \item \textbf{httpx}: Client HTTP asincrono per comunicazioni ad alte performance
    \item \textbf{anyio}: Framework per programmazione asincrona cross-platform
    \item \textbf{requests}: Per configurazione automatica di GrayLog tramite API REST
\end{itemize}

\subsubsection{Sensitive Data Detector}
Il core del sistema è implementato in tre versioni evolutive, ognuna con caratteristiche specifiche:

\paragraph{Versione 1: Proof of Concept}
Implementazione sincrona che utilizza:
\begin{itemize}
    \item Elaborazione batch di file di log pre-esistenti
    \item Interfaccia diretta con l'API Ollama per chiamate sequenziali
    \item Gestione manuale del context window con reset periodici
    \item Output file-based per analisi offline
\end{itemize}

Per ulteriori dettagli della versione 1 si rimanda alla sezione \ref{sec:ver1}.

\paragraph{Versione 2: Streaming e valutazione}
Evoluzione con focus sulla scalabilità:
\begin{itemize}
    \item Elaborazione riga per riga per ottimizzazione della memoria
    \item Utilizzo di modelli personalizzati con prompt incorporati
    \item Sistema di ground truth per valutazione automatica delle performance
    \item Calcolo real-time di metriche di classificazione
\end{itemize}

Per ulteriori dettagli della versione 2 si rimanda alla sezione \ref{sec:ver2}.

\paragraph{Versione 3: Integrazione enterprise}
Versione production-ready con architettura asincrona:
\begin{itemize}
    \item Programmazione asincrona con \texttt{asyncio} per gestione concorrente
    \item Protocollo GELF per comunicazione nativa con GrayLog
    \item Server TCP non-bloccante per elaborazione real-time
    \item Sistema di notifiche bidirezionale per integrazione operativa
\end{itemize}

Per ulteriori dettagli della versione 3 si rimanda alla sezione \ref{sec:ver3}.

\subsubsection{Script di inizializzazione GrayLog}
Il file \texttt{graylog\_setup.py} automatizza la configurazione iniziale di GrayLog utilizzando:

\begin{itemize}
    \item \textbf{API REST di GrayLog}: Per creazione programmatica di input, stream, output e pipeline
    \item \textbf{Configurazione idempotente}: Script eseguibile multiple volte senza effetti collaterali
    \item \textbf{Gestione degli errori}: Validazione delle configurazioni e rollback automatico in caso di errori
    \item \textbf{Template personalizzabili}: Configurazioni parametrizzate per adattamento a diversi ambienti
\end{itemize}

Lo script configura automaticamente:
\begin{itemize}
    \item Input TCP per log in ingresso e notifiche di ritorno
    \item Stream per routing intelligente dei messaggi
    \item Output GELF per comunicazione con il \textit{Sensitive Data Detector}
    \item Pipeline con regole regex per identificazione rapida di pattern comuni
\end{itemize}

Il codice completo dello script è disponibile nell'appendice \ref{app:code_graylog_script}.
\todo{TODO aggiungere script graylog in appendice}

\subsection{Docker}
\label{subsec:docker}

Docker viene utilizzato per la containerizzazione dell'ambiente GrayLog, garantendo portabilità, isolamento e semplicità di deployment.

\subsubsection{Architettura multi-container}
La configurazione Docker Compose definisce un ambiente completo che include:

\begin{itemize}
    \item \textbf{MongoDB}: Database per memorizzazione dei metadati di configurazione di GrayLog
    \item \textbf{GrayLog DataNode}: Nodo di indicizzazione per storage e ricerca dei log
    \item \textbf{GrayLog Enterprise}: Server principale con interfaccia web e API REST
\end{itemize}

Il codice completo della configurazione del container è disponibile nell'appendice \ref{app:code_container_config}.
\todo{TODO aggiungere script graylog in appendice}

\subsubsection{Configurazione di rete e persistenza}
L'ambiente Docker implementa:

\begin{itemize}
    \item \textbf{Rete isolata}: Bridge network dedicata per comunicazione sicura tra container
    \item \textbf{Volumi persistenti}: Storage permanente per i dati di MongoDB, indici GrayLog e le configurazioni
    \item \textbf{Port mapping}: Esposizione selettiva delle porte necessarie per l'integrazione esterna
    \item \textbf{Health checks}: Monitoraggio automatico dello stato dei servizi con restart policies
\end{itemize}

\subsubsection{Vantaggi operativi}
L'utilizzo di Docker porta diversi benefici:

\begin{itemize}
    \item \textbf{Deployment semplificato}: Avvio dell'intero stack con un singolo comando
    \item \textbf{Isolamento delle dipendenze}: Prevenzione di conflitti con software pre-esistente nel sistema host
    \item \textbf{Portabilità cross-platform}: Funzionamento consistente su Windows, macOS e Linux
    \item \textbf{Scalabilità orizzontale}: Possibilità di replicare l'ambiente per testing o deployment distribuito
    \item \textbf{Gestione delle versioni}: Controllo preciso delle versioni dei componenti utilizzati
\end{itemize}



%
%			CAPITOLO: Il lavoro svolto
%

\chapter{Sensitive Data Detector}
\label{chap:sensitive_data_detector}

Il lavoro è stato diviso in tre versioni separate, ognuna con un obiettivo specifico. \\
La prima versione è stata sviluppata per dimostrare la fattibilità del progetto e opera analizzando dei piccoli file di log. \\
La seconda versione è un miglioramento della prima e permette di analizzare file di log di dimensioni elevate, tramite lo streaming delle righe. \\
La terza versione è la versione finale e si integra con GrayLog per la lettura dei log e per riportare i risultati dell'analisi.

\section{Versione 1: Proof of Concept}
\label{sec:ver1}

La prima versione del \textit{Sensitive Data Detector} rappresenta un proof of concept sviluppato per dimostrare la fattibilità dell'utilizzo di Large Language Models nell'identificazione automatica di dati sensibili all'interno dei file di log. \\
Questa versione costituisce la base concettuale dell'intero progetto, implementando un approccio diretto e semplificato per l'analisi di file di log di dimensioni contenute.

\subsection{Obiettivi e scopo}
\label{subsec:ver1_obiettivi}

L'obiettivo principale di questa prima implementazione è verificare l'efficacia dei modelli LLM nell'identificazione di informazioni sensibili presenti nei log di sistema. La versione si concentra sull'analisi di file di log pre-esistenti, utilizzando un approccio batch che processa il contenuto in modo sequenziale.

Il sistema è progettato per riconoscere diverse tipologie di dati sensibili, tra cui:
\begin{itemize}
    \item Informazioni personali identificabili (PII)
    \item Credenziali di accesso e token di autenticazione
    \item Indirizzi email e informazioni di contatto
    \item Chiavi crittografiche e certificati
\end{itemize}

\subsection{Architettura e funzionamento}
\label{subsec:ver1_architettura}

Il sistema è implementato come uno script Python che utilizza la libreria \texttt{ollama} per interfacciarsi con diversi modelli LLM tramite API REST. L'architettura segue un paradigma client-server dove lo script Python agisce come client che invia richieste al server Ollama locale. \\
L'implementazione completa è disponibile nell'appendice \ref{app:code_ver1}.

Il flusso di elaborazione si articola nelle seguenti fasi:

\begin{enumerate}
    \item \textbf{Inizializzazione}: Lo script carica il prompt di sistema da un file esterno e configura i parametri specifici per il modello selezionato
    \item \textbf{Lettura dei log}: Il sistema legge il file di log dall'archivio di esempi e lo suddivide in blocchi di dimensioni configurabili
    \item \textbf{Elaborazione batch}: Ogni blocco viene inviato al modello LLM insieme al prompt di sistema per l'analisi
    \item \textbf{Raccolta risultati}: Le risposte del modello vengono aggregate e formattate
    \item \textbf{Salvataggio output}: I risultati vengono salvati in file per consentire tracciabilità e analisi successive
\end{enumerate}

\subsection{Gestione del context window}
\label{subsec:ver1_context_window}

Un aspetto critico dell'implementazione riguarda la gestione del context window limitato dei modelli LLM. Il sistema implementa due strategie principali:

\begin{itemize}
    \item \textbf{Segmentazione dei dati}: I file di log vengono suddivisi in blocchi di dimensioni adattive basate sulle capacità del modello
    \item \textbf{Reset periodico della conversazione}: Dopo un numero predefinito di messaggi, la cronologia della chat viene resettata per evitare il superamento dei limiti di token
\end{itemize}

Questa approccio garantisce che il modello mantenga sempre accesso al prompt iniziale e alle linee guida per l'identificazione dei dati sensibili.

\subsection{Prompt engineering}
\label{subsec:ver1_prompt_engineering}

Il prompt di sistema è stato progettato per guidare il modello nell'identificazione precisa dei dati sensibili, fornendo:

\begin{itemize}
    \item Definizioni chiare di cosa costituisce un dato sensibile
    \item Esempi specifici di dati da identificare (token, credenziali, informazioni personali)
    \item Esclusioni esplicite per dati non sensibili (username, ID, timestamp)
    \item Formato di output strutturato in bullet point
    \item Esempio pratico con input e output atteso
\end{itemize}

Il prompt include inoltre istruzioni per la gestione di oggetti JSON, richiedendo al modello di estrarre solo i dati sensibili contenuti all'interno di essi e non l'intero oggetto. \\
Il prompt completo è disponibile nell'appendice \ref{app:prompt_ver1}.

\subsection{Modelli supportati e configurazione}
\label{subsec:ver1_modelli_supportati}

La prima versione supporta cinque diversi modelli LLM, ognuno con parametri di configurazione ottimizzati, come mostrato nella tabella \ref{tab:modelli_llm_ver1}.

\begin{table}[h!]
    \centering
    \begin{tabular}{|l|c|c|c|}
        \hline
        \textbf{Modello} & \textbf{Parametri} & \textbf{Righe per messaggio} & \textbf{Reset chat} \\ \hline
        Llama3           & 8B                 & 3                            & Ogni messaggio      \\ \hline
        Llama3           & 70B                & 10                           & Ogni 5 messaggi     \\ \hline
        Llama3.1         & 8B                 & 3                            & Ogni messaggio      \\ \hline
        Llama3.1         & 70B                & 10                           & Ogni 5 messaggi     \\ \hline
        Command-R        & 35B                & 5                            & Ogni 5 messaggi     \\ \hline
    \end{tabular}
    \caption{Configurazioni ottimizzate per modello nella versione 1}
    \label{tab:modelli_llm_ver1}
\end{table}
\todo{TODO: verificare correttezza tabella}

La differenziazione dei parametri riflette le diverse capacità computazionali e di context window dei modelli. I modelli più grandi (70B parametri) possono processare blocchi più ampi di dati e mantenere conversazioni più lunghe prima del reset del contesto.

\subsection{Output e tracciabilità}
\label{subsec:ver1_output}

Il sistema genera file di output timestampati che includono:
\begin{itemize}
    \item Metadati dell'esecuzione (timestamp, modello utilizzato, file di log analizzato)
    \item Prompt iniziale utilizzato
    \item Risposte complete del modello LLM
    \item Statistiche di elaborazione (righe processate, tempo di esecuzione)
\end{itemize}

Questa struttura consente la tracciabilità completa del processo di analisi e facilita il confronto tra diversi modelli e configurazioni.

\subsection{Limitazioni della versione}
\label{subsec:ver1_limitazioni}

La prima versione presenta alcune limitazioni significative:

\begin{itemize}
    \item \textbf{Scalabilità limitata}: Il processamento batch sequenziale non è ottimale per file di log di grandi dimensioni
    \item \textbf{Mancanza di integrazione real-time}: Il sistema opera solo su file pre-esistenti, senza capacità di elaborazione in tempo reale
    \item \textbf{Assenza di metriche di valutazione}: Non sono implementati meccanismi automatici per valutare l'accuratezza delle identificazioni
\end{itemize}

Nonostante queste limitazioni, la prima versione ha fornito risultati promettenti che hanno giustificato lo sviluppo delle versioni successive più avanzate.

\section{Versione 2: Elaborazione di batch di log}
\label{sec:ver2}

La seconda versione del \textit{Sensitive Data Detector} rappresenta un'evoluzione significativa del proof of concept, introducendo un sistema di valutazione automatica delle performance e l'utilizzo di modelli LLM personalizzati ottimizzati per il compito specifico. \\
Questa versione si concentra sull'elaborazione efficiente di file di log di dimensioni maggiori tramite streaming delle righe e implementa metriche di valutazione quantitative.

\subsection{Obiettivi e scopo}
\label{subsec:ver2_obiettivi}

L'obiettivo principale di questa seconda implementazione è duplice: da un lato migliorare la scalabilità del sistema per gestire volumi di log più elevati, dall'altro introdurre un framework di valutazione automatica per misurare obiettivamente le performance dei modelli LLM.

La versione introduce diversi miglioramenti sostanziali:
\begin{itemize}
    \item Elaborazione riga per riga per ottimizzare l'uso della memoria e semplificare il processo di rilevazione della correttezza delle risposte
    \item Utilizzo di modelli personalizzati con prompt engineering avanzato
    \item Sistema di ground truth per la valutazione automatica
    \item Calcolo di metriche di classificazione binaria (veri/falsi positivi/negativi e accuratezza)
\end{itemize}

\subsection{Architettura e funzionamento}
\label{subsec:ver2_architettura}

L'architettura della seconda versione mantiene il paradigma client-server della versione precedente e al contempo introduce ottimizzazioni nel flusso di elaborazione. \\
Il sistema utilizza modelli LLM personalizzati creati tramite \textit{Modelfile}, una funzionalità di Ollama, che incorporano prompt specifici e configurazioni ottimizzate per l'identificazione di dati sensibili.\\
L'implementazione completa è disponibile nell'appendice \ref{app:code_ver2}.

Il flusso di elaborazione ottimizzato si articola nelle seguenti fasi:

\begin{enumerate}
    \item \textbf{Inizializzazione del dataset}: Il sistema carica file di log pre-annotati con etichette ground truth (Y/N) per ogni riga
    \item \textbf{Streaming sequenziale}: Ogni riga viene processata individualmente, eliminando la necessità di caricare grandi volumi in memoria
    \item \textbf{Preprocessamento}: Le etichette di ground truth vengono estratte e conservate per la valutazione, mentre il contenuto della riga di log viene inviato al modello
    \item \textbf{Analisi LLM}: Il modello personalizzato analizza la singola riga e produce una classificazione binaria (sensibile/non sensibile)
    \item \textbf{Post-processing}: Le risposte vengono elaborate per rimuovere eventuali tag di reasoning e standardizzare il formato
    \item \textbf{Valutazione in tempo reale}: Ogni risposta viene immediatamente confrontata con il ground truth e le metriche vengono aggiornate
    \item \textbf{Gestione della memoria}: Il contesto viene resettato periodicamente per mantenere performance ottimali indefinitamente, anche su file di log di grandi dimensioni
\end{enumerate}

\subsection{Prompt engineering}
\label{subsec:ver2_prompt_engineering}

La seconda versione introduce un approccio di prompt engineering più sofisticato attraverso l'utilizzo di modelli personalizzati. \\
A differenza della prima versione, dove il prompt veniva inviato ad ogni interazione, i modelli personalizzati incorporano le istruzioni direttamente nella loro configurazione tramite \textit{Modelfile}, una funzionalità di Ollama.

Questa strategia offre diversi vantaggi:
\begin{itemize}
    \item \textbf{Consistenza}: Il prompt è sempre presente nel contesto del modello, eliminando la variabilità dovuta alla gestione manuale
    \item \textbf{Efficienza}: Riduzione della lunghezza dei messaggi inviati, ottimizzando l'utilizzo del context window
    \item \textbf{Specializzazione}: Ogni modello può essere ottimizzato con prompt specifici per le sue caratteristiche architetturali
    \item \textbf{Reasoning avanzato}: Supporto per modelli con capacità di reasoning che utilizzano tag speciali come \texttt{<think>}
\end{itemize}

Il sistema implementa anche un meccanismo di post-processing per gestire le risposte dei modelli reasoning, rimuovendo automaticamente i tag \texttt{<think>...</think>} che contengono il processo di ragionamento interno del modello, considerando solo la risposta finale.

\subsection{Modelli supportati e configurazione}
\label{subsec:ver2_modelli_supportati}

La seconda versione espande significativamente il supporto per diversi modelli LLM, introducendo nove modelli personalizzati ottimizzati specificamente per l'identificazione di dati sensibili. \\
Tutti i modelli condividono una configurazione unificata ottimizzata per l'elaborazione sequenziale di singole righe di log, come mostrato nella tabella \ref{tab:modelli_llm_ver2}.

\begin{table}[h!]
    \centering
    \begin{tabular}{|l|c|c|c|}
        \hline
        \textbf{Modello} & \textbf{Parametri} & \textbf{Righe per messaggio} & \textbf{Reset chat} \\ \hline
        Llama3.1         & 8B                 & 1                            & Ogni 15 messaggi    \\ \hline
        Llama3.2         & 3B                 & 1                            & Ogni 15 messaggi    \\ \hline
        Mistral          & 7B                 & 1                            & Ogni 15 messaggi    \\ \hline
        Mistral Nemo     & 12B                & 1                            & Ogni 15 messaggi    \\ \hline
        Qwen2.5          & 7B                 & 1                            & Ogni 15 messaggi    \\ \hline
        Gemma3           & 4B                 & 1                            & Ogni 15 messaggi    \\ \hline
        Gemma3           & 12B                & 1                            & Ogni 15 messaggi    \\ \hline
        DeepSeek R1      & 7B                 & 1                            & Ogni 15 messaggi    \\ \hline
        DeepSeek R1      & 8B                 & 1                            & Ogni 15 messaggi    \\ \hline
    \end{tabular}
    \caption{Configurazioni ottimizzate per modello nella versione 2}
    \label{tab:modelli_llm_ver2}
\end{table}
\todo{TODO: verificare correttezza tabella}

A differenza della prima versione, tutti i modelli utilizzano una configurazione unificata con elaborazione di una riga per messaggio e reset della conversazione ogni 15 messaggi. Questa standardizzazione facilita il confronto diretto delle performance tra modelli diversi e ottimizza l'utilizzo delle risorse computazionali. \\
Inoltre, l'elaborazione di singole righe consente una valutazione immediata dell'accuratezza delle classificazioni prodotte dal modello.

\subsection{Output e tracciabilità}
\label{subsec:ver2_output}

La seconda versione introduce un sistema di output e tracciabilità migliorato, con particolare focus sulle metriche di valutazione quantitative. Il sistema genera report dettagliati che includono sia le informazioni di base dell'esecuzione che analisi statistiche delle performance di classificazione.

I file di output contengono le seguenti informazioni:
\begin{itemize}
    \item \textbf{Metadati dell'esecuzione}:
          \begin{itemize}
              \item timestamp
              \item modello utilizzato
              \item file di log analizzato
              \item numero totale di righe processate
          \end{itemize}
    \item \textbf{Metriche di classificazione}:
          \begin{itemize}
              \item veri positivi
              \item veri negativi
              \item falsi positivi
              \item falsi negativi
          \end{itemize}
    \item \textbf{Statistiche di accuratezza}:
          \begin{itemize}
              \item percentuale di risposte corrette
              \item percentuale di risposte errate
          \end{itemize}
    \item \textbf{Log dettagliato}: traccia completa di tutte le interazioni con il modello LLM
\end{itemize}

Il sistema calcola e visualizza in tempo reale le metriche di performance durante l'elaborazione, fornendo feedback immediato sull'efficacia del modello. \\
Questa funzionalità è particolarmente utile per confronti comparativi tra diversi modelli e per l'ottimizzazione dei parametri di configurazione.

\subsection{Limitazioni della versione}
\label{subsec:ver2_limitazioni}

Nonostante i significativi miglioramenti rispetto alla prima versione, la seconda implementazione presenta ancora alcune limitazioni che hanno guidato lo sviluppo della versione successiva:

\begin{itemize}
    \item \textbf{Dipendenza da dataset pre-annotati}: Il sistema di valutazione richiede file di log manualmente etichettati, limitando la scalabilità dell'approccio di testing
    \item \textbf{Elaborazione offline}: Il sistema opera esclusivamente su file pre-esistenti, senza capacità di integrazione con sistemi di logging in tempo reale
\end{itemize}

Queste limitazioni, sebbene non compromettano l'utilità della versione per scopi di ricerca e testing, evidenziano la necessità di un'integrazione più robusta con sistemi di produzione, obiettivo che viene affrontato nella terza versione del progetto.

\section{Versione 3: Integrazione con GrayLog}
\label{sec:ver3}

La terza versione del \textit{Sensitive Data Detector} rappresenta l'evoluzione finale del progetto, trasformando il sistema da uno strumento di analisi offline a una soluzione completamente integrata per l'elaborazione in tempo reale dei log. \\
Questa versione si interfaccia direttamente con GrayLog, una piattaforma enterprise di gestione e analisi dei log, creando un sistema end-to-end per il monitoraggio continuo della sicurezza dei dati.

\subsection{Obiettivi e scopo}
\label{subsec:ver3_obiettivi}

L'obiettivo principale della terza versione è fornire una soluzione production-ready per l'identificazione automatica di dati sensibili in ambienti operativi reali. Questa implementazione affronta le principali limitazioni delle versioni precedenti, introducendo:

\begin{itemize}
    \item \textbf{Elaborazione in tempo reale}: Analisi immediata dei log non appena vengono generati dai sistemi
    \item \textbf{Integrazione enterprise}: Interfacciamento nativo con GrayLog per inserimento seamless negli stack tecnologici esistenti
    \item \textbf{Scalabilità operativa}: Architettura asincrona progettata per gestire volumi elevati di log in produzione
    \item \textbf{Feedback loop}: Sistema di notifica automatica che remanda i risultati delle analisi a GrayLog, in modo da poter visualizzare i risultati tramite un'interfaccia unificata
    \item \textbf{Configurazione centralizzata}: Gestione unificata di input, processing pipeline e output attraverso l'interfaccia GrayLog
\end{itemize}

\subsection{Architettura e funzionamento}
\label{subsec:ver3_architettura}

L'architettura della terza versione introduce un cambio del paradigma rispetto alle versioni precedenti, passando da un modello client-server a un sistema basato su stream processing. Il sistema opera come un microservizio che si posiziona tra GrayLog e i modelli LLM, fungendo da bridge intelligente per l'analisi dei contenuti. \\
L'implementazione completa è disponibile nell'appendice \ref{app:code_ver3}.

Il flusso di elaborazione real-time si articola nelle seguenti fasi:

\begin{enumerate}
    \item \textbf{Ricezione stream}: Il sistema ascolta sulla porta TCP 24367 per ricevere log in formato GELF da GrayLog
    \item \textbf{Parsing dei messaggi}: Ogni messaggio GELF viene deserializzato per estrarre contenuto e metadati del log
    \item \textbf{Analisi LLM}: Il contenuto viene inviato al modello personalizzato per l'identificazione di dati sensibili
    \item \textbf{Gestione risultati}: Le risposte positive vengono elaborate e formattate con gli identificatori originali del log
    \item \textbf{Notifica a GrayLog}: I risultati vengono reinviati a GrayLog sulla porta 5556 per integrazione nei flussi di alerting
    \item \textbf{Gestione della memoria}: Il contesto conversazionale viene periodicamente resettato per mantenere performance costanti
\end{enumerate}

La comunicazione avviene tramite protocollo GELF, che garantisce la preservazione di tutti i metadati necessari per la tracciabilità e il troubleshooting.

\subsection{Integrazione con GrayLog}
\label{subsec:ver3_integrazione}

L'integrazione con GrayLog avviene attraverso una configurazione di input, stream e output che crea un pipeline completo di elaborazione:

\begin{itemize}
    \item \textbf{Input TCP 5555}: Riceve i log originali dai sistemi sorgente
    \item \textbf{Stream di routing}: Indirizza i log verso il Sensitive Data Detector tramite output GELF
    \item \textbf{Input TCP 5556}: Riceve le notifiche di dati sensibili dal sistema di analisi
    \item \textbf{Pipeline di processing}: Applica regole aggiuntive per categorizzazione e alerting
\end{itemize}

Questa architettura permette di mantenere separati i log originali dalle notifiche di sicurezza, facilitando la creazione di dashboard specializzate e la configurazione di alert specifici per la conformità ai regolamenti sulla privacy.

\subsection{Modelli supportati e configurazione}
\label{subsec:ver3_modelli_supportati}

La terza versione eredita il supporto completo per i nove modelli personalizzati della versione precedente, mantenendo la stessa configurazione unificata ottimizzata per l'elaborazione in tempo reale, come mostrato nella tabella \ref{tab:modelli_llm_ver3}.

\begin{table}[h!]
    \centering
    \begin{tabular}{|l|c|c|c|}
        \hline
        \textbf{Modello} & \textbf{Parametri} & \textbf{Messaggi per conversazione} & \textbf{Reset chat} \\ \hline
        Llama3.1         & 8B                 & 1                                   & Ogni 15 messaggi    \\ \hline
        Llama3.2         & 3B                 & 1                                   & Ogni 15 messaggi    \\ \hline
        Mistral          & 7B                 & 1                                   & Ogni 15 messaggi    \\ \hline
        Mistral Nemo     & 12B                & 1                                   & Ogni 15 messaggi    \\ \hline
        Qwen2.5          & 7B                 & 1                                   & Ogni 15 messaggi    \\ \hline
        Gemma3           & 4B                 & 1                                   & Ogni 15 messaggi    \\ \hline
        Gemma3           & 12B                & 1                                   & Ogni 15 messaggi    \\ \hline
        DeepSeek R1      & 7B                 & 1                                   & Ogni 15 messaggi    \\ \hline
        DeepSeek R1      & 8B                 & 1                                   & Ogni 15 messaggi    \\ \hline
    \end{tabular}
    \caption{Configurazioni ottimizzate per modello nella versione 3}
    \label{tab:modelli_llm_ver3}
\end{table}
\todo{TODO: verificare correttezza tabella}

La configurazione real-time mantiene l'elaborazione di un messaggio per volta per semplificare il processo di rilevazione della correttezza delle risposte. \\
L'analisi di un messaggio alla volta permette inoltre di ottimizzare la latenza di risposta, aspetto critico in ambienti di produzione dove la velocità di identificazione delle vulnerabilità può essere determinante per la sicurezza del sistema.

\subsection{Programmazione asincrona e scalabilità}
\label{subsec:ver3_asincrona}

La terza versione introduce l'uso di programmazione asincrona tramite \texttt{asyncio}, permettendo la gestione concorrente di multiple connessioni senza bloccare l'elaborazione. Questa architettura offre diversi vantaggi:

\begin{itemize}
    \item \textbf{Gestione concorrente}: Elaborazione parallela di log provenienti da fonti multiple
    \item \textbf{Resilienza}: Isolamento degli errori per singola connessione senza impatto sul sistema globale
    \item \textbf{Efficienza delle risorse}: Utilizzo ottimale della CPU attraverso I/O non bloccante
    \item \textbf{Scalabilità orizzontale}: Possibilità di deployment di multiple istanze per load balancing
\end{itemize}

\subsection{Output e tracciabilità}
\label{subsec:ver3_output}

Il sistema di output della terza versione è progettato per l'integrazione operativa, producendo messaggi strutturati che vengono inviati direttamente in GrayLog:

\begin{itemize}
    \item \textbf{Notifiche real-time}: Ogni identificazione di dato sensibile genera immediatamente una notifica a GrayLog
    \item \textbf{Preservazione degli identificatori}: I risultati mantengono il collegamento con i log originali tramite ID univoci
    \item \textbf{Formato standardizzato}: Output strutturato compatibile con pipeline di processing esistenti
    \item \textbf{Logging delle operazioni}: Tracciatura completa delle operazioni per audit e debugging
\end{itemize}

Le notifiche seguono il formato: \\
\texttt{<log\_id>: <tipo\_dato\_sensibile> <contenuto\_identificato>}, facilitando la correlazione con i log originali e l'automazione delle azioni correttive.

\subsection{Limitazioni e considerazioni operative}
\label{subsec:ver3_limitazioni}

Nonostante rappresenti una soluzione production-ready, la terza versione presenta alcune considerazioni operative importanti:

\begin{itemize}
    \item \textbf{Dipendenza da Ollama}: Il sistema richiede un'istanza Ollama sempre attiva, introducendo un single point of failure
    \item \textbf{Latenza variabile}: I tempi di risposta dipendono dalle performance del modello LLM selezionato e dal carico del sistema
    \item \textbf{Gestione degli errori}: Le interruzioni di connessione con Ollama causano terminazione del servizio
    \item \textbf{Scalabilità verticale}: Le performance sono limitate dalle risorse hardware disponibili per l'inferenza LLM
    \item \textbf{Utilizzo di modelli personalizzati}: Sebbene i modelli personalizzati migliorino significativamente le performance rispetto ai modelli base, il sistema non utilizza tecniche di fine-tuning avanzate come LoRa\footnote{LoRa (Low-Rank Adaptation) è una tecnica di fine-tuning efficiente che modifica solo un sottoinsieme dei parametri del modello e permette di creare un nuovo LLM a partire da uno già addestrato.}, che rappresenterebbe la soluzione ottimale per l'adattamento specifico al dominio dei log. Per ulteriori dettagli, si rimanda alla sezione \ref{subsec:addestramento_lora}
\end{itemize}

Queste limitazioni, pur non compromettendo l'utilità operativa del sistema, rappresentano aree di miglioramento e suggeriscono direzioni per sviluppi futuri del progetto.



%
%			CAPITOLO: Test
%

\chapter{Test e valutazione}
\label{chap:test}

Il sistema \textit{Sensitive Data Detector} è stato sottoposto a un processo di testing e valutazione sistematico attraverso due fasi evolutive principali, corrispondenti alle prime due versioni del progetto. Ogni fase ha introdotto metodologie di test sempre più raffinate e metriche di valutazione quantitative per misurare l'efficacia del sistema nell'identificazione di dati sensibili.

La terza versione del sistema, pur rappresentando un'evoluzione significativa dal punto di vista architetturale e operativo, non è stata sottoposta a un processo di testing sistematico analogo alle versioni precedenti. Questa scelta è motivata da considerazioni sia tecniche che metodologiche:

\begin{itemize}
    \item \textbf{Equivalenza algoritmica}: La logica di identificazione dei dati sensibili rimane sostanzialmente invariata rispetto alla seconda versione, mantenendo gli stessi LLM e \textit{Modelfile}
    \item \textbf{Natura operativa}: La terza versione opera direttamente su stream di log provenienti da GrayLog, rendendo impraticabile l'utilizzo di dataset pre-annotati con ground truth
    \item \textbf{Ambiente di produzione}: L'integrazione con sistemi reali introduce variabili ambientali che comprometterebbero la riproducibilità dei test quantitativi
\end{itemize}

Le performance della terza versione possono essere quindi stimate sulla base dei risultati ottenuti dalla seconda versione, dato che condivide gli stessi algoritmi fondamentali di identificazione

\section{Metodologia di test}
\label{sec:metodologia_test}

La metodologia di test si è evoluta significativamente tra le due versioni principali del sistema, passando da un approccio qualitativo basato su osservazione manuale a un sistema quantitativo automatizzato.

\subsection{Versione 1: Testing esplorativo}
\label{subsec:metodologia_v1}

La prima versione ha utilizzato un approccio esplorativo per validare la fattibilità dell'utilizzo di Large Language Models nell'identificazione di dati sensibili. Il testing si è concentrato su:

\begin{itemize}
    \item \textbf{Analisi qualitativa delle risposte}: Ogni output del modello LLM veniva analizzato manualmente per identificare la presenza di falsi positivi e falsi negativi
    \item \textbf{Ottimizzazione delle configurazioni}: Test di diverse combinazioni di parametri (righe per messaggio, messaggi per chat) per massimizzare l'efficacia
    \item \textbf{Gestione delle limitazioni hardware}: Valutazione dei vincoli di memoria e performance sull'architettura hardware utilizzata
    \item \textbf{Test di scalabilità}: Analisi del comportamento del sistema su dataset di dimensioni crescenti (100, 1000 righe)
\end{itemize}

L'approccio della prima versione ha evidenziato problematiche critiche legate alla gestione del context window limitato dei modelli LLM, portando allo sviluppo di strategie di segmentazione e reset delle conversazioni.

\subsection{Versione 2: Testing quantitativo}
\label{subsec:metodologia_v2}

La seconda versione ha introdotto un framework di testing quantitativo basato su ground truth annotato manualmente. La metodologia include:

\begin{itemize}
    \item \textbf{Preparazione del dataset}: Annotazione manuale di file di log con etichette binarie (Y/N) per ogni riga, indicando la presenza o assenza di dati sensibili
    \item \textbf{Elaborazione automatizzata}: Implementazione di un sistema di confronto automatico tra le risposte del modello e il ground truth
    \item \textbf{Calcolo delle metriche}: Calcolo real-time di veri positivi, veri negativi, falsi positivi e falsi negativi
    \item \textbf{Test comparativi}: Valutazione sistematica di nove modelli LLM diversi con configurazioni standardizzate
\end{itemize}

Questa evoluzione metodologica ha permesso un confronto oggettivo e riproducibile delle performance tra diversi modelli e configurazioni.

\section{Metriche di valutazione}
\label{sec:metriche_test}

Il sistema di valutazione utilizza metriche standard di classificazione binaria, adattate al contesto specifico dell'identificazione di dati sensibili nei log.

\subsection{Metriche primarie}
\label{subsec:metriche_primarie}

Le metriche principali utilizzate per la valutazione sono:

\begin{itemize}
    \item \textbf{Veri Positivi (TP)}: Righe contenenti dati sensibili correttamente identificate dal modello
    \item \textbf{Veri Negativi (TN)}: Righe senza dati sensibili correttamente classificate come non sensibili
    \item \textbf{Falsi Positivi (FP)}: Righe senza dati sensibili erroneamente identificate come contenenti dati sensibili
    \item \textbf{Falsi Negativi (FN)}: Righe contenenti dati sensibili non identificate dal modello
\end{itemize}

\subsection{Metriche derivate}
\label{subsec:metriche_derivate}

Dalle metriche primarie vengono calcolate le seguenti percentuali:

\begin{itemize}
    \item \textbf{Accuratezza Complessiva}: $\frac{TP + TN}{TP + TN + FP + FN} \times 100$
    \item \textbf{Percentuale di Falsi Positivi}: $\frac{FP}{TP + TN + FP + FN} \times 100$
    \item \textbf{Percentuale di Falsi Negativi}: $\frac{FN}{TP + TN + FP + FN} \times 100$
\end{itemize}

Nel contesto applicativo, i falsi negativi rappresentano un rischio di sicurezza maggiore rispetto ai falsi positivi, in quanto comportano la mancata identificazione di dati sensibili effettivamente presenti nei log.

\section{Risultati sperimentali}
\label{sec:risultati_sperimentali}

I test sono stati condotti su architettura Apple M1 Max (32 GB di memoria unificata, 32 GPU Core) utilizzando il framework Ollama per l'esecuzione locale dei modelli LLM.

\subsection{Risultati Versione 1}
\label{subsec:risultati_v1}

La prima versione ha fornito risultati promettenti ma ha evidenziato significative limitazioni hardware, come mostrato nella tabella \ref{tab:risultati_v1}.

\begin{table}[h!]
    \centering
    \begin{tabular}{|l|c|c|c|c|c|}
        \hline
        \textbf{Modello} & \textbf{Righe} & \textbf{Righe/msg} & \textbf{Msg/chat} & \textbf{Tempo} & \textbf{Falsi pos.} \\ \hline
        Llama3 8b & 100 & 3 & 1 & 01m 42s & $\approx$15\% \\ \hline
        Llama3 8b & 1000 & 3 & 1 & 14m 05s & $\approx$5\% \\ \hline
        Llama3.1 8b & 100 & 3 & 1 & 02m 27s & $\approx$5\% \\ \hline
        Llama3.1 8b & 1000 & 3 & 1 & 20m 26s & $\approx$5\% \\ \hline
        Llama3 70b & \multicolumn{5}{c|}{\textit{Memoria non sufficiente}} \\ \hline
        Llama3.1 70b & \multicolumn{5}{c|}{\textit{Memoria non sufficiente}} \\ \hline
        Command-R 35b & \multicolumn{5}{c|}{\textit{Memoria non sufficiente}} \\ \hline
    \end{tabular}
    \caption{Risultati della versione 1 su Apple M1 Max}
    \label{tab:risultati_v1}
\end{table}

I risultati mostrano che:
\begin{itemize}
    \item I modelli a 8 miliardi di parametri sono eseguibili con performance accettabili
    \item L'incremento del dataset da 100 a 1000 righe migliora l'accuratezza riducendo i falsi positivi
    \item I modelli più grandi (70b+ parametri) superano i limiti hardware disponibili
\end{itemize}

\subsection{Risultati Versione 2}
\label{subsec:risultati_v2}

La seconda versione ha introdotto metriche quantitative precise permettendo un confronto sistematico, come mostrato nella tabella \ref{tab:risultati_v2} e il grafico \ref{fig:grafico_risultati_v2}.

\begin{table}[h!]
    \centering
    \begin{tabular}{|l|c|c|c|c|c|c|}
        \hline
        \textbf{Modello} & \textbf{Righe} & \textbf{Tempo} & \textbf{VN} & \textbf{VP} & \textbf{FN} & \textbf{FP} \\ \hline
        llama3.1:8b & 100 & 00m 57s & $\approx$70\% & $\approx$14\% & $\approx$10\% & $\approx$6\% \\ \hline
        llama3.1:8b & 1000 & 09m 05s & $\approx$68\% & $\approx$14\% & $\approx$5\% & $\approx$13\% \\ \hline
        llama3.2:3b & 100 & 01m 08s & $\approx$64\% & $\approx$21\% & $\approx$3\% & $\approx$12\% \\ \hline
        llama3.2:3b & 1000 & 07m 15s & $\approx$59\% & $\approx$16\% & $\approx$2\% & $\approx$23\% \\ \hline
        mistral:7b & 100 & 06m 48s & $\approx$0\% & $\approx$24\% & $\approx$0\% & $\approx$76\% \\ \hline
        mistral:7b & 1000 & 72m 58s & $\approx$0\% & $\approx$19\% & $\approx$0\% & $\approx$81\% \\ \hline
        mistral-nemo:12b & 100 & 04m 10s & $\approx$45\% & $\approx$13\% & $\approx$11\% & $\approx$31\% \\ \hline
        mistral-nemo:12b & 1000 & 35m 15s & $\approx$48\% & $\approx$12\% & $\approx$7\% & $\approx$33\% \\ \hline
        qwen2.5:7b & 100 & 01m 45s & $\approx$76\% & $\approx$21\% & $\approx$3\% & $\approx$0\% \\ \hline
        qwen2.5:7b & 1000 & 16m 44s & $\approx$68\% & $\approx$18\% & $\approx$1\% & $\approx$13\% \\ \hline
        gemma3:4b & 100 & 01m 17s & $\approx$58\% & $\approx$21\% & $\approx$3\% & $\approx$18\% \\ \hline
        gemma3:12b & 100 & 03m 18s & $\approx$74\% & $\approx$23\% & $\approx$1\% & $\approx$2\% \\ \hline
        deepseek & todo & todo & todo & todo & todo & todo \\ \hline
        deepseek & todo & todo & todo & todo & todo & todo \\ \hline
    \end{tabular}
    \caption{Risultati della versione 2 con metriche quantitative}
    \label{tab:risultati_v2}
\end{table}

\todo{TODO: aggiungere i risultati dei modelli deepseek}
\todo{TODO: aggiungere i risultati dei modelli gemma con 1000 righe}

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            ybar,
            width=\textwidth,
            height=8cm,
            xlabel={Modelli LLM},
            ylabel={Accuratezza (\%)},
            symbolic x coords={llama3.1:8b, llama3.2:3b, mistral:7b, mistral-nemo:12b, qwen2.5:7b, gemma3:4b, gemma3:12b, deepseek:7b, deepseek:8b},
            xtick=data,
            x tick label style={rotate=45, anchor=east},
            legend pos=north west,
            bar width=15pt,
            ymin=0,
            ymax=100
        ]

                % Accuratezza su 100 righe (VN% + VP%)
        \addplot coordinates {
            (llama3.1:8b, 84)      % 70+14
            (llama3.2:3b, 85)      % 64+21
            (mistral:7b, 24)       % 0+24
            (mistral-nemo:12b, 58) % 45+13
            (qwen2.5:7b, 97)       % 76+21
            (gemma3:4b, 79)        % 58+21
            (gemma3:12b, 97)       % 74+23
            (deepseek:7b, 1)       % TODO: placeholder
            (deepseek:8b, 1)       % TODO: placeholder
        };

        % Accuratezza su 1000 righe (VN% + VP%) dove disponibile
        \addplot coordinates {
            (llama3.1:8b, 82)      % 68+14
            (llama3.2:3b, 75)      % 59+16
            (mistral:7b, 19)       % 0+19
            (mistral-nemo:12b, 60) % 48+12
            (qwen2.5:7b, 86)       % 68+18
            (gemma3:4b, 0)         % Non testato su 1000
            (gemma3:12b, 0)        % Non testato su 1000
            (deepseek:7b, 1)       % TODO: placeholder
            (deepseek:8b, 1)       % TODO: placeholder
        };

        \legend{100 righe, 1000 righe}
        \end{axis}
    \end{tikzpicture}
    \caption{Confronto dell'accuratezza dei modelli LLM sui dataset di test}
    \label{fig:grafico_risultati_v2}
\end{figure}


\subsection{Confronto dei modelli LLM utilizzati}
\label{sec:confronto_modelli}

L'analisi comparativa dei modelli rivela pattern interessanti:

\subsubsection{Modelli ad alte performance}
I modelli che hanno mostrato le migliori performance complessive sono:

\begin{itemize}
    \item \textbf{Qwen2.5:7b}: Eccellente combinazione di accuratezza ($\approx$94\% su 100 righe, $\approx$86\% su 1000 righe) e velocità di esecuzione
    \item \textbf{Gemma3:12b}: Migliore accuratezza ($\approx$97\% su 100 righe) ma tempi di esecuzione più elevati
    \item \textbf{Llama3.1:8b}: Performance equilibrate con buona scalabilità
\end{itemize}

\subsubsection{Modelli problematici}
Alcuni modelli hanno mostrato comportamenti problematici:

\begin{itemize}
    \item \textbf{Mistral:7b}: Tasso di falsi positivi estremamente elevato ($\approx$76-81\%), probabilmente dovuto a una configurazione di prompt non ottimale per questo modello specifico
    \item \textbf{Mistral-nemo:12b}: Performance mediocri nonostante le dimensioni maggiori
\end{itemize}

\subsection{Modelli reasoning}
\label{subsec:modelli_reasoning}

Alcuni modelli testati includono capacità di reasoning avanzate, utilizzando tag speciali per mostrare il processo di ragionamento interno. I modelli DeepSeek R1 (7B e 8B) utilizzano tag `<think>...</think>` per mostrare il ragionamento prima della risposta finale.

Il sistema implementa un meccanismo di post-processing per rimuovere automaticamente questi tag di reasoning e considerare solo la risposta finale, garantendo compatibilità con il formato di output atteso.

\section{Analisi delle performance}
\label{sec:analisi_performance}

\subsection{Relazione dimensioni-accuratezza}
\label{subsec:relazione_dimensioni_accuratezza}

L'analisi delle performance evidenzia diversi fattori critici per l'efficacia del sistema.

Contrariamente alle aspettative, i modelli più grandi non sempre offrono performance superiori:
\begin{itemize}
    \item Qwen2.5:7b supera modelli più grandi come Mistral-nemo:12b
    \item Llama3.2:3b mostra performance comparabili a modelli significativamente più grandi
    \item La qualità del fine-tuning sembra essere più importante delle dimensioni del modello
\end{itemize}

\subsection{Efficienza computazionale}
\label{subsec:efficienza_computazionale}

I risultati mostrano variazioni significative nell'efficienza computazionale:
\begin{itemize}
    \item \textbf{Modelli veloci}: Llama3.2:3b, Qwen2.5:7b, Gemma3:4b (< 2 minuti per 100 righe)
    \item \textbf{Modelli lenti}: Mistral:7b (> 6 minuti per 100 righe)
    \item \textbf{Relazione performance-velocità}: Non sempre i modelli più veloci sono meno accurati
\end{itemize}

\todosubsection{Limitazioni hardware}{subsec:limitazioni_hardware}{Rivalutare in seguito a test con altro hw}

L'hardware utilizzato (Apple M1 Max, 32GB RAM) ha posto vincoli significativi:
\begin{itemize}
    \item Impossibilità di eseguire modelli superiori a 12-15 miliardi di parametri
    \item Necessità di ottimizzazioni specifiche per l'architettura Apple Silicon
    \item Compromessi tra dimensioni del modello e velocità di inferenza
\end{itemize}

I risultati confermano la fattibilità dell'utilizzo di LLM per l'identificazione di dati sensibili, evidenziando l'importanza della selezione del modello appropriato in base ai requisiti specifici di accuratezza, velocità e risorse disponibili.



%
%			CAPITOLO: Conclusioni e sviluppi futuri
%

\todochapter{Conclusioni}{chap:conclusioni}{conclusioni}

\todosection{Risultati raggiunti}{sec:risultati_raggiunti}{finire}

Questo progesso ci ha permesso di confermare l'utilizzo di LLM per l'identificazione e il report di dati sensibili all'interno di log di sistemi complessi in un'ambiente di produzione.

\section{Sviluppi futuri}
\label{sec:sviluppi_futuri}

\subsection{Addestramento di LoRA}
\label{subsec:addestramento_lora}

LoRa (Low-Rank Adaptation) è una tecnica di fine-tuning efficiente che modifica solo un sottoinsieme dei parametri del modello e permette di creare un nuovo LLM a partire da uno già addestrato.

Tramite questa tecnica è possibile addestrare un modello per un dominio specifico, ad esempio il dominio dei log di sicurezza, senza dover addestrare completamente un nuovo modello da zero, operazione praticamente impossibile per progetti di ricerca accademica a causa dei costi computazionali elevati, dei tempi di sviluppo estremamente lunghi, delle competenze multidisciplinari richieste e della necessità di dataset di addestramento di dimensioni enormi.

In seguito a un periodo prolungato di utilizzo del sistema in ambiente di produzione e di raccolta sistematica dei dati e dei report generati, sarebbe possibile utilizzare questi dati come dataset di training per addestrare un LoRA specificamente ottimizzato per questo caso d'uso. \\
Il fine-tuning potrebbe partire da uno dei modelli LLM attualmente utilizzati che ha dimostrato le performance migliori durante la fase di testing.

\subsection{Integrazione con altri sistemi}
\label{subsec:integrazione_sistemi}

La versione finale del progetto è stata sviluppata specificamente per l'integrazione con GrayLog e lo script di setup dell'ambiente è stato scritto per interagire con questa piattaforma. \\
Un possibile sviluppo futuro consiste nella generalizzazione delle integrazioni attraverso lo sviluppo di un protocollo standard che consenta la creazione di adattatori modulari per diversi sistemi di gestione dei log.

Questa evoluzione permetterebbe di:
\begin{itemize}
    \item Estendere la compatibilità del sistema a piattaforme alternative come Elasticsearch, Splunk, o Fluentd
    \item Standardizzare le interfacce di comunicazione tra il sistema di analisi e i sistemi di logging
    \item Semplificare la distribuzione del sistema in ambienti enterprise eterogenei
    \item Migliorare la tracciabilità e la gestione delle notifiche attraverso un framework unificato
\end{itemize}

Un protocollo di integrazione standardizzato ridurrebbe significativamente la complessità di deployment e aumenterebbe l'adozione del sistema in contesti operativi diversificati.



\chapter{da organizzare (TODO DA RIMUOVERE)}
\label{chap:da_organizzare}

\section{Readme}
\label{sec:readme}

\begin{tabular}{|p{0.25\textwidth}|p{0.25\textwidth}|p{0.25\textwidth}|p{0.25\textwidth}|}
    \hline
    \textbf{Strumento}        & \textbf{Descrizione}                                                                                                                       & \textbf{Pro}                                                                                                                                                                                                  & \textbf{Contro}                                                                                                                                                                                                                                     \\ \hline
    Presidio                  & SDK per la protezione e anonimizzazione di dati privati in testi e immagini. Funziona tramite regex e NLP                                  & Ha il supporto alla ricerca tramite NLP e quindi dovrebbe essere più facile trovare dati non perfettamente rappresentabili con regex                                                                          & Lavorando con NLP potrebbe generare falsi negativi e falsi positivi                                                                                                                                                                                 \\ \hline
    anonympy                  & Libreria python per l'anonimizzazione di dati in tabelle, immagini e PDF                                                                   & È una libreria e quindi si può integrare facilmente in software custom                                                                                                                                        & Non è fatto per lavorare su testi semplici, anche se si potrebbe provare a modificare l'elaborazione dei PDF per renderlo possibile                                                                                                                 \\ \hline
    Data Protection Framework & Strumento molto simile a Microsoft Presidio, è una libreria Python che permette di trovare e anonimizzare dati privati tramite regex e NLP & Libreria FOSS integrabile in software custom oppure richiamabile direttamente da linea di comando                                                                                                             & Il progetto non è completo, mancano ancora alcuni detector                                                                                                                                                                                          \\ \hline
    NgAnonymize               & Libreria Angular per anonimizzare dati                                                                                                     & Supporta diversi metodo per anonimizzare i dati                                                                                                                                                               & I dati da anonimizzare devono essere specificati singolarmente, non ha alcuna feature di rilevamento dei dati da anonimizzare                                                                                                                       \\ \hline
    arx-deidentifier          & OSS per l'anonimizzazione di dati personali                                                                                                & FOSS offerto sia come sw completo che libreria Java. È stato sviluppato come ricerca universitaria e ha paper associati                                                                                       & Supporta solo dati tabulari                                                                                                                                                                                                                         \\ \hline
    loganalyzer               & FOSS software che trova e rimuove pattern pre-definiti da file di log                                                                      & Permette sia di rimuovere dati che di riportarli in un report, aiutando quindi la compilazione di un "punteggio" di sicurezza dei log. Scritto in C e quindi più veloce delle alternative in Angular o Python & Scritto in C, quindi il codice è più difficile da modificare. Non sembra essere offerto come libreria, quindi difficilmente integrabile in altri SW. Supporta solo ambienti grafici QT quindi la compilazione potrebbe dare problemi in base all'OS \\ \hline
\end{tabular}

\subsection*{Strumenti di raccoglimento e trattamento di log alternativi a Graylog}

\begin{tabular}{|p{0.3\textwidth}|p{0.7\textwidth}|}
    \hline
    \textbf{Strumento} & \textbf{Descrizione}                                                                  \\ \hline
    SigNoz             & Pannello per visualizzare traces, metriche e log di OpenTelemetry                     \\ \hline
    Logstash           & Pipeline di data processing che permette di trasformare log in ingresso               \\ \hline
    FluentD )          & Data collector da multiple sorgenti, OSS, supporta plugin                             \\ \hline
    Syslog-ng          & Implementazione FOSS di syslog, raccoglie, elabora e salva log da multiple sorgenti   \\ \hline
    Apache Flume       & Servizio per raccoglimento, aggregazione e spostamento di log. Basato su data streams \\ \hline
\end{tabular}



\clearpage



%
%			APPENDICE: materiali aggiuntivi e dimostrazioni
%

\appendix

\todochapter{Codice}{chap:appendice_codice}{riguardare e sistemare commenti nel codice}

\section{Versione 1: Proof of Concept}

\lstinputlisting[
  caption=Codice sorgente della versione 1,
label=app:code_ver1,
  language=Python,
  literate={è}{{\`e}}1 {à}{{\`a}}1 {ò}{{\`o}}1 {ù}{{\`u}}1 {ì}{{\`i}}1
]{"../ollama extractor v1/main.py"}


\lstinputlisting[
  caption=Prompt usato nella versione 1,
label=app:prompt_ver1,
  literate={è}{{\`e}}1 {à}{{\`a}}1 {ò}{{\`o}}1 {ù}{{\`u}}1 {ì}{{\`i}}1
]{"../ollama extractor v1/prompt.txt"}
\todo{sistemare testo che esce dai confini del box}

\clearpage

\section{Versione 2: Elaborazione di batch di log}

\lstinputlisting[
  caption=Codice sorgente della versione 2,
label=app:code_ver2,
  language=Python,
  literate={è}{{\`e}}1 {à}{{\`a}}1 {ò}{{\`o}}1 {ù}{{\`u}}1 {ì}{{\`i}}1
]{"../ollama extractor v2/main.py"}

\clearpage

\section{Versione 3: Integrazione con GrayLog}

\lstinputlisting[
  caption=Codice sorgente della versione 3,
label=app:code_ver3,
  language=Python,
  literate={è}{{\`e}}1 {à}{{\`a}}1 {ò}{{\`o}}1 {ù}{{\`u}}1 {ì}{{\`i}}1
]{"../ollama extractor v3/main.py"}

\clearpage

\section{\textit{Modelfile}}
\label{sec:code_modelfile}

\lstinputlisting[
caption=\textit{Modelfile} di partenza usato per generare i modelli personalizzati,
label=app:modelfile,
  language=Python,
  literate={è}{{\`e}}1 {à}{{\`a}}1 {ò}{{\`o}}1 {ù}{{\`u}}1 {ì}{{\`i}}1
]{"../modelfiles/sensitive-data-detector-BASE.modelfile"}
\todo{sistemare testo che esce dai confini del box}


%
%			BIBLIOGRAFIA
%

% Si può specificare a che livello della TOC deve essere la bibliografia.
% Il default è 'chapter', per 'part' usare
% \beforebibliography[part]
\beforebibliography
\bibliographystyle{unsrt}
\bibliography{bibliografia}
https://mistral.ai/news/mixtral-of-experts

% Pagina di chiusura tesi
% \closingpage

% Pagina bianca finale
% \clearpage
% \thispagestyle{empty}
% \null
% \clearpage

\end{document}

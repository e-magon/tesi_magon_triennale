%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                             %
%           TEMPLATE LATEX PER TESI                                           %
%           ______________                                                    %
%                                                                             %
%           Ultima revisione: 28 Novembre 2024                                %
%           Revisori: G.Presti; L.A.Ludovico; F. Avanzini; M. Tiraboschi      %
%                                                                             %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Pacchetti necessari per compilare questo documento:
% - babel
% - geometry
% - pgfplots
% - todonotes
% - xcolor

% Per compilare il documento eseguire:
% cd "elaborato"
% latexmk -pdf -f "tesi.tex"

\documentclass[12pt]{report}

% --- PREAMBOLO ---------------------------------------------------------------
% Inserire qui eventuali package da includere o
% definizioni di comandi personalizzati

% Selezione lingua
\usepackage[italian]{babel}

\usepackage{tesi}
% Puoi usare il font di default di LaTeX con la relativa opzione del package
% \usepackage[defaultfont]{tesi}
% Esiste anche un'opzione per il formato 17x24 per le tesi di dottorato
% \usepackage[phd]{tesi}

% Mostra i bounding box per visualizzare il layout (TODO rimuovere per versione finale)
\geometry{showframe}

% Per disabilitare i todo:
% \usepackage[disable]{todonotes}
% \usepackage{todonotes}

% Inclusione comandi personalizzati
\usepackage{todo-chapters}
\usepackage{refwithpage}

% Pacchetto per grafici con dati integrati
\usepackage{pgfplots}
\pgfplotsset{compat=1.18} % Compatibilità con la versione 1.18 di pgfplots

% In caso il copia-incolla del PDF generato perda gli spazi,
% provare a decommentare la seguente riga
% \pdfinterwordspaceon

% !!! INFORMAZIONI SULLA TESI DA COMPILARE !!!

%   UNIVERSITA' E CORSO DI LAUREA:
\university{Università degli Studi di Milano}
\unilogo{immagini/loghi/unimi}
\faculty{Facoltà di Scienze e Tecnologie}
\department{Dipartimento di Informatica\\Giovanni Degli Antoni}
\cdl{Corso di Laurea Triennale in\\Sicurezza dei Sistemi e delle Reti Informatiche}

%   TITOLO TESI:
\title{Tesi}
% Questo comando (opzionale) sovrascrive \title per quanto riguarda la copertina
% Può essere usato per stampare caratteri speciali, tenendo i metadati puliti
\printedtitle{(TODO: NON DEFINITIVO) \\
Identificazione automatica di dati sensibili all'interno di log di sistemi complessi tramite analisi statica e Large Language Models}

%   AUTORE:
\author{Emanuele Magon}
\matricola{909482}
% "Elaborato Finale" per i CdL triennali
% "Tesi di Laurea" per i CdL magistrali
\typeofthesis{Elaborato Finale}

%   RELATORE E CORRELATORE:
\relatore{Prof. Marco Anisetti}
\correlatore{Dott. Antongiacomo Polimeno}

%   LABORATORIO:
% Questa sezione crea una pagina di chiusura della tesi con
% il logo dell'ente/laboratorio presso cui si è svolto il tirocinio.
% Più afferenze/url/loghi sono supportate,
% e la frase può essere personalizzata.
% Qui trovate alcuni predefiniti del nostro dipartimento
% \adaptlab
% \aislab
% \anacletolab
% \bisplab
% \connetslab
% \everywarelab
% \falselab
% \iebilab
% \islab
% \lailalab
% \lalalab
% \lawlab
% \laserlab
% \limlab
% \mipslab
% \optlab
% \phuselab
% \ponglab
% \sesarlab
% \spdplab

% Esempio di personalizzazione della pagina di chiusura
% (non consegnate con questo esempio!)
% (da commentare in caso sia sufficiente una delle macro precedenti)
% \lab{Laboratorio di Ricerca}
% \lab[in collaborazione con l']{Azienda Specifica}
% \laburl{https://di.unimi.it/it/ricerca/risorse-e-luoghi-della-ricerca/laboratori-di-ricerca}
% \lablogo{immagini/redqmark}

% Con questo comando si può cambiare la dimensione (massima
% altezza e larghezza consentite) dei loghi
% \setlength\lablogosize{25mm}

%   ANNO ACCADEMICO
% \the\year inserisce l'anno corrente
% per specificare manualmente un anno accademico
% NON inserire nel formato 1970-1971, ma
% inserire solo 1970
\academicyear{2025}

%   INDICI:
% elenco delle figure (facoltativo)
% \figurespagetrue
% elenco delle tabelle (facoltativo)
% \tablespagetrue
% prefazioni nell'indice (facoltativo)
% \prefaceintoctrue
% indice nell'indice (facoltativo)
% \tocintoctrue

\setlength {\marginparwidth }{2cm}

% Stile dei blocchi di codice
\usepackage{xcolor}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.97,0.97,0.95}

\lstdefinestyle{codelistingstyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\scriptsize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=t,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=codelistingstyle}

% Cambia il nome usato nelle caption del codice
\renewcommand{\lstlistingname}{Sorgente}
\renewcommand{\lstlistlistingname}{Elenco dei sorgenti}

% --- FINE PREAMBOLO ----------------------------------------------------------

\begin{document}

% Creazione automatica della copertina
% Centra la copertina nel foglio: usa questo comando per la copertina esterna
\makecenteredfrontpage
% Copertina allineata alle altre pagine: usa questo comando per la copertina interna
% \makefrontpage

%
%			PREFAZIONE (facoltativa)
%

% \prefacesection{Prefazione}
% Le prefazioni non sono molto comuni, tuttavia a volte capita che qualcuno voglia dire qualcosa che esuli dal lavoro in sé (come un meta-commento sull'elaborato), o voglia fornire informazioni riguardanti l'eventuale progetto entro cui la tesi si colloca (in questo caso è probabile che sia il relatore a scrivere questa parte).

%
%			RINGRAZIAMENTI (facoltativi)
%

\todoprefacesection{Ringraziamenti}{ringraziamenti}
Questa sezione, facoltativa, contiene i ringraziamenti.

%
%			Creazione automatica dell'indice
%

\afterpreface



%
%			CAPITOLO: Introduzione o Abstract
%

\todochapter{Introduzione}{chap:introduzione}{inserire riassunto che spiega velocemente ogni capitolo cosa contiene}

L'obiettivo di questo lavoro è sviluppare un sistema automatico per l'identificazione e la notificazione della presenza di dati sensibili all'interno di log di grandi dimensioni, fornendo agli amministratori di sistema una soluzione efficace per il monitoraggio della conformità alla privacy. \\
Sebbene il sistema sia progettato per essere generico e applicabile a diverse tipologie di log, il presente lavoro è stato sviluppato e testato specificamente per l'integrazione nel mondo dell'\textit{IoT}, nello specifico per una piattaforma complessa di gestione domotica di \textit{smart city}, composta da svariati micro-servizi eterogenei.

L'approccio progettuale adottato integra un sistema ibrido che combina due metodologie complementari per l'identificazione di dati sensibili nei log. La soluzione utilizza espressioni regolari per l'identificazione efficiente di dati sensibili strutturati e ben definiti (come codici fiscali, numeri di telefono, indirizzi email, JWT\footnote{JWT (JSON Web Token) è uno standard RFC 7519 per la trasmissione sicura di informazioni tra parti come in file JSON firmati digitalmente. In genere è utilizzato per l'autenticazione e l'autorizzazione di utenti.}, coordinate geografiche etc.) insieme a Large Language Models che, con la loro capacità di comprensione contestuale, permettono il rilevamento di informazioni sensibili non strutturate o espresse in linguaggio naturale. \\
Inoltre, gli LLM sono stati impiegati durante la fase di sviluppo per assistere nell'identificazione di dati sensibili all'interno di campioni di log, facilitando la progettazione di espressioni regolari specifiche e ottimizzate per i pattern identificati.

Questa architettura ibrida permette di sfruttare i vantaggi di entrambe le tecnologie: l'efficienza computazionale e il determinismo delle espressioni regolari per pattern ricorrenti e la flessibilità semantica degli LLM per l'analisi di contenuti più complessi che potrebbero sfuggire a un approccio puramente basato su regex.


%
%			CAPITOLO: Stato dell'arte
%

\todochapter{Stato dell'arte}{chap:stato_arte}{stato dell'arte}
\todo{TODO provare a chiedere a chatGPT aiuto su come cercare paper in google scholar e simili (che keyword usare ecc.)}

TODO da scrivere

\begin{itemize}
    \item Log deformati, fare ricerca su soluzioni attuali su fonti autorevoli (paper con scholar.google.com). Se proprio non si trova si possono usare siti (se autoritevoli)
    \item Parlare di llm orientati nel nostro caso specifico
    \item Stato dell'arte dei log in generale: perché fare raccolta di log, raccorta centralizzata etc.
    \item Stato dell'arte su analisi sicurezza e lettura dei log etc.
    \item Vedere se si trova nello stato dell'arte soluzioni già presenti, simili o diverse
\end{itemize}

\todosection{Sicurezza nei log}{sec:sicurezza_log}{finire}

\todosection{Identificazione di dati sensibili nei log}{sec:identificazione_dati_sensibili}{finire}

\todosection{Soluzioni esistenti per l'analisi dei log}{sec:soluzioni_esistenti}{finire}

\section{Large Language Models}
\label{sec:llm}

I Large Language Models (LLM) rappresentano una delle applicazioni più avanzate e innovative del \textit{deep learning} nel campo dell'elaborazione del linguaggio naturale. Il \textit{deep learning} costituisce un sottoinsieme del machine learning che si basa su reti neurali artificiali con molteplici livelli di elaborazione, capaci di apprendere rappresentazioni gerarchiche sempre più complesse dei dati~\cite{shinde2018review_ml_dl}. Mentre le applicazioni tradizionali del \textit{deep learning} includono il riconoscimento di immagini (\textit{computer vision}), il riconoscimento vocale e l'analisi predittiva, i Large Language Models si sono affermati come strumenti particolarmente potenti per la comprensione e generazione di testo~\cite{shinde2018review_ml_dl}.

Gli LLM sono modelli di intelligenza artificiale addestrati su enormi quantità di testo per apprendere le strutture linguistiche, i pattern semantici e le relazioni contestuali presenti nel linguaggio umano. La loro architettura si basa tipicamente su reti neurali \textit{transformer}\footnote{Transformers \url{https://aws.amazon.com/what-is/transformers-in-artificial-intelligence}}, introdotte nel 2017, che rappresentano un'innovazione fondamentale nel campo del \textit{deep learning}. I \textit{transformer} utilizzano un meccanismo di auto-attenzione (\textit{self-attention}) che permette al modello di valutare l'importanza relativa di ogni parola rispetto alle altre in una frase, indipendentemente dalla loro posizione. Questo consente di elaborare sequenze di testo in parallelo anziché sequenzialmente, migliorando significativamente l'efficienza computazionale e la capacità di comprendere le relazioni tra parole anche distanti tra loro nel testo.

Prima di utilizzare un LLM, il testo viene convertito in \textbf{token}, che rappresentano le unità base di elaborazione. Un token può corrispondere a una parola intera, una porzione di parola (come una sillaba), o anche singoli caratteri, a seconda del metodo di \textit{tokenizzazione} adottato. Il \textbf{\textit{context window}} definisce il numero massimo di token che il modello può elaborare in una singola interazione: ad esempio, un \textit{context window} di 8.192 token consente al modello di analizzare circa 6.000-8.000 parole contemporaneamente, a seconda della lingua e della complessità del testo. Questo limite determina quanta informazione il modello può considerare nel contesto della conversazione o del documento in esame.

Il numero di \textbf{parametri} di un modello, espresso tipicamente in miliardi (\textit{B}), rappresenta invece la quantità di connessioni neurali che il modello ha appreso durante la fase di addestramento. Un modello con 7 miliardi di parametri (7B) ha memorizzato 7 miliardi di \textit{pesi} che determinano come elaborare e interpretare il testo. Modelli con più parametri sono generalmente più adatti a comprendere sfumature linguistiche complesse ed a gestire compiti diversificati, ma richiedono anche maggiori risorse di memoria e potenza di calcolo per funzionare.

\subsection{Machine learning}
\label{subsec:machine_learning}

Il machine learning rappresenta una branca dell'intelligenza artificiale che consente ai sistemi informatici di apprendere e migliorare le proprie prestazioni attraverso l'esperienza, senza essere esplicitamente programmati per ogni specifica situazione~\cite{shinde2018review_ml_dl}. Le sue origini risalgono agli anni '50 del XX secolo, quando ricercatori come Alan Turing e Arthur Samuel iniziarono a esplorare la possibilità di creare macchine capaci di apprendere autonomamente.

Le reti neurali artificiali costituiscono uno dei paradigmi fondamentali del machine learning. Queste architetture computazionali si ispirano al funzionamento del sistema nervoso biologico, organizzando unità di elaborazione artificiali (neuroni) interconnesse in strati. Ogni neurone riceve input, li elabora attraverso funzioni matematiche e propaga il risultato agli strati successivi. Attraverso processi iterativi di addestramento, la rete modifica i pesi delle connessioni tra neuroni per minimizzare l'errore tra output predetto e risultato atteso, apprendendo così a riconoscere pattern complessi nei dati.

Nel corso del tempo il machine learning si è affermato come strumento fondamentale nell'analisi dei dati, trovando applicazioni in numerosi settori quali il riconoscimento di pattern, la classificazione di documenti, i sistemi di raccomandazione e l'analisi predittiva. Gli algoritmi di machine learning hanno dimostrato efficacia significativa in contesti dove i dati presentano strutture relativamente semplici e ben definite.

L'avvento del deep learning negli ultimi anni ha segnato una svolta nel campo dell'intelligenza artificiale~\cite{shinde2018review_ml_dl}. Questa evoluzione del machine learning si basa su reti neurali artificiali con molteplici livelli di elaborazione, capaci di apprendere rappresentazioni gerarchiche sempre più complesse dei dati. I modelli di deep learning eccellono particolarmente nell'elaborazione di linguaggio naturale, nel riconoscimento di immagini e nella comprensione contestuale, superando significativamente le prestazioni degli approcci tradizionali in task che richiedono una comprensione semantica profonda~\cite{elkhatiba2024performance_benchmarking}.

I Large Language Models rappresentano l'applicazione più avanzata di questa tecnologia nel dominio dell'elaborazione del linguaggio, dimostrando capacità notevolmente superiori nella comprensione e generazione di testo rispetto ai metodi tradizionali, specialmente in compiti di classificazione complessi~\cite{kostina2025llm_text_classification}. Tuttavia, la loro efficacia dipende fortemente dalla capacità di comprendere feature contestuali, un aspetto che può variare significativamente tra modelli pre-addestrati e modelli fine-tuned~\cite{zhu2024can_llm_context}.

\subsection{Modelli proprietari e modelli open weight}
\label{subsec:modelli_proprietari_openweight}

I Large Language Model possono essere suddivisi in due categorie principali: modelli proprietari e modelli \textit{open weight}.

I modelli proprietari sono sviluppati e mantenuti da aziende tecnologiche di rilievo internazionale, come, per esempio, da \textbf{OpenAI}\footnote{OpenAI \url{https://openai.com}}, che offre i modelli GPT (Generative Pre-trained Transformer); \textbf{Anthropic}\footnote{Anthropic \url{https://www.anthropic.com}}, che offre i modelli Claude; \textbf{Google}\footnote{Google AI \url{https://ai.google}}, che offre i modelli Gemini e \textbf{Meta}\footnote{Meta AI \url{https://ai.meta.com}}, che offre i modelli Llama.

Questi modelli vengono eseguiti su server appositi e gli utenti possono accedervi tramite interfacce web, applicazioni desktop o mobile, oppure attraverso \textit{API} per l'integrazione programmatica in applicazioni personalizzate. L'accesso ai modelli proprietari comporta generalmente costi basati sul numero di token elaborati o su sottoscrizioni mensili, con diversi LLM a disposizione in base alle esigenze di utilizzo.

I modelli \textit{open weight}, al contrario, sono resi disponibili pubblicamente con i pesi pre-addestrati. Questo approccio permette agli utenti di scaricare i modelli e eseguirli sul proprio hardware o su server dedicati, utilizzando software specializzati come Ollama\footnote{Ollama \url{https://ollama.com}}, llama.cpp\footnote{llama.cpp \url{https://github.com/ggerganov/llama.cpp}}, o LM Studio\footnote{LM Studio \url{https://lmstudio.ai}}~\cite{bendiouis2024deploying_opensource}.

L'esecuzione locale di modelli \textit{open weight} offre vantaggi significativi in termini di privacy, eliminando la necessità di trasmettere dati sensibili a servizi esterni, e di costi operativi, evitando le tariffe per token tipiche dei servizi cloud~\cite{bendiouis2024deploying_opensource}. Tuttavia, questa modalità richiede hardware adeguato in termini di memoria (RAM e VRAM) e capacità di calcolo, specialmente per modelli di grandi dimensioni.~\cite{bendiouis2024deploying_opensource}.

È importante distinguere tra diversi livelli di apertura nel contesto dei modelli linguistici. I modelli \textit{open source} forniscono accesso completo al codice sorgente, ai dati di addestramento e all'architettura del modello, permettendo modifiche complete e ridistribuzione. I modelli \textit{open weight}, pur fornendo i pesi pre-addestrati, non sempre includono i dataset di addestramento o documentazione completa sui processi di training. I modelli \textit{open data} condividono i dataset utilizzati per l'addestramento, facilitando la riproducibilità della ricerca ma non necessariamente i pesi del modello finale.

Alcuni provider di modelli proprietari, come Google\footnote{Google Gemma \url{https://deepmind.google/models/gemma}} e Meta\footnote{Meta Llama \url{https://www.llama.com}}, rilasciano anche modelli \textit{open weight}, permettendo agli utenti di eseguire localmente versioni ridotte o specifiche dei loro modelli principali. \\
Questo approccio ibrido consente di beneficiare delle capacità avanzate dei modelli proprietari, pur mantenendo il controllo sui dati e i costi associati all'esecuzione.

La ricerca accademica ha evidenziato che, sebbene i modelli proprietari tendano a mostrare performance superiori in benchmark generali, modelli \textit{open weight} opportunamente ottimizzati possono raggiungere risultati comparabili in task specifici come la comprensione di testi~\cite{alassan2024comparison_opensource_proprietary}. La valutazione delle performance deve inoltre considerare non solo l'accuratezza, ma anche aspetti pratici come la riproducibilità dei risultati e la trasparenza dei processi di valutazione~\cite{white2024livebench}. In particolare, benchmark come LiveBench sono stati sviluppati per fornire valutazioni più eque e resistenti alla contaminazione del test set, problematica che può favorire artificialmente modelli addestrati su dataset che includono i dati di test~\cite{white2024livebench}. Per una comparativa dei modelli proprietari e open weight, si rimanda alla Figura~\refwithpage{fig:livebench_comparison}.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.9\textwidth]{immagini/white2024livebench-1.png}
	\caption{Confronto delle performance di modelli proprietari e open weight su LiveBench~\cite{white2024livebench}}
	\label{fig:livebench_comparison}
\end{figure}


\todosection{Large Language Models nell'analisi dei testi}{sec:llm_analisi_testi}{finire}



%
%			CAPITOLO: Metodologia teorica
%

\todochapter{Metodologia}{chap:metodologia_teorica}{metodologia teorica}

\todosection{Architettura proposta}{sec:architettura_proposta}{grafico di flusso dell'architettura}

TODO qua una descrizione generale della soluzione teorica, che utilizza sia llm che regex per l'analisi di log alla ricerca di dati sensibili. \\
Spiegare come gli LLM sono stati usati sia come supporto per definire le regex per l'analisi statica, sia come strumento attivo per l'analisi di log in tempo reale.

\todo{TODO completare}

\todosection{Approccio all'identificazione dei dati sensibili}{sec:approccio_identificazione}{approccio all'identificazione dei dati sensibili}

L'approccio metodologico del progetto prevede l'utilizzo dei Large Language Models sia come strumento di supporto per la definizione e la validazione delle espressioni regolari, sia come componente attiva integrata nel processo di analisi in tempo reale.

\todosubsection{Utilizzo di LLM per supportare la definizione di regex}{subsec:llm_definizione_regex}{verificare e nel caso cambiare tempi verbali}

La strategia si basa sul principio che l'identificazione di dati sensibili strutturati (come codici fiscali, numeri di telefono, indirizzi email, JWT, coordinate geografiche etc.) può essere efficacemente realizzata tramite espressioni regolari opportunamente progettate. \\
Gli LLM, in questo contesto, svolgono un ruolo di \textit{consulenza esperta} durante la fase di sviluppo, contribuendo a identificare pattern complessi nei campioni di log, a validare e ottimizzare le espressioni regolari analizzando le righe non catturate per verificare la presenza di dati sensibili non identificati e a gestire i casi limite individuando varianti e formati \textit{edge-case} che potrebbero sfuggire alle regex iniziali.
\todo{TODO migliorare questo paragrafo}

\subsubsection{Processo di verifica e ottimizzazione delle regex assistito da LLM}
Il workflow previsto per la generazione delle espressioni regolari comprende:

\begin{enumerate}
    \item \textbf{Analisi esplorativa}: Utilizzo di LLM per analizzare campioni di log e identificare potenziali dati sensibili, inclusi formati non convenzionali
    \item \textbf{Categorizzazione}: Classificazione dei dati sensibili identificati in categorie omogenee (credenziali, PII, token, ecc.)
    \item \textbf{Generazione iterativa}: Creazione progressiva di regex specifiche per ogni categoria, con il supporto dell'LLM per gestire variazioni e casi speciali
    \item \textbf{Test e raffinamento}: Applicazione delle regex su dataset di test più ampi e analisi tramite LLM delle righe non catturate, con l'obiettivo di raggiungere un punto in cui l'LLM non identifichi più dati sensibili nelle righe non rilevate dalle regex
    \item \textbf{Consolidamento}: Creazione di un set finale di espressioni regolari ottimizzate e pronte per l'utilizzo in produzione
\end{enumerate}

\subsubsection{Limitazioni e sfide identificate}
In questo approccio si identificano diverse sfide significative:

\begin{itemize}
    \item \textbf{Complessità dei pattern}: Alcuni tipi di dati sensibili, specialmente quelli espressi in linguaggio naturale, risultano difficilmente catturabili tramite regex
    \item \textbf{Contesto semantico}: Le espressioni regolari non possono comprendere il contesto semantico in cui un dato appare, potendo generare falsi positivi
    \item \textbf{Evoluzione continua}: La necessità di aggiornamento continuo delle regex per adattarsi a nuovi formati e variazioni nei log
    \item \textbf{Manutenzione complessa}: La gestione di un insieme crescente di regex specializzate comporta complessità di manutenzione significative
\end{itemize}

Queste considerazioni vengono in parte affrontate dalla scelta di un sistema ibrido, descritto nella sezione~\refwithpage{subsec:ibrido_llm_regex}, che mantiene i vantaggi delle espressioni regolari integrandoli con le capacità di comprensione contestuale degli LLM.
\todo{TODO migliorare questo paragrafo}

\subsection{Integrazione ibrida di LLM e regex per l'identificazione in tempo reale}
\label{subsec:ibrido_llm_regex}

L'approccio metodologico proposto integra un sistema ibrido che combina l'efficienza computazionale delle espressioni regolari con la flessibilità semantica dei Large Language Models, creando una soluzione ibrida per l'identificazione di dati sensibili in tempo reale.

\subsubsection{Principio architetturale}

L'architettura ibrida si basa sul principio di complementarietà tra le due tecnologie.

Il primo livello ha un ruolo di analisi sintattica, cioè di identificazione rapida e deterministica di pattern strutturati ben definiti mediante espressioni regolari (email, token di autenticazione, coordinate geografiche, identificatori numerici, etc.).

Il secondo livello ha invece un ruolo di analisi semantica, quindi di comprensione contestuale di contenuti complessi tramite modelli linguistici, riuscendo quindi a rilevare dati sensibili espressi in linguaggio naturale o formati non convenzionali.

\subsubsection{Modello operativo}

Il flusso di elaborazione si articola concettualmente nelle seguenti fasi:

\begin{enumerate}
    \item \textbf{Acquisizione}: I log vengono acquisiti dal sistema centralizzato di gestione

    \item \textbf{Pre-elaborazione sintattica}: Un primo layer applica espressioni regolari ottimizzate per identificare pattern comuni e ad alta occorrenza. I log contenenti corrispondenze positive vengono immediatamente marcati per notifica rapida

    \item \textbf{Instradamento intelligente}: I log (sia quelli già marcati che quelli non catturati dalle espressioni regolari) vengono inoltrati al sistema di analisi semantica

    \item \textbf{Analisi contestuale}: Il modello linguistico analizza ogni unità di log per identificare:
    \begin{itemize}
        \item Dati sensibili non rilevati dall'analisi sintattica iniziale
        \item Varianti e formattazioni non standard di dati già categorizzati
        \item Informazioni sensibili espresse in linguaggio naturale
        \item Contesti in cui dati apparentemente innocui assumono natura sensibile
    \end{itemize}

    \item \textbf{Consolidamento}: Le identificazioni del modello linguistico vengono associate agli identificatori univoci dei log originali e strutturate per l'integrazione nel sistema di gestione

    \item \textbf{Aggregazione}: Il sistema centrale aggrega i risultati provenienti da entrambi i layer di analisi, creando una vista unificata di tutti i dati sensibili identificati e attivando eventuali meccanismi di notifica
\end{enumerate}

\subsubsection{Vantaggi dell'approccio ibrido}

L'integrazione di analisi sintattica e semantica offre diversi vantaggi rispetto a soluzioni basate su una singola tecnologia:

\begin{itemize}
    \item \textbf{Copertura completa}: La combinazione delle due metodologie permette di identificare sia dati strutturati che non strutturati

    \item \textbf{Riduzione dei falsi negativi}: I dati che sfuggono all'analisi sintattica vengono catturati dalla comprensione contestuale dei modelli linguistici

    \item \textbf{Scalabilità economica}: L'uso selettivo dei modelli linguistici solo per i casi complessi mantiene contenuti i costi computazionali

    \item \textbf{Flessibilità evolutiva}: Nuovi pattern identificati dai modelli linguistici possono essere formalizzati in espressioni regolari per ottimizzare elaborazioni future

    \item \textbf{Resilienza}: Il fallimento di un layer non compromette completamente la capacità di identificazione del sistema

    \item \textbf{Adattabilità}: Il sistema può essere calibrato in base alle esigenze specifiche, privilegiando velocità o accuratezza mediante la regolazione del bilanciamento tra i due layer
\end{itemize}

\subsubsection{Considerazioni metodologiche}

L'approccio ibrido proposto richiede alcune considerazioni metodologiche fondamentali per garantirne l'efficacia operativa.

\paragraph{Separazione delle preoccupazioni}
La chiara distinzione tra analisi sintattica e semantica costituisce un principio architetturale fondamentale che permette di ottimizzare indipendentemente ciascun layer secondo le proprie caratteristiche specifiche.

Questa separazione consente di sostituire o aggiornare singoli componenti senza impattare il funzionamento dell'intero sistema, facilitando l'evoluzione tecnologica e la manutenzione a lungo termine. Inoltre, tale approccio modulare permette di valutare separatamente le performance di ogni metodologia attraverso metriche appropriate e di adattare dinamicamente la distribuzione del carico computazionale in base alle risorse disponibili e ai requisiti operativi.

\paragraph{Granularità dell'analisi}
La scelta metodologica di elaborare singolarmente ogni unità di log, anziché processarle in batch, è motivata da diverse considerazioni architetturali. In termini di precisione, l'elaborazione unitaria garantisce che ogni riga riceva un'analisi dedicata senza interferenze da contesto irrilevante, massimizzando l'accuratezza dell'identificazione. Dal punto di vista della tracciabilità, questo approccio assicura una corrispondenza diretta e biunivoca tra input e risultato dell'analisi, semplificando il debugging e l'audit dei processi. La latenza risulta inoltre controllata e uniforme, con tempi di risposta prevedibili che facilitano la pianificazione delle risorse. Infine, l'isolamento garantito dall'elaborazione unitaria previene la propagazione a cascata di eventuali errori o anomalie, aumentando la resilienza complessiva del sistema.

Questa architettura metodologica rappresenta la base teorica per un sistema di identificazione efficace ed efficiente, bilanciando precisione, prestazioni e manutenibilità, e costituisce il fondamento concettuale di \textit{Sensitive Data Detector}.


%
%			CAPITOLO: Implementazione e tecnologie
%

\chapter{Implementazione e tecnologie}
\label{chap:implementazione_tecnologie}

Il progetto \textit{Sensitive Data Detector} integra diverse tecnologie per creare una soluzione completa di analisi automatica dei log. L'architettura si basa su un approccio multi-componente che combina diversi strumenti.

Questo capitolo descrive nel dettaglio le tecnologie scelte, motivando le decisioni architetturali e spiegando come ogni componente contribuisce al funzionamento complessivo del sistema.

\todo{TODO aggiungere un diagramma del sistema}

\section{Scenario applicativo}
\label{sec:scenario_applicativo}
La necessità di implementare questo sistema è emersa dalle esigenze operative di un'azienda che opera nel settore dell'\textit{IoT}, sviluppando e integrando diversi micro-servizi destinati alla gestione domotica di \textit{smart city}. \\
L'integrazione di una moltitudine di sistemi software eterogenei ha comportato un incremento significativo sia del volume che della complessità dei log generati. \\
Inoltre, la natura composta, le dimensioni e la provenienza da fornitori terzi di molti di questi sistemi hanno reso impraticabile intervenire direttamente sul codice sorgente per eliminare alla fonte la generazione di log contenenti dati sensibili, introducendo quindi la necessità di un sistema automatico, scalabile e preciso per l'analisi dei log.

\clearpage

\subsection{Formato dei log}
\label{subsec:formato_log}

I log del sistema seguono il formato strutturato definito nella tabella~\refwithpage{tab:formato_log}. \\
Il sorgente~\refwithpage{lst:esempio_log} presenta un campione di cinque righe di log rappresentativo della struttura dati utilizzata.

\begin{table}[h!]
    \centering
    \begin{tabular}{|p{0.11\textwidth}|p{0.25\textwidth}|p{0.27\textwidth}|p{0.25\textwidth}|}
        \hline
        \textbf{Nome} & \textbf{Formato}   & \textbf{Descrizione}                                                    & \textbf{Esempio}                    \\
        \hline
        Nome file     & Stringa            & Nome del file di log compresso                                          & `messages.2.gz`                     \\
        \hline
        Data          & `MMM dd HH:mm:ss`  & Timestamp nel formato syslog                                            & `Apr  7 00:03:39`                   \\
        \hline
        ID            & `ID` + numero      & Identificativo univoco dell'istanza del software che ha generato il log & `ID24167`                           \\
        \hline
        Host+PID      & Stringa + `[PID]:` & Nome host e process ID del processo che ha generato il log              & `sw3-devaccess|55dacabaa 607[561]:` \\
        \hline
        Body          & Oggetto JSON       & Contenuto strutturato con `level` e `message`                           & `\{"level":"info",

        "message":"..."\}`                                                                                                                                 \\
        \hline
    \end{tabular}
    \caption{Formato strutturato dei log del sistema}
    \label{tab:formato_log}
\end{table}

\lstinputlisting[
caption=Esempio di log,
label=lst:esempio_log,
literate={è}{{\`e}}1 {à}{{\`a}}1 {ò}{{\`o}}1 {ù}{{\`u}}1 {ì}{{\`i}}1 {µ}{{$\mu$}}1
]{"../example_logs/raw_logs.txt"}

Poiché gli eventuali dati sensibili sono presenti esclusivamente nel contenuto testuale dei log, il sistema concentra l'analisi unicamente sul campo \texttt{message} del body JSON\footnote{JSON (JavaScript Object Notation) è un formato standard per la rappresentazione di dati strutturati.}, ignorando gli altri elementi strutturali.

\section{Elaborazione}
\label{sec:elaborazione_log}

Il livello di elaborazione costituisce il cuore del sistema ed è responsabile della raccolta, gestione e analisi dei log. Le tecnologie in questo layer sono state selezionate per garantire scalabilità, affidabilità e precisione nell'identificazione dei dati sensibili.

\subsection{GrayLog}
\label{subsec:graylog}

GrayLog\footnote{GrayLog \url{https://graylog.org}} rappresenta la piattaforma centrale per la gestione dei log del sistema. \\
Si tratta di una soluzione enterprise e open-source che fornisce funzionalità avanzate di raccolta, indicizzazione, ricerca e analisi dei log in tempo reale.

La scelta di GrayLog è stata motivata principalmente dal fatto che questa piattaforma viene già utilizzata nell'ambiente di produzione per il quale è stato sviluppato questo progetto, rendendo quindi l'integrazione di quest'ultimo una naturale estensione dell'infrastruttura esistente.

\subsubsection{Caratteristiche principali}
Oltre al vantaggio dell'integrazione con l'ambiente esistente, le caratteristiche tecniche che rendono GrayLog ideale per questo progetto includono:

\begin{itemize}
    \item \textbf{Raccolta centralizzata}: Supporto nativo per diversi protocolli di logging (GELF\footnote{GELF (GrayLog Extended Log Format) è un formato strutturato progettato specificamente per il trasporto di log su reti TCP/UDP, che estende il formato syslog standard con campi aggiuntivi e supporto nativo per metadati.}, Syslog\footnote{Syslog è un protocollo standard per il logging di sistema definito dall'RFC 3164 (\url{https://datatracker.ietf.org/doc/html/rfc3164}), ampiamente utilizzato per la trasmissione di messaggi di log su reti IP.}) che permette l'integrazione con sistemi eterogenei
    \item \textbf{Indicizzazione real-time}: Capacità di indicizzare e rendere ricercabili i log immediatamente dopo la ricezione
    \item \textbf{Pipeline di processing}: Sistema di regole configurabili per trasformare, filtrare e arricchire i log durante l'ingestion
    \item \textbf{API REST complete}: Interfacce programmatiche per l'automazione della configurazione e l'integrazione con sistemi esterni
    \item \textbf{Scalabilità orizzontale}: Architettura distribuita che supporta cluster multi-nodo per gestire volumi elevati di log
\end{itemize}

\subsubsection{Integrazione nel progetto}
Nel contesto del \textit{Sensitive Data Detector}, GrayLog svolge un ruolo duplice:

\begin{enumerate}
    \item \textbf{Sorgente di dati}: Riceve log da sistemi di produzione tramite input configurabili sulla porta \texttt{TCP 5555}
    \item \textbf{Destinazione per notifiche}: Riceve le segnalazioni di dati sensibili identificati dal sistema tramite la porta \texttt{TCP 5556}
\end{enumerate}

La configurazione utilizza stream e output GELF per creare una pipeline che instrada i log in ingresso verso il \textit{Sensitive Data Detector} tramite output GELF sulla porta \texttt{TCP 24367}, raccoglie le notifiche di ritorno per visualizzazione e alerting e applica regole regex aggiuntive per l'identificazione rapida di pattern comuni.

\subsection{Ollama}
\label{subsec:ollama}

Ollama\footnote{Ollama \url{https://ollama.com}} è un framework open-source progettato per semplificare l'esecuzione locale di Large Language Models.\\
Esso offre un'interfaccia uniforme per l'integrazione e l'orchestrazione di differenti modelli LLM.
\todo{TODO aggiungere tutti gli LLM utilizzati in tutte le versioni, con link a sito/modello e spiegazione}

\subsubsection{Architettura e funzionalità}
Ollama offre un'architettura client-server che include:

\begin{itemize}
    \item \textbf{Server locale}: Processo daemon che gestisce il caricamento e l'esecuzione dei modelli LLM
    \item \textbf{API REST}: Interfaccia HTTP standardizzata per l'interazione con i modelli caricati
    \item \textbf{Gestione dei modelli}: Comandi per il download, l'aggiornamento e la configurazione di modelli da repository pubblici
    \item \textit{\textbf{Modelfile}}: Formato di configurazione che permette la creazione di modelli personalizzati (a partire da modelli già esistenti) con prompt di sistema ad-hoc incorporati. Per ulteriori dettagli riguardanti i \textit{Modelfile} si rimanda alla sezione~\refwithpage{subsec:llm}.
    \item \textbf{Ottimizzazioni hardware}: Supporto per accelerazione GPU e ottimizzazioni specifiche per diverse architetture (CPU, Metal\footnote{Metal è l'API grafica e di calcolo di Apple per macOS e iOS.} su macOS, CUDA\footnote{CUDA è la piattaforma di calcolo parallelo di NVIDIA per GPU.}/ROCM\footnote{ROCM è la piattaforma open-source di AMD per calcolo su GPU.} su Windows e GNU/Linux)
\end{itemize}

\subsubsection{Vantaggi per il progetto}
La scelta di Ollama ha portato diversi vantaggi architetturali:

\begin{itemize}
    \item \textbf{Deployment locale}: Eliminazione della dipendenza da servizi cloud esterni, preservando la privacy dei dati ed evitando la trasmissione di informazioni sensibili a terze parti
    \item \textbf{Costi operativi ridotti}: Assenza di costi per token o chiamate API, permettendo elaborazioni intensive
    \item \textbf{Latenza ottimizzata}: Comunicazione locale che riduce significativamente i tempi di risposta
    \item \textbf{Flessibilità dei modelli}: Possibilità di sperimentare con diversi modelli senza vincoli commerciali
\end{itemize}

\subsection{Large Language Models}
\label{subsec:llm}

Il sistema utilizza un approccio multi-modello per massimizzare l'accuratezza nell'identificazione di dati sensibili. La strategia prevede l'utilizzo di modelli personalizzati ottimizzati specificamente per il dominio dei log di sistema.

\subsubsection{Modelli supportati}
Il progetto supporta nove diversi modelli LLM selezionati per bilanciare performance, accuratezza e requisiti computazionali. I modelli sono elencati nella tabella~\refwithpage{tab:modelli_llm}.

\begin{table}[h!]
    \centering
    \begin{tabular}{|p{0.15\textwidth}|p{0.18\textwidth}|p{0.52\textwidth}|}
        \hline
        \textbf{Nome} & \textbf{Num. di token} & \textbf{Caratteristiche principali}                  \\ \hline
        Llama 3.1     & 8B                     & Modello generale con ottimo rapporto qualità/performance   \\ \hline
        Llama 3.2     & 3B                     & Versione compatta per ambienti con risorse limitate  \\ \hline
        Mistral       & 7B, 12B                & Modelli europei con architettura Mixture of Experts  \\ \hline
        Qwen 2.5      & 7B                     & Modello multilingue trainato su dati di svariati tipi \\ \hline
        Gemma 3       & 4B, 12B                & Modelli Google con ottimizzazioni per task specifici \\ \hline
        DeepSeek R1   & 7B, 8B                 & Modelli con capacità di reasoning avanzate           \\ \hline
    \end{tabular}
    \caption{Modelli LLM supportati dal sistema}
    \label{tab:modelli_llm}
\end{table}

I modelli Mistral sono stati sviluppati seguendo l'architettura \textit{Mixture of Experts}, cioè un approccio di machine learning che suddivide un modello di intelligenza artificiale in sottoreti specializzate chiamate "esperti", ognuna delle quali si concentra su un sottoinsieme specifico dei dati di input.~\cite{mu2025moe_survey}~\cite{ibm_moe}

Mistral ha implementato questo approccio utilizzando 8 gruppi distinti di parametri: a ogni livello e per ogni token, una rete di instradamento seleziona due di questi esperti per elaborare il token, permettendo di utilizzare solo 12,9 miliardi di parametri sui 46,7 miliardi totali e ottenendo prestazioni superiori con maggiore efficienza computazionale.~\cite{mistral_moe}

I modelli DeepSeek R1 includono capacità di reasoning\footnote{Il reasoning negli LLM si riferisce alla capacità di generare esplicitamente il processo di ragionamento step-by-step che porta alla risposta finale. Questi modelli utilizzano tag speciali (come \texttt{<think>...</think>}) per separare il processo di ragionamento interno dalla risposta definitiva, permettendo maggiore trasparenza nel processo decisionale.} avanzate che permettono di visualizzare il processo di ragionamento interno del modello.

\subsubsection{Personalizzazione tramite \textit{Modelfile}}
Ogni modello viene personalizzato attraverso dei file di configurazione \textit{Modelfile} che includono:

\begin{itemize}
    \item \textbf{Prompt di sistema}: Istruzioni specifiche per l'identificazione di dati sensibili nei log
    \item \textbf{Parametri ottimizzati}: Configurazione di temperatura\footnote{La temperatura è un parametro che controlla la casualità delle risposte dell'LLM: valori bassi (0.0-0.3) producono risposte più deterministiche e conservative, mentre valori alti (0.7-1.0) aumentano la creatività ma riducono la coerenza.} e context window\footnote{Il context window rappresenta il numero massimo di token (parole o porzioni di parole) che un LLM può processare simultaneamente in una singola conversazione, inclusi prompt, messaggi precedenti e risposta generata.} per massimizzare la precisione
    \item \textbf{Esempi di training}: Messaggi di esempio che guidano il modello nel formato di output atteso
    \item \textbf{Definizioni precise}: Specifiche dettagliate su cosa costituisce un dato sensibile e cosa escludere dall'analisi
\end{itemize}

Il prompt di sistema incorpora linee guida specifiche per:
\begin{itemize}
    \item Identificazione di email, token di autenticazione, coordinate geografiche
    \item Esclusione di dati non sensibili come username, ID, timestamp
    \item Estrazione selettiva dei valori sensibili da oggetti JSON complessi
    \item Formato di output standardizzato per facilità di parsing
\end{itemize}

Per ulteriori dettagli riguardanti i \textit{Modelfile} e la configurazione dei LLM si rimanda alla sezione~\refwithpage{subsubsec:ver2_prompt_engineering}.


\section{Codice}
\label{sec:codice}

Il livello di implementazione si basa su tecnologie consolidate che garantiscono affidabilità, manutenibilità e portabilità del sistema.

\subsection{Python}
\label{subsec:python}

Python\footnote{Python \url{https://www.python.org}} rappresenta il linguaggio di programmazione principale del progetto, scelto per la sua versatilità e l'ampio ecosistema di librerie specializzate per l'integrazione con sistemi di AI.

\subsubsection{Versione e compatibilità}
Il progetto utilizza Python 3.12, che garantisce:
\begin{itemize}
    \item Supporto completo per programmazione asincrona con \texttt{asyncio}
    \item Ottimizzazioni di performance per applicazioni I/O intensive
    \item Compatibilità con le librerie più recenti dell'ecosistema AI
    \item Funzionalità avanzate di type hinting per migliorare la manutenibilità del codice
\end{itemize}

\subsubsection{Dipendenze principali}
Le librerie Python utilizzate includono:

\begin{itemize}
    \item \textbf{ollama}: Libreria ufficiale per l'interazione con il server Ollama tramite API REST
    \item \textbf{httpx}: Client HTTP asincrono per comunicazioni non bloccanti
    \item \textbf{anyio}: Framework per programmazione asincrona cross-platform
    \item \textbf{requests}: Per configurazione automatica di GrayLog tramite API REST
\end{itemize}

\subsubsection{Il progetto \textit{Sensitive Data Detector}}
Il core del sistema è implementato in tre versioni evolutive che rappresentano l'evoluzione del progetto dalla dimostrazione di fattibilità all'integrazione enterprise.

La \textbf{prima versione} costituisce un proof of concept per validare l'uso di LLM nell'identificazione di dati sensibili.

La \textbf{seconda versione} introduce sistemi di valutazione quantitativa e modelli personalizzati per migliorare scalabilità e accuratezza.

La \textbf{terza versione} implementa l'integrazione real-time con GrayLog per deployment in ambiente di produzione.

Per una descrizione completa dell'architettura, delle funzionalità e dei risultati di ogni versione si rimanda al capitolo~\refwithpage{chap:sensitive_data_detector}.

\subsubsection{Script di inizializzazione GrayLog}
Il file \texttt{graylog\_setup.py} automatizza la configurazione iniziale di GrayLog tramite le API REST per la creazione programmatica di input, stream, output e pipeline; adotta una configurazione idempotente che consente esecuzioni ripetute senza effetti collaterali; include meccanismi di gestione degli errori con validazione delle configurazioni e, dove previsto, rollback automatico; e supporta template personalizzabili per adattare le configurazioni a diversi ambienti.

In particolare, lo script provvede alla definizione degli input \texttt{TCP} per i log in ingresso e per le notifiche di ritorno, alla creazione di stream per il routing intelligente dei messaggi, alla configurazione di output GELF per la comunicazione con il \textit{Sensitive Data Detector} e all'impostazione di pipeline con regole regex per l'identificazione rapida di pattern comuni.

Il codice completo dello script è disponibile nell'appendice~\refwithpage{lst:code_graylog_script}.

\subsection{Docker}
\label{subsec:docker}

Docker\footnote{Docker \url{https://www.docker.com}} viene utilizzato per la containerizzazione dell'ambiente GrayLog, garantendo portabilità, isolamento e semplicità di deployment.

\subsubsection{Architettura multi-container}
La configurazione Docker Compose definisce un ambiente completo che include:

\begin{itemize}
    \item \textbf{MongoDB}\footnote{MongoDB \url{https://www.mongodb.com}}: Database per memorizzazione dei metadati di configurazione di GrayLog
    \item \textbf{GrayLog DataNode}: Nodo di indicizzazione per storage e ricerca dei log
    \item \textbf{GrayLog Enterprise}: Server principale con interfaccia web e API REST
\end{itemize}

Il codice completo della configurazione del container è disponibile nell'appendice~\refwithpage{lst:code_container_config}.

\subsubsection{Configurazione di rete e persistenza}
L'ambiente Docker implementa:

\begin{itemize}
    \item \textbf{Rete isolata}: Bridge network dedicata per comunicazione sicura tra container
    \item \textbf{Volumi persistenti}: Storage permanente per i dati di MongoDB, indici GrayLog e le configurazioni
    \item \textbf{Port mapping}: Esposizione selettiva delle porte necessarie per l'integrazione esterna
    \item \textbf{Health checks}: Monitoraggio automatico dello stato dei servizi con restart policies
\end{itemize}

\subsubsection{Vantaggi operativi}
L'utilizzo di Docker porta diversi benefici, tra cui un deployment semplificato che consente l'avvio dell'intero stack con un singolo comando, l'isolamento delle dipendenze per prevenire conflitti con software preesistente sull'host, la portabilità cross-platform con comportamento consistente su Windows, macOS e Linux, la scalabilità orizzontale per replicare l'ambiente a fini di testing o di deployment distribuito e una gestione puntuale delle versioni dei componenti utilizzati.


%
%			CAPITOLO: Il lavoro svolto
%

\chapter{Sensitive Data Detector}
\label{chap:sensitive_data_detector}

Il progetto \textit{Sensitive Data Detector} rappresenta il cuore del sistema ed è stato sviluppato attraverso un processo iterativo che ha portato alla realizzazione di tre versioni evolutive, ognuna con obiettivi specifici e caratteristiche tecniche distinte.

Questo approccio progressivo ha permesso di affrontare le sfide tecniche in modo sistematico, partendo dalla validazione del concetto di base fino all'implementazione di una soluzione pronta per il rilascio in produzione. Ogni versione introduce significativi miglioramenti in termini di scalabilità, accuratezza e facilità di integrazione in ambienti esistenti rispetto alla precedente.

\textbf{Versione 1: Proof of Concept} \\
La prima implementazione costituisce un proof of concept sviluppato per dimostrare la fattibilità dell'utilizzo di Large Language Models nell'identificazione automatica di dati sensibili all'interno dei file di log.

Questa versione utilizza un'architettura sincrona con elaborazione batch, interfacciandosi direttamente con l'API Ollama per analizzare file di log pre-esistenti. Il sistema impiega LLM base senza personalizzazione, guidati esclusivamente da un prompt iniziale esterno e implementa strategie di gestione manuale del context window con ripristino periodico.

L'output viene salvato su file per consentire analisi offline e tracciabilità del processo.

Ulteriori dettagli sono disponibili nella sezione~\refwithpage{sec:ver1}.

\textbf{Versione 2: Streaming e valutazione} \\
La seconda versione rappresenta un'evoluzione significativa con focus sulla scalabilità e la valutazione quantitativa delle performance. Introduce l'elaborazione riga per riga dei log per ottimizzare l'uso della memoria e utilizza modelli LLM personalizzati creati tramite \textit{Modelfile} di Ollama, che incorporano prompt specifici per l'identificazione di dati sensibili.

Un aspetto fondamentale di questa versione è l'implementazione di un sistema di ground truth\footnote{Ground truth indica i dati di riferimento considerati corretti e utilizzati come standard per valutare la precisione di un sistema di classificazione. Nel contesto di questo progetto, consiste in etichette manuali che indicano se ogni riga di log contiene o meno dati sensibili.} per la valutazione automatica, che consente il calcolo real-time di metriche di classificazione binaria (veri/falsi positivi/negativi) e l'analisi comparativa di diversi modelli LLM.

Ulteriori dettagli sono disponibili nella sezione~\refwithpage{sec:ver2}.

\textbf{Versione 3: Integrazione enterprise} \\
La terza versione costituisce la soluzione \textit{production-ready} del progetto, implementando un'architettura asincrona completa per l'integrazione real-time con GrayLog. \\
Utilizza programmazione asincrona con \texttt{asyncio} per gestione concorrente delle connessioni, protocollo GELF per comunicazione nativa con la piattaforma di gestione log e un server TCP non bloccante per elaborazione in tempo reale.

Il sistema implementa un feedback loop bidirezionale che reinvia automaticamente i risultati delle analisi a GrayLog, permettendo la visualizzazione unificata di log originali e notifiche di rilevamento di dati sensibili attraverso le sue dashboard specializzate.

Ulteriori dettagli sono disponibili nella sezione~\refwithpage{sec:ver3}.


\section{Versione 1: Proof of Concept}
\label{sec:ver1}

La prima versione del \textit{Sensitive Data Detector} rappresenta un proof of concept sviluppato per dimostrare la fattibilità dell'utilizzo di Large Language Models nell'identificazione automatica di dati sensibili all'interno dei file di log. \\
Questa versione costituisce la base concettuale dell'intero progetto, implementando un approccio diretto e semplificato per l'analisi di file di log di dimensioni contenute.

\subsubsection{Obiettivi e scopo}
\label{subsubsec:ver1_obiettivi}

L'obiettivo principale di questa prima implementazione è verificare l'efficacia dei modelli LLM nell'identificazione di informazioni sensibili presenti nei log di sistema. La versione si concentra sull'analisi di file di log pre-esistenti, utilizzando un approccio batch che processa il contenuto in modo sequenziale.

Il sistema è progettato per riconoscere diverse tipologie di dati sensibili, tra i quali \textbf{informazioni personali identificabili} come i PII, \textbf{credenziali di accesso e token di autenticazione}, \textbf{indirizzi email e informazioni di contatto}, e \textbf{chiavi crittografiche e certificati}.

\subsubsection{Architettura e funzionamento}
\label{subsubsec:ver1_architettura}

Il sistema è implementato tramite linguaggio Python e utilizza la libreria \texttt{ollama}\footnote{\texttt{ollama} \url{https://pypi.org/project/ollama}} per interfacciarsi con diversi modelli LLM tramite API REST. L'architettura segue un paradigma client-server dove lo script Python agisce come client che invia richieste al server Ollama locale. L'implementazione completa è disponibile nell'appendice~\refwithpage{lst:code_ver1}.

Il flusso di elaborazione si articola nelle seguenti fasi:

\begin{enumerate}
    \item \textbf{Inizializzazione}: Lo script carica il prompt di sistema da un file esterno, lo invia all'LLM come messaggio e configura i parametri specifici per il modello selezionato
    \item \textbf{Lettura dei log}: Il sistema legge il file di log dall'archivio di esempi e lo suddivide in blocchi di dimensioni arbitrarie specifiche per il modello selezionato
    \item \textbf{Elaborazione batch}: Ogni blocco viene inviato all'LLM per l'analisi
    \item \textbf{Raccolta risultati}: Le risposte del modello vengono aggregate e formattate
    \item \textbf{Salvataggio output}: I risultati vengono salvati in file per consentire tracciabilità e analisi successive
\end{enumerate}

\subsubsection{Gestione del context window}
\label{subsubsec:ver1_context_window}

Un aspetto critico dell'implementazione riguarda la gestione del context window limitato dei modelli LLM. Il sistema implementa due strategie principali: \textbf{Segmentazione dei dati}, in cui i file di log vengono suddivisi in blocchi di dimensioni adattive basate sulle capacità del modello, e \textbf{Ripristino periodico della conversazione}, per cui, dopo un numero predefinito di messaggi, la cronologia della chat viene ripristinata per evitare il superamento dei limiti di token. Questo approccio garantisce che il modello mantenga sempre accesso al prompt iniziale e alle linee guida per l'identificazione dei dati sensibili.

\subsubsection{Prompt engineering}
\label{subsubsec:ver1_prompt_engineering}

Il prompt di sistema è stato progettato per guidare il modello nell'identificazione precisa dei dati sensibili, fornendo \textbf{definizioni chiare} di cosa costituisce un dato sensibile, \textbf{esempi specifici} di dati da identificare come token, credenziali e informazioni personali, \textbf{esclusioni esplicite} per dati non sensibili (quali username, ID e timestamp), un \textbf{formato di output strutturato} in bullet point e un \textbf{esempio pratico} con input e output atteso.

Il prompt include inoltre istruzioni per la gestione di oggetti JSON, richiedendo al modello di estrarre solo i dati sensibili contenuti all'interno di essi e non l'intero oggetto. \\
Il prompt completo è disponibile nell'appendice~\refwithpage{lst:prompt_ver1}.

\subsubsection{Modelli supportati e configurazione}
\label{subsubsec:ver1_modelli_supportati}

La prima versione supporta cinque diversi modelli LLM, ognuno con parametri di configurazione ottimizzati, come mostrato nella tabella~\refwithpage{tab:modelli_llm_ver1}.

\begin{table}[h!]
    \centering
    \begin{tabular}{|l|c|r|c|}
        \hline
        \textbf{Modello} & \textbf{Parametri} & \textbf{Righe per messaggio} & \textbf{Reset chat} \\ \hline
        Llama3           & 8B                 & 3                            & Ogni messaggio      \\ \hline
        Llama3           & 70B                & 10                           & Ogni 5 messaggi     \\ \hline
        Llama3.1         & 8B                 & 3                            & Ogni messaggio      \\ \hline
        Llama3.1         & 70B                & 10                           & Ogni 5 messaggi     \\ \hline
        Command-R        & 35B                & 5                            & Ogni 5 messaggi     \\ \hline
    \end{tabular}
    \caption{Configurazioni ottimizzate per modello nella versione 1}
    \label{tab:modelli_llm_ver1}
\end{table}
\todo{TODO: ri-verificare tabella}

\todo{TODO spiegare che il numero di righe per messaggio e reset chat è stato deciso a tentativi, notando che tenendolo troppo alto gli LLM perdevano il prompt iniziale e quindi finivano a non rispettare più la richiesta originale Con riferimento a sottosezione subsec metodologia\_v1}

La distinzione dei parametri riflette le diverse capacità computazionali e di context window dei modelli. I modelli più grandi con 70 miliardi di parametri possono processare blocchi più ampi di dati e mantenere conversazioni più lunghe prima del ripristino del contesto.

\subsubsection{Output e tracciabilità}
\label{subsubsec:ver1_output}

Il sistema genera file di output timestampati che includono \textbf{metadati dell'esecuzione} come timestamp, modello utilizzato e file di log analizzato, il \textbf{prompt iniziale utilizzato}, le \textbf{risposte complete dell'LLM} e \textbf{statistiche di elaborazione} quali righe processate e tempo di esecuzione.

Questa struttura consente la tracciabilità completa del processo di analisi e facilita il confronto tra diversi modelli e configurazioni.

\subsubsection{Limitazioni della versione}
\label{subsubsec:ver1_limitazioni}

La prima versione presenta alcune limitazioni che hanno orientato lo sviluppo delle versioni successive. La \textbf{scalabilità limitata} costituisce una criticità significativa, poiché il processamento batch sequenziale non è ottimale per file di log di grandi dimensioni. Inoltre, la \textbf{mancanza di integrazione real-time} rappresenta un vincolo funzionale rilevante, dato che il sistema opera esclusivamente su file pre-esistenti senza capacità di elaborazione in tempo reale. Infine, l'\textbf{assenza di metriche di valutazione} impedisce una valutazione oggettiva delle performance, non essendo implementati meccanismi automatici per misurare l'accuratezza delle identificazioni.

Nonostante queste limitazioni, la prima versione ha fornito risultati promettenti che hanno giustificato lo sviluppo delle versioni successive più avanzate.

\section{Versione 2: Elaborazione di batch di log}
\label{sec:ver2}

La seconda versione del \textit{Sensitive Data Detector} rappresenta un'evoluzione significativa del proof of concept, introducendo un sistema di valutazione automatica delle performance e l'utilizzo di modelli LLM personalizzati ottimizzati per il compito specifico.

Questa versione si concentra sull'elaborazione efficiente di file di log di dimensioni maggiori tramite streaming delle righe e implementa metriche di valutazione quantitative.

\subsubsection{Obiettivi e scopo}
\label{subsubsec:ver2_obiettivi}

L'obiettivo principale di questa seconda implementazione è duplice: da un lato migliorare la scalabilità del sistema per gestire volumi di log più elevati, dall'altro introdurre un framework di valutazione automatica per misurare obiettivamente le performance dei modelli LLM.

La versione introduce diversi miglioramenti sostanziali: l'\textbf{elaborazione riga per riga} per ottimizzare l'uso della memoria e semplificare il processo di rilevazione della correttezza delle risposte, l'\textbf{utilizzo di modelli personalizzati} con prompt engineering avanzato, un \textbf{sistema di ground truth} per la valutazione automatica e il \textbf{calcolo di metriche di classificazione binaria} quali veri/falsi positivi/negativi e accuratezza.

\subsubsection{Architettura e funzionamento}
\label{subsubsec:ver2_architettura}

L'architettura della seconda versione mantiene il paradigma client-server della versione precedente e al contempo introduce ottimizzazioni nel flusso di elaborazione. \\
Il sistema utilizza modelli LLM personalizzati creati tramite \textit{Modelfile}, una funzionalità di Ollama, che incorporano prompt specifici e configurazioni ottimizzate per l'identificazione di dati sensibili.\\
L'implementazione completa è disponibile nell'appendice~\refwithpage{lst:code_ver2}.
\todo{TODO specificare meglio cosa sono i modelfile, assicurarsi che "funzionalità di Ollama" non sia presente altrove. In caso, spostare la spiegazione in quel punto}

Il flusso di elaborazione ottimizzato si articola nelle seguenti fasi:

\begin{enumerate}
    \item \textbf{Inizializzazione del dataset}: Il sistema carica file di log pre-annotati con etichette ground truth (Y/N) per ogni riga
    \item \textbf{Streaming sequenziale}: Ogni riga viene processata individualmente, eliminando la necessità di caricare grandi volumi in memoria
    \item \textbf{Preprocessamento}: Le etichette di ground truth vengono estratte e conservate per la valutazione, mentre il contenuto della riga di log viene inviato al modello
    \item \textbf{Analisi LLM}: Il modello personalizzato analizza la singola riga e produce una classificazione binaria (sensibile/non sensibile)
    \item \textbf{Post-processing}: Le risposte vengono elaborate per rimuovere eventuali tag di reasoning e standardizzare il formato
    \item \textbf{Valutazione in tempo reale}: Ogni risposta viene immediatamente confrontata con il ground truth e le metriche vengono aggiornate
    \item \textbf{Gestione della memoria}: Il contesto viene ripristinato periodicamente per mantenere performance ottimali indefinitamente, anche su file di log di grandi dimensioni
\end{enumerate}

\subsubsection{Prompt engineering}
\label{subsubsec:ver2_prompt_engineering}

La seconda versione introduce un approccio di prompt engineering più sofisticato attraverso l'utilizzo di modelli personalizzati. \\
A differenza della prima versione, dove il prompt veniva inviato ad ogni interazione, i modelli personalizzati incorporano le istruzioni direttamente nella loro configurazione tramite \textit{Modelfile}.

Questa strategia offre vantaggi significativi: garantisce \textbf{consistenza} mantenendo il prompt sempre presente nel contesto del modello ed eliminando la variabilità dovuta alla gestione manuale, migliora l'\textbf{efficienza} riducendo la lunghezza dei messaggi inviati e ottimizzando l'utilizzo del context window, permette la \textbf{specializzazione} ottimizzando ogni modello con prompt specifici per le sue caratteristiche architetturali e supporta il \textbf{reasoning avanzato} per modelli con capacità di ragionamento che utilizzano tag speciali come \texttt{<think>}.

Il sistema implementa anche un meccanismo di post-processing per gestire le risposte dei modelli reasoning, rimuovendo automaticamente i tag \texttt{<think>...</think>} che contengono il processo di ragionamento interno del modello, considerando solamente la risposta finale.

\subsubsection{Modelli supportati e configurazione}
\label{subsubsec:ver2_modelli_supportati}

La seconda versione espande significativamente il supporto per diversi modelli LLM, introducendo nove modelli personalizzati ottimizzati specificamente per l'identificazione di dati sensibili. \\
Tutti i modelli condividono una configurazione unificata ottimizzata per l'elaborazione sequenziale di singole righe di log, come mostrato nella tabella~\refwithpage{tab:modelli_llm_ver2}.

\begin{table}[h!]
    \centering
    \begin{tabular}{|l|c|r|c|}
        \hline
        \textbf{\textit{Modelfile} basato su} & \textbf{Parametri} & \textbf{Righe per messaggio} & \textbf{Reset chat} \\ \hline
        Llama3.1                              & 8B                 & 1                            & Ogni 15 messaggi    \\ \hline
        Llama3.2                              & 3B                 & 1                            & Ogni 15 messaggi    \\ \hline
        Mistral                               & 7B                 & 1                            & Ogni 15 messaggi    \\ \hline
        Mistral Nemo                          & 12B                & 1                            & Ogni 15 messaggi    \\ \hline
        Qwen2.5                               & 7B                 & 1                            & Ogni 15 messaggi    \\ \hline
        Gemma3                                & 4B                 & 1                            & Ogni 15 messaggi    \\ \hline
        Gemma3                                & 12B                & 1                            & Ogni 15 messaggi    \\ \hline
        DeepSeek R1                           & 7B                 & 1                            & Ogni 15 messaggi    \\ \hline
        DeepSeek R1                           & 8B                 & 1                            & Ogni 15 messaggi    \\ \hline
    \end{tabular}
    \caption{Configurazioni ottimizzate per modello nella versione 2 e 3}
    \label{tab:modelli_llm_ver2}
\end{table}
\todo{TODO: ri-verificare tabella}

A differenza della prima versione, tutti i modelli utilizzano una configurazione unificata con elaborazione di una riga per messaggio e ripristino della conversazione ogni 15 messaggi. Questa standardizzazione facilita il confronto diretto delle performance tra modelli diversi e ottimizza l'utilizzo delle risorse computazionali. \\
Inoltre, l'elaborazione di singole righe consente una valutazione immediata dell'accuratezza delle classificazioni prodotte dal modello.
\todo{TODO spiegare che gestendo una riga alla volta si ottiene una risposta "decisa" dall'LLM per ogni singola riga. Mandando più righe alla volta bisogna parsare la risposta per capire quali righe l'LLM reputa sensibili. Inoltre aumenta il rischio che l'LLM non risponda correttamente o che salti qualche riga}

\subsubsection{Output e tracciabilità}
\label{subsubsec:ver2_output}

La seconda versione introduce un sistema di output e tracciabilità migliorato, con particolare focus sulle metriche di valutazione quantitative. Il sistema genera report dettagliati che includono sia le informazioni di base dell'esecuzione che analisi statistiche delle performance di classificazione.

I file di output contengono le seguenti informazioni:
\begin{itemize}
    \item \textbf{Metadati dell'esecuzione}:
        \begin{itemize}
            \item timestamp
            \item modello utilizzato
            \item file di log analizzato
            \item numero totale di righe processate
        \end{itemize}
    \item \textbf{Metriche di classificazione}:
        \begin{itemize}
            \item veri positivi
            \item veri negativi
            \item falsi positivi
            \item falsi negativi
        \end{itemize}
    \item \textbf{Statistiche di accuratezza}:
        \begin{itemize}
            \item percentuale di risposte corrette
            \item percentuale di risposte errate
        \end{itemize}
    \item \textbf{Log dettagliato}: traccia completa di tutte le interazioni con l'LLM
\end{itemize}

Il sistema calcola e visualizza in tempo reale le metriche di performance durante l'elaborazione, fornendo feedback immediato sull'efficacia del modello. \\
Questa funzionalità è particolarmente utile per confronti comparativi tra diversi modelli e per l'ottimizzazione dei parametri di configurazione.

\subsubsection{Limitazioni della versione}
\label{subsubsec:ver2_limitazioni}

Nonostante i significativi miglioramenti rispetto alla prima versione, la seconda implementazione presenta ancora alcune limitazioni che hanno guidato lo sviluppo della versione successiva.

La principale criticità riguarda la \textbf{dipendenza da dataset pre-annotati}, in quanto il sistema di valutazione richiede file di log manualmente etichettati, limitando la scalabilità dell'approccio di testing. Inoltre, la modalità di \textbf{elaborazione offline} rappresenta un vincolo significativo, poiché il sistema opera esclusivamente su file pre-esistenti senza capacità di integrazione con sistemi di logging in tempo reale.

Queste limitazioni, sebbene non compromettano l'utilità della versione per scopi di ricerca e testing, evidenziano la necessità di un'integrazione più robusta con sistemi di produzione, obiettivo che viene affrontato nella terza versione del progetto.

\section{Versione 3: Integrazione con GrayLog}
\label{sec:ver3}

La terza versione del \textit{Sensitive Data Detector} rappresenta l'evoluzione finale del progetto, trasformando il sistema da uno strumento di analisi offline a una soluzione completamente integrata per l'elaborazione in tempo reale dei log.

Questa versione si interfaccia direttamente con GrayLog, creando un sistema \\
end-to-end per il monitoraggio continuo dei log.

\subsubsection{Obiettivi e scopo}
\label{subsubsec:ver3_obiettivi}

L'obiettivo principale della terza versione è fornire una soluzione pronta per il rilascio in produzione per l'identificazione automatica di dati sensibili in ambienti operativi reali. Questa implementazione affronta le principali limitazioni delle versioni precedenti introducendo funzionalità chiave per contesti enterprise.

Il sistema implementa innanzitutto l'\textbf{elaborazione in tempo reale}, permettendo l'analisi immediata dei log non appena vengono generati dai sistemi. L'interfacciamento nativo con GrayLog consente l'inserimento immediato negli stack tecnologici esistenti, mentre l'architettura asincrona garantisce la \textbf{scalabilità operativa} necessaria per gestire volumi elevati di log in produzione. Il sistema include inoltre un \textbf{feedback loop} attraverso il quale i risultati delle analisi vengono automaticamente rimandati a GrayLog, permettendo di visualizzarli tramite interfaccia integrata. Infine, la \textbf{configurazione centralizzata} consente la gestione unificata di input, processing pipeline e output direttamente attraverso l'interfaccia GrayLog.

\subsubsection{Architettura e funzionamento}
\label{subsubsec:ver3_architettura}

L'architettura della terza versione introduce un cambio del paradigma rispetto alle versioni precedenti, passando da un modello client-server a un sistema basato su stream processing. Il sistema opera come un micro-servizio che si posiziona tra GrayLog e i modelli LLM, fungendo da ponte per l'analisi dei log. \\
L'implementazione completa è disponibile nell'appendice~\refwithpage{lst:code_ver3}.

Il flusso di elaborazione real-time si articola nelle seguenti fasi:

\begin{enumerate}
    \item \textbf{Ricezione stream}: Il sistema ascolta sulla porta \texttt{TCP 24367} per ricevere log in formato GELF da GrayLog
    \item \textbf{Parsing dei messaggi}: Ogni messaggio GELF viene deserializzato per estrarre contenuto e metadati del log
    \item \textbf{Analisi LLM}: Il contenuto viene inviato al modello personalizzato per l'identificazione di dati sensibili
    \item \textbf{Gestione risultati}: Le risposte positive vengono elaborate e formattate con gli identificatori originali del log
    \item \textbf{Notifica a GrayLog}: I risultati vengono reinviati a GrayLog sulla porta \texttt{TCP 5556} per integrazione nei flussi di alerting
    \item \textbf{Gestione della memoria}: Il contesto conversazionale dell'LLM viene periodicamente ripristinato per mantenere performance costanti
\end{enumerate}

La comunicazione avviene tramite protocollo GELF, che garantisce la preservazione di tutti i metadati necessari per la tracciabilità e la risoluzione di eventuali problemi.

\todo{TODO: esempio di messaggio GELF (vedere se magari spostare in capitolo 4)}

\subsubsection{Integrazione con GrayLog}
\label{subsubsec:ver3_integrazione}

L'integrazione con GrayLog avviene attraverso una configurazione di input, stream e output che crea una pipeline completa di elaborazione:

\begin{itemize}
    \item \textbf{Input \texttt{TCP 5555}}: Riceve i log originali dai sistemi sorgente
    \item \textbf{Stream di routing}: Indirizza i log verso il Sensitive Data Detector tramite output GELF
    \item \textbf{Input \texttt{TCP 5556}}: Riceve le notifiche di dati sensibili dal sistema di analisi
    \item \textbf{Pipeline di processing}: Applica regole aggiuntive per categorizzazione e alerting
\end{itemize}

Questa architettura permette di mantenere separati i log originali dalle notifiche di sicurezza, facilitando la creazione di dashboard specializzate e la configurazione di alert specifici per la conformità ai regolamenti sulla privacy.

\subsubsection{Modelli supportati e configurazione}
\label{subsubsec:ver3_modelli_supportati}

La terza versione eredita il supporto completo per tutti i modelli personalizzati della versione precedente, mantenendo la stessa configurazione unificata e ottimizzata per l'elaborazione in tempo reale, come mostrato nella tabella~\refwithpage{tab:modelli_llm_ver2}.

La configurazione real-time mantiene l'elaborazione di un messaggio per volta per semplificare il processo di rilevazione della correttezza delle risposte. \\
L'analisi di un messaggio alla volta permette inoltre di ottimizzare la latenza di risposta, aspetto critico in ambienti di produzione dove la velocità di identificazione delle vulnerabilità può essere determinante per la sicurezza del sistema.

\subsubsection{Programmazione asincrona e scalabilità}
\label{subsubsec:ver3_asincrona}

La terza versione introduce l'uso di programmazione asincrona tramite \texttt{asyncio}\footnote{\texttt{asyncio} \url{https://docs.python.org/3/library/asyncio.html}}, permettendo la gestione concorrente di multiple connessioni senza bloccare l'elaborazione. Questa architettura offre vantaggi significativi in termini di performance e affidabilità.

Il sistema implementa \textbf{gestione concorrente} per elaborazione parallela di log provenienti da fonti multiple, garantisce \textbf{resilienza} attraverso isolamento degli errori per singola connessione senza impatto sul sistema globale, ottimizza l'\textbf{efficienza delle risorse} mediante utilizzo ottimale della CPU attraverso I/O non bloccante, e permette \textbf{scalabilità orizzontale} con possibilità di deployment di multiple istanze per load balancing.

\subsubsection{Output e tracciabilità}
\label{subsubsec:ver3_output}

Il sistema di output della terza versione è progettato per l'integrazione operativa, producendo messaggi strutturati che vengono inviati direttamente in GrayLog.

Il sistema genera \textbf{notifiche real-time} in cui ogni identificazione di dato sensibile produce immediatamente una notifica a GrayLog, garantisce la \textbf{preservazione degli identificatori} mantenendo il collegamento con i log originali tramite ID univoci, utilizza un \textbf{formato standardizzato} con output strutturato compatibile con pipeline di processing esistenti e implementa il \textbf{logging delle operazioni} per tracciatura completa delle attività ai fini di audit e debugging.

Le notifiche seguono il formato: \\
\texttt{<log\_id>: <tipo\_dato\_sensibile> <contenuto\_identificato>}, facilitando la correlazione con i log originali e l'automazione delle azioni correttive.

\subsubsection{Limitazioni e considerazioni operative}
\label{subsubsec:ver3_limitazioni}

Nonostante rappresenti una soluzione pronta per ambienti di produzione, la terza versione presenta alcune considerazioni operative importanti.

Il sistema è \textbf{dipendente da Ollama}, richiedendo un'istanza sempre attiva che introduce un \textit{single point of failure}. La \textbf{latenza variabile} rappresenta un'altra criticità, poiché i tempi di risposta dipendono dalle performance dell'LLM selezionato e dal carico del sistema. Inoltre, la \textbf{scalabilità verticale} è limitata dalle risorse hardware disponibili per l'inferenza dell'LLM.

Relativamente all'\textbf{utilizzo di modelli personalizzati}, sebbene questi migliorino significativamente le performance rispetto ai modelli base, il sistema non utilizza tecniche di \textit{fine-tuning} avanzate come LoRA\footnote{LoRA (Low-Rank Adaptation) è una tecnica di fine-tuning efficiente che modifica solo un sottoinsieme dei parametri del modello e permette di creare un nuovo LLM a partire da uno già addestrato.}, che rappresenterebbe la soluzione ottimale per l'adattamento specifico al dominio dei log. Per ulteriori dettagli, si rimanda alla sezione~\refwithpage{subsubsec:addestramento_lora}.

Queste limitazioni, pur non compromettendo l'utilità operativa del sistema, rappresentano aree di miglioramento e suggeriscono direzioni per sviluppi futuri del progetto.



%
%			CAPITOLO: Test
%

\chapter{Test e valutazione}
\label{chap:test}

Il sistema \textit{Sensitive Data Detector} è stato sottoposto a un processo di testing e valutazione sistematico attraverso due fasi evolutive principali, corrispondenti alle prime due versioni del progetto. Ogni fase ha introdotto metodologie di test sempre più raffinate e metriche di valutazione quantitative per misurare l'efficacia del sistema nell'identificazione di dati sensibili.

La terza versione del sistema, pur rappresentando un'evoluzione significativa dal punto di vista architetturale e operativo, non è stata sottoposta a un processo di testing sistematico analogo alle versioni precedenti. \\
Questa scelta è motivata da considerazioni sia tecniche che metodologiche. Da un lato, sussiste un'equivalenza algoritmica con la seconda versione: la logica di identificazione dei dati sensibili rimane sostanzialmente invariata e vengono utilizzati gli stessi LLM e i medesimi \textit{Modelfile}, riducendo il valore informativo di una nuova campagna di test controllati. Dall'altro, la natura operativa della terza versione, che elabora direttamente stream di log provenienti da GrayLog, rende impraticabile l'impiego di dataset pre-annotati con ground truth. \\
Inoltre, l'integrazione con sistemi reali introduce variabili ambientali non controllabili che comprometterebbero la riproducibilità e l'affidabilità di misure quantitative.

\section{Metodologia di test}
\label{sec:metodologia_test}

La metodologia di test si è evoluta significativamente tra le due versioni principali del sistema, passando da un approccio qualitativo basato su osservazione manuale a un sistema quantitativo automatizzato.

\subsubsection{Versione 1: Testing esplorativo}
\label{subsubsec:metodologia_v1}

La prima versione ha utilizzato un approccio esplorativo per validare la fattibilità dell'utilizzo di Large Language Models nell'identificazione di dati sensibili. Il testing si è concentrato sull'\textbf{analisi qualitativa delle risposte}, dove ogni output del modello LLM veniva analizzato manualmente per identificare la presenza di falsi positivi e falsi negativi. Parallelamente, sono stati condotti test di diverse combinazioni di parametri, come il numero di righe per messaggio e i messaggi per chat, per l'\textbf{ottimizzazione delle configurazioni} e massimizzare l'efficacia del sistema. La valutazione dei vincoli di memoria e performance sull'architettura hardware utilizzata ha permesso di \textbf{gestire le limitazioni hardware}, mentre l'analisi del comportamento del sistema su dataset di dimensioni crescenti (100 e 1000 righe) ha fornito indicazioni sulla \textbf{scalabilità} della soluzione.

L'approccio della prima versione ha evidenziato problematiche critiche legate alla gestione del context window limitato dei modelli LLM, portando allo sviluppo di strategie di segmentazione e ripristino delle conversazioni.

\subsubsection{Versione 2: Testing quantitativo}
\label{subsubsec:metodologia_v2}

La seconda versione ha introdotto un framework di testing quantitativo basato su ground truth annotato manualmente. La metodologia prevede innanzitutto la \textbf{preparazione del dataset} attraverso l'annotazione manuale di file di log con etichette binarie (Y/N) per ogni riga, indicando la presenza o assenza di dati sensibili. Successivamente, l'\textbf{elaborazione automatizzata} permette l'implementazione di un sistema di confronto automatico tra le risposte del modello e il ground truth, consentendo il \textbf{calcolo real-time delle metriche} di veri positivi, veri negativi, falsi positivi e falsi negativi. Infine, i \textbf{test comparativi} permettono la valutazione sistematica di modelli LLM differenti con configurazioni standardizzate.

Questa evoluzione metodologica ha permesso un confronto oggettivo e riproducibile delle performance tra diversi modelli e configurazioni.

\section{Metriche di valutazione}
\label{sec:metriche_test}

Il sistema di valutazione utilizza metriche standard di classificazione binaria, adattate al contesto specifico dell'identificazione di dati sensibili nei log.

\subsubsection{Metriche primarie}
\label{subsubsec:metriche_primarie}

Le metriche principali utilizzate per la valutazione sono elencate nella tabella~\refwithpage{tab:metriche_primarie}.

\begin{table}[h!]
    \centering
    \begin{tabular}{|p{0.25\textwidth}|p{0.65\textwidth}|}
        \hline
        \textbf{Metrica} & \textbf{Descrizione} \\
        \hline
        Veri Positivi (TP) & Righe contenenti dati sensibili correttamente identificate dal modello \\
        \hline
        Veri Negativi (TN) & Righe senza dati sensibili correttamente classificate come non sensibili \\
        \hline
        Falsi Positivi (FP) & Righe senza dati sensibili erroneamente identificate come contenenti dati sensibili \\
        \hline
        Falsi Negativi (FN) & Righe contenenti dati sensibili non identificate dal modello \\
        \hline
    \end{tabular}
    \caption{Metriche primarie di classificazione}
    \label{tab:metriche_primarie}
\end{table}

\subsubsection{Metriche derivate}
\label{subsubsec:metriche_derivate}

Dalle metriche primarie vengono calcolate le percentuali elencate nella tabella~\refwithpage{tab:metriche_derivate}.

\begin{table}[h!]
    \centering
    \begin{tabular}{|p{0.3\textwidth}|p{0.25\textwidth}|p{0.35\textwidth}|}
        \hline
        \textbf{Metrica} & \textbf{Formula} & \textbf{Descrizione} \\
        \hline
        Accuratezza Complessiva & $\frac{TP + TN}{TP + TN + FP + FN} \times 100$ & Percentuale complessiva di classificazioni corrette \\
        \hline
        Percentuale di Falsi Positivi & $\frac{FP}{TP + TN + FP + FN} \times 100$ & Incidenza di righe non sensibili classificate come sensibili \\
        \hline
        Percentuale di Falsi Negativi & $\frac{FN}{TP + TN + FP + FN} \times 100$ & Incidenza di righe sensibili non identificate \\
        \hline
    \end{tabular}
    \caption{Metriche derivate di valutazione}
    \label{tab:metriche_derivate}
\end{table}

Nel contesto applicativo, i falsi negativi rappresentano un rischio di sicurezza maggiore rispetto ai falsi positivi, in quanto comportano la mancata identificazione di dati sensibili effettivamente presenti nei log.

\section{Risultati sperimentali}
\label{sec:risultati_sperimentali}

I test sono stati condotti su architettura Apple M1 Max (32 GB di memoria unificata, 32 GPU Core) utilizzando il framework Ollama per l'esecuzione locale dei modelli LLM.

\subsubsection{Risultati Versione 1}
\label{subsubsec:risultati_v1}

La prima versione ha fornito risultati promettenti ma ha evidenziato significative limitazioni hardware, come mostrato nella tabella~\refwithpage{tab:risultati_v1}.

\begin{table}[h!]
    \centering
    \begin{tabular}{|l|c|c|c|c|c|}
        \hline
        \textbf{Modello} & \textbf{Righe}                                        & \textbf{Righe/msg} & \textbf{Msg/chat} & \textbf{Tempo} & \textbf{Falsi pos.} \\ \hline
        Llama3 8b        & 100                                                   & 3                  & 1                 & 01m 42s        & $\approx$15\%       \\ \hline
        Llama3 8b        & 1000                                                  & 3                  & 1                 & 14m 05s        & $\approx$5\%        \\ \hline
        Llama3.1 8b      & 100                                                   & 3                  & 1                 & 02m 27s        & $\approx$5\%        \\ \hline
        Llama3.1 8b      & 1000                                                  & 3                  & 1                 & 20m 26s        & $\approx$5\%        \\ \hline
        Llama3 70b       & \multicolumn{5}{c|}{\textit{Memoria non sufficiente}}                                                                                 \\ \hline
        Llama3.1 70b     & \multicolumn{5}{c|}{\textit{Memoria non sufficiente}}                                                                                 \\ \hline
        Command-R 35b    & \multicolumn{5}{c|}{\textit{Memoria non sufficiente}}                                                                                 \\ \hline
    \end{tabular}
    \caption{Risultati della versione 1 su Apple M1 Max}
    \label{tab:risultati_v1}
\end{table}
\todo{TODO ri-verificare tabella}

I risultati mostrano che:
\begin{itemize}
    \item I modelli a 8 miliardi di parametri sono eseguibili con performance accettabili
    \item L'incremento del dataset da 100 a 1000 righe migliora l'accuratezza riducendo i falsi positivi
    \item I modelli più grandi (70b+ parametri) superano i limiti hardware disponibili
\end{itemize}

\subsubsection{Risultati Versione 2}
\label{subsubsec:risultati_v2}

La seconda versione ha introdotto metriche quantitative precise permettendo un confronto sistematico, come mostrato nella tabella~\refwithpage{tab:risultati_v2} e il grafico~\refwithpage{fig:grafico_risultati_v2}.

\begin{table}[h!]
    \centering
    \begin{tabular}{|l|c|c|c|c|c|c|}
        \hline
        \textbf{Modello} & \textbf{Righe} & \textbf{Tempo} & \textbf{VN}   & \textbf{VP}   & \textbf{FN}   & \textbf{FP}   \\ \hline
        llama3.1:8b      & 100            & 00m 57s        & $\approx$70\% & $\approx$14\% & $\approx$10\% & $\approx$6\%  \\ \hline
        llama3.1:8b      & 1000           & 09m 05s        & $\approx$68\% & $\approx$14\% & $\approx$5\%  & $\approx$13\% \\ \hline
        llama3.2:3b      & 100            & 01m 08s        & $\approx$64\% & $\approx$21\% & $\approx$3\%  & $\approx$12\% \\ \hline
        llama3.2:3b      & 1000           & 07m 15s        & $\approx$59\% & $\approx$16\% & $\approx$2\%  & $\approx$23\% \\ \hline
        mistral:7b       & 100            & 06m 48s        & $\approx$0\%  & $\approx$24\% & $\approx$0\%  & $\approx$76\% \\ \hline
        mistral:7b       & 1000           & 72m 58s        & $\approx$0\%  & $\approx$19\% & $\approx$0\%  & $\approx$81\% \\ \hline
        mistral-nemo:12b & 100            & 04m 10s        & $\approx$45\% & $\approx$13\% & $\approx$11\% & $\approx$31\% \\ \hline
        mistral-nemo:12b & 1000           & 35m 15s        & $\approx$48\% & $\approx$12\% & $\approx$7\%  & $\approx$33\% \\ \hline
        qwen2.5:7b       & 100            & 01m 45s        & $\approx$76\% & $\approx$21\% & $\approx$3\%  & $\approx$0\%  \\ \hline
        qwen2.5:7b       & 1000           & 16m 44s        & $\approx$68\% & $\approx$18\% & $\approx$1\%  & $\approx$13\% \\ \hline
        gemma3:4b        & 100            & 01m 17s        & $\approx$58\% & $\approx$21\% & $\approx$3\%  & $\approx$18\% \\ \hline
        gemma3:12b       & 100            & 03m 18s        & $\approx$74\% & $\approx$23\% & $\approx$1\%  & $\approx$2\%  \\ \hline
        deepseek         & todo           & todo           & todo          & todo          & todo          & todo          \\ \hline
        deepseek         & todo           & todo           & todo          & todo          & todo          & todo          \\ \hline
    \end{tabular}
    \caption{Risultati della versione 2 con metriche quantitative}
    \label{tab:risultati_v2}
\end{table}

\todo{TODO: aggiungere i risultati dei modelli deepseek}
\todo{TODO: aggiungere i risultati dei modelli gemma con 1000 righe}

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
                ybar,
                width=\textwidth,
                height=8cm,
                xlabel={Modelli LLM},
                ylabel={Accuratezza (\%)},
                symbolic x coords={llama3.1:8b, llama3.2:3b, mistral:7b, mistral-nemo:12b, qwen2.5:7b, gemma3:4b, gemma3:12b, deepseek:7b, deepseek:8b},
                xtick=data,
                x tick label style={rotate=45, anchor=east},
                legend pos=north west,
                bar width=15pt,
                ymin=0,
                ymax=100
            ]

            % Accuratezza su 100 righe (VN% + VP%)
            \addplot coordinates {
                    (llama3.1:8b, 84)      % 70+14
                    (llama3.2:3b, 85)      % 64+21
                    (mistral:7b, 24)       % 0+24
                    (mistral-nemo:12b, 58) % 45+13
                    (qwen2.5:7b, 97)       % 76+21
                    (gemma3:4b, 79)        % 58+21
                    (gemma3:12b, 97)       % 74+23
                    (deepseek:7b, 1)       % TODO: placeholder
                    (deepseek:8b, 1)       % TODO: placeholder
                };

            % Accuratezza su 1000 righe (VN% + VP%) dove disponibile
            \addplot coordinates {
                    (llama3.1:8b, 82)      % 68+14
                    (llama3.2:3b, 75)      % 59+16
                    (mistral:7b, 19)       % 0+19
                    (mistral-nemo:12b, 60) % 48+12
                    (qwen2.5:7b, 86)       % 68+18
                    (gemma3:4b, 0)         % Non testato su 1000
                    (gemma3:12b, 0)        % Non testato su 1000
                    (deepseek:7b, 1)       % TODO: placeholder
                    (deepseek:8b, 1)       % TODO: placeholder
                };

            \legend{100 righe, 1000 righe}
        \end{axis}
    \end{tikzpicture}
    \caption{Confronto dell'accuratezza dei modelli LLM sui dataset di test}
    \label{fig:grafico_risultati_v2}
\end{figure}


\subsubsection{Confronto dei modelli LLM utilizzati}
\label{subsubsec:confronto_modelli}

\subsubsection{Modelli ad alte performance}
I modelli che hanno mostrato le migliori performance complessive sono:

\begin{itemize}
    \item \textbf{Qwen2.5:7b}: Eccellente combinazione di accuratezza ($\approx$94\% su 100 righe, $\approx$86\% su 1000 righe) e velocità di esecuzione
    \item \textbf{Gemma3:12b}: Migliore accuratezza ($\approx$97\% su 100 righe) ma tempi di esecuzione più elevati
    \item \textbf{Llama3.1:8b}: Performance equilibrate con buona scalabilità
\end{itemize}

\subsubsection{Modelli problematici}
Alcuni modelli hanno mostrato comportamenti problematici:

\begin{itemize}
    \item \textbf{Mistral:7b}: Tasso di falsi positivi estremamente elevato ($\approx$76-81\%), probabilmente dovuto a una configurazione di prompt non ottimale per questo modello specifico
    \item \textbf{Mistral-nemo:12b}: Performance mediocri nonostante le dimensioni maggiori
\end{itemize}

\subsubsection{Modelli reasoning}
\label{subsubsec:modelli_reasoning}

Alcuni modelli testati includono capacità di reasoning avanzate, utilizzando tag speciali per mostrare il processo di ragionamento interno. I modelli DeepSeek R1 (7B e 8B) utilizzano tag \texttt{<think>...</think>} per visualizzare il ragionamento prima della risposta finale.

Il sistema implementa un meccanismo di post-processing per rimuovere automaticamente questi tag di reasoning e considerare solo la risposta finale, garantendo compatibilità con il formato di output atteso.

\section{Analisi delle performance}
\label{sec:analisi_performance}

\subsubsection{Relazione dimensioni-accuratezza}
\label{subsubsec:relazione_dimensioni_accuratezza}

L'analisi delle performance evidenzia diversi fattori critici per l'efficacia del sistema.

Contrariamente alle aspettative, i modelli più grandi non sempre offrono performance superiori:
\begin{itemize}
    \item Qwen2.5:7b supera modelli più grandi come Mistral-nemo:12b
    \item Llama3.2:3b mostra performance comparabili a modelli significativamente più grandi
    \item La qualità del fine-tuning sembra essere più importante delle dimensioni del modello
\end{itemize}

\subsubsection{Efficienza computazionale}
\label{subsubsec:efficienza_computazionale}

I risultati mostrano variazioni significative nell'efficienza computazionale. In particolare, modelli come Llama3.2:3b, Qwen2.5:7b e Gemma3:4b completano l'elaborazione di 100 righe in meno di due minuti, mentre Mistral:7b richiede oltre sei minuti per lo stesso carico. Da ciò si evince che una maggiore velocità non si traduce necessariamente in minore accuratezza.
\todo{TODO aggiungere più confronti}

\subsubsection{Limitazioni hardware}
\label{subsubsec:limitazioni_hardware}
\todo{TODO Rivalutare in seguito a test con altro hw}

L'hardware utilizzato (Apple M1 Max, 32GB RAM) ha posto vincoli significativi:
\begin{itemize}
    \item Impossibilità di eseguire modelli superiori a 12-15 miliardi di parametri
    \item Necessità di ottimizzazioni specifiche per l'architettura Apple Silicon
    \item Compromessi tra dimensioni del modello e velocità di inferenza
\end{itemize}

I risultati confermano la fattibilità dell'utilizzo di LLM per l'identificazione di dati sensibili, evidenziando l'importanza della selezione del modello appropriato in base ai requisiti specifici di accuratezza, velocità e risorse disponibili.



%
%			CAPITOLO: Conclusioni e sviluppi futuri
%

\todochapter{Conclusioni}{chap:conclusioni}{conclusioni}

\todosection{Risultati raggiunti}{sec:risultati_raggiunti}{finire}

Questo progesso ci ha permesso di confermare l'utilizzo di LLM per l'identificazione e il report di dati sensibili all'interno di log di sistemi complessi in un'ambiente di produzione.

\section{Sviluppi futuri}
\label{sec:sviluppi_futuri}

\subsubsection{Addestramento di LoRA}
\label{subsubsec:addestramento_lora}

LoRA (Low-Rank Adaptation) è una tecnica di fine-tuning efficiente che modifica solo un sottoinsieme dei parametri del modello e permette di creare un nuovo LLM a partire da uno già addestrato.

Tramite questa tecnica è possibile addestrare un modello per un dominio specifico, ad esempio il dominio dei log, senza dover addestrare un nuovo modello completamente da zero, operazione estremamente complicata da eseguire nei progetti di ricerca accademica a causa dei costi computazionali elevati, dei tempi di sviluppo estremamente lunghi, delle competenze multidisciplinari richieste e della necessità di dataset di addestramento di dimensioni elevate.

In seguito a un periodo prolungato di utilizzo del sistema in ambiente di produzione e di raccolta sistematica dei dati e dei report generati, sarebbe possibile utilizzare questi dati come dataset di training per addestrare un LoRA specificamente ottimizzato per questo caso d'uso. \\
Il fine-tuning potrebbe partire da uno dei modelli LLM attualmente utilizzati che ha dimostrato le performance migliori durante la fase di testing.

\subsubsection{Integrazione con altri sistemi}
\label{subsubsec:integrazione_sistemi}

La versione finale del progetto è stata sviluppata specificamente per l'integrazione con GrayLog e lo script di setup dell'ambiente è stato scritto per interagire con questa piattaforma. \\
Un possibile sviluppo futuro consiste nella generalizzazione delle integrazioni attraverso lo sviluppo di un protocollo standard che consenta la creazione di adattatori modulari per diversi sistemi di gestione dei log.

Questa evoluzione permetterebbe di:
\begin{enumerate}
    \item Estendere la compatibilità del sistema a piattaforme alternative come Elasticsearch\footnote{Elasticsearch \url{https://www.elastic.co/elasticsearch}}, Splunk\footnote{Splunk \url{https://www.splunk.com}}, o Fluentd\footnote{Fluentd \url{https://www.fluentd.org}}
    \item Standardizzare le interfacce di comunicazione tra il sistema di analisi e i sistemi di logging
    \item Semplificare la distribuzione del sistema in ambienti enterprise eterogenei
    \item Migliorare la tracciabilità e la gestione delle notifiche attraverso un framework unificato
\end{enumerate}

Un protocollo di integrazione standardizzato ridurrebbe significativamente la complessità di deployment e aumenterebbe l'adozione del sistema in contesti operativi diversificati.

\subsubsection{Rassegna delle alternative a GrayLog}
\label{subsubsec:alternative_graylog}

Tra le alternative più rilevanti si distinguono diverse categorie di strumenti.\\
\textbf{SigNoz}\footnote{SigNoz \url{https://signoz.io}} rappresenta una soluzione moderna open source focalizzata sull'osservabilità completa, offrendo un pannello unificato per visualizzare traces, metriche e log provenienti da OpenTelemetry. Utilizza ClickHouse\footnote{ClickHouse \url{https://clickhouse.com}} come database per garantire performance elevate e supporta l'ingestione di dati da oltre 50 sorgenti diverse, rendendolo particolarmente adatto ad ambienti cloud-native dove l'integrazione con standard aperti è prioritaria. La piattaforma offre inoltre correlazione automatica tra i diversi segnali telemetrici per un debugging più efficace.

\textbf{Logstash}\footnote{Logstash \url{https://www.elastic.co/logstash}} si configura come una pipeline di data processing server-side altamente flessibile con un framework estensibile che conta oltre 200 plugin. Il sistema implementa code persistenti per garantire delivery at-least-once degli eventi e supporta monitoraggio centralizzato delle pipeline attraverso un'interfaccia unificata, risultando ideale in scenari dove è necessaria una pre-elaborazione complessa dei dati.

\textbf{FluentD}\footnote{FluentD \url{https://www.fluentd.org}} emerge come un data collector open source caratterizzato da un'architettura basata su plugin che conta oltre 500 estensioni disponibili. Progetto della Cloud Native Computing Foundation (CNCF)\footnote{CNCF \url{https://www.cncf.io}}, fornisce un layer di logging unificato che consente di raccogliere log da sorgenti eterogenee e inoltrarli a destinazioni multiple, disaccoppiando le sorgenti dati dai sistemi backend.

\textbf{Syslog-ng}\footnote{Syslog-ng \url{https://www.syslog-ng.com}}, implementazione avanzata del protocollo syslog\footnote{Syslog è un protocollo standard per il logging di sistema definito dall'RFC 3164 (\url{https://datatracker.ietf.org/doc/html/rfc3164})}, fornisce funzionalità di raccolta, elaborazione e salvataggio dei log con particolare enfasi sulla compatibilità con sistemi legacy e sulla conformità agli standard di compliance.

Infine, \textbf{Apache Flume}\footnote{Apache Flume \url{https://flume.apache.org}} si posiziona come un servizio distribuito specializzato nel raccoglimento, aggregazione e spostamento di grandi volumi di streaming event data attraverso un modello basato su data flows.

L'integrazione del \textit{Sensitive Data Detector} con queste piattaforme alternative richiederebbe l'adattamento del layer di comunicazione e la definizione delle pipeline di elaborazione, mantenendo invariata la logica di analisi basata su LLM e regex.



\chapter{da organizzare (TODO DA RIMUOVERE)}
\label{chap:da_organizzare}

\section{Readme}
\label{sec:readme}

\begin{tabular}{|p{0.25\textwidth}|p{0.25\textwidth}|p{0.25\textwidth}|p{0.25\textwidth}|}
    \hline
    \textbf{Strumento}        & \textbf{Descrizione}                                                                                                                       & \textbf{Pro}                                                                                                                                                                                                  & \textbf{Contro}                                                                                                                                                                                                                                     \\ \hline
    Presidio                  & SDK per la protezione e anonimizzazione di dati privati in testi e immagini. Funziona tramite regex e NLP                                  & Ha il supporto alla ricerca tramite NLP e quindi dovrebbe essere più facile trovare dati non perfettamente rappresentabili con regex                                                                          & Lavorando con NLP potrebbe generare falsi negativi e falsi positivi                                                                                                                                                                                 \\ \hline
    anonympy                  & Libreria python per l'anonimizzazione di dati in tabelle, immagini e PDF                                                                   & È una libreria e quindi si può integrare facilmente in software custom                                                                                                                                        & Non è fatto per lavorare su testi semplici, anche se si potrebbe provare a modificare l'elaborazione dei PDF per renderlo possibile                                                                                                                 \\ \hline
    Data Protection Framework & Strumento molto simile a Microsoft Presidio, è una libreria Python che permette di trovare e anonimizzare dati privati tramite regex e NLP & Libreria FOSS integrabile in software custom oppure richiamabile direttamente da linea di comando                                                                                                             & Il progetto non è completo, mancano ancora alcuni detector                                                                                                                                                                                          \\ \hline
    NgAnonymize               & Libreria Angular per anonimizzare dati                                                                                                     & Supporta diversi metodo per anonimizzare i dati                                                                                                                                                               & I dati da anonimizzare devono essere specificati singolarmente, non ha alcuna feature di rilevamento dei dati da anonimizzare                                                                                                                       \\ \hline
    arx-deidentifier          & OSS per l'anonimizzazione di dati personali                                                                                                & FOSS offerto sia come sw completo che libreria Java. È stato sviluppato come ricerca universitaria e ha paper associati                                                                                       & Supporta solo dati tabulari                                                                                                                                                                                                                         \\ \hline
    loganalyzer               & FOSS software che trova e rimuove pattern pre-definiti da file di log                                                                      & Permette sia di rimuovere dati che di riportarli in un report, aiutando quindi la compilazione di un "punteggio" di sicurezza dei log. Scritto in C e quindi più veloce delle alternative in Angular o Python & Scritto in C, quindi il codice è più difficile da modificare. Non sembra essere offerto come libreria, quindi difficilmente integrabile in altri SW. Supporta solo ambienti grafici QT quindi la compilazione potrebbe dare problemi in base all'OS \\ \hline
\end{tabular}

\clearpage



%
%			APPENDICE: materiali aggiuntivi e dimostrazioni
%

\appendix

\todochapter{Codice}{chap:appendice_codice}{riguardare e sistemare commenti nel codice}

\section{Versione 1: Proof of Concept}

\lstinputlisting[
caption=Codice sorgente della versione 1,
label=lst:code_ver1,
language=Python,
literate={è}{{\`e}}1 {à}{{\`a}}1 {ò}{{\`o}}1 {ù}{{\`u}}1 {ì}{{\`i}}1
]{"../ollama extractor v1/main.py"}


\lstinputlisting[
caption=Prompt usato nella versione 1,
label=lst:prompt_ver1,
literate={è}{{\`e}}1 {à}{{\`a}}1 {ò}{{\`o}}1 {ù}{{\`u}}1 {ì}{{\`i}}1
]{"../ollama extractor v1/prompt.txt"}
\todo{sistemare testo che esce dai confini del box}

\clearpage

\section{Versione 2: Elaborazione di batch di log}

\lstinputlisting[
caption=Codice sorgente della versione 2,
label=lst:code_ver2,
language=Python,
literate={è}{{\`e}}1 {à}{{\`a}}1 {ò}{{\`o}}1 {ù}{{\`u}}1 {ì}{{\`i}}1
]{"../ollama extractor v2/main.py"}

\clearpage

\section{Versione 3: Integrazione con GrayLog}

\lstinputlisting[
caption=Codice sorgente della versione 3,
label=lst:code_ver3,
language=Python,
literate={è}{{\`e}}1 {à}{{\`a}}1 {ò}{{\`o}}1 {ù}{{\`u}}1 {ì}{{\`i}}1
]{"../ollama extractor v3/main.py"}

\clearpage

\section{\textit{Modelfile}}
\label{sec:code_modelfile}

\lstinputlisting[
caption=\textit{Modelfile} di partenza usato per generare i modelli personalizzati,
label=lst:modelfile,
language=Python,
literate={è}{{\`e}}1 {à}{{\`a}}1 {ò}{{\`o}}1 {ù}{{\`u}}1 {ì}{{\`i}}1
]{"../modelfiles/sensitive-data-detector-BASE.modelfile"}
\todo{sistemare testo che esce dai confini del box}

\clearpage

\section{Script di configurazione GrayLog}
\label{sec:code_graylog_script}

\lstinputlisting[
caption=Script Python per la configurazione automatica di GrayLog,
label=lst:code_graylog_script,
language=Python,
literate={è}{{\`e}}1 {à}{{\`a}}1 {ò}{{\`o}}1 {ù}{{\`u}}1 {ì}{{\`i}}1
]{"../graylog docker/graylog_setup.py"}

\clearpage

\section{Configurazione Docker Compose}
\label{sec:code_container_config}

\lstinputlisting[
caption=File Docker Compose per l'ambiente di test con GrayLog,
label=lst:code_container_config,
literate={è}{{\`e}}1 {à}{{\`a}}1 {ò}{{\`o}}1 {ù}{{\`u}}1 {ì}{{\`i}}1
]{"../graylog docker/docker-compose.yml"}


%
%			BIBLIOGRAFIA
%

% Si può specificare a che livello della TOC deve essere la bibliografia.
% Il default è 'chapter', per 'part' usare
% \beforebibliography[part]
\beforebibliography
\bibliographystyle{unsrt}
\bibliography{bibliografia}

% Pagina di chiusura tesi
% \closingpage

% Pagina bianca finale
% \clearpage
% \thispagestyle{empty}
% \null
% \clearpage

\end{document}

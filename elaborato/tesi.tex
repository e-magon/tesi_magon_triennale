%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                             %
%           TEMPLATE LATEX PER TESI                                           %
%           ______________                                                    %
%                                                                             %
%           Ultima revisione: 28 Novembre 2024                                %
%           Revisori: G.Presti; L.A.Ludovico; F. Avanzini; M. Tiraboschi      %
%                                                                             %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Pacchetti necessari per compilare questo documento:
% - babel
% - geometry
% - pgfplots
% - todonotes
% - xcolor

% Per compilare il documento eseguire:
% cd "elaborato"
% latexmk -pdf -f "tesi.tex"

\documentclass[12pt]{report}

% --- PREAMBOLO ---------------------------------------------------------------
% Inserire qui eventuali package da includere o
% definizioni di comandi personalizzati

% Selezione lingua
\usepackage[italian]{babel}

\usepackage{tesi}
% Puoi usare il font di default di LaTeX con la relativa opzione del package
% \usepackage[defaultfont]{tesi}
% Esiste anche un'opzione per il formato 17x24 per le tesi di dottorato
% \usepackage[phd]{tesi}

% Mostra i bounding box per visualizzare il layout (TODO rimuovere per versione finale)
% \geometry{showframe}

% Per disabilitare i todo:
% \usepackage[disable]{todonotes}
% \usepackage{todonotes}

% Inclusione comandi personalizzati
\usepackage{todo-chapters}
\usepackage{refwithpage}

% Pacchetto per grafici con dati integrati
\usepackage{pgfplots}
\pgfplotsset{compat=1.18} % Compatibilità con la versione 1.18 di pgfplots

% In caso il copia-incolla del PDF generato perda gli spazi,
% provare a decommentare la seguente riga
% \pdfinterwordspaceon

% !!! INFORMAZIONI SULLA TESI DA COMPILARE !!!

%   UNIVERSITA' E CORSO DI LAUREA:
\university{Università degli Studi di Milano}
\unilogo{immagini/loghi/unimi}
\faculty{Facoltà di Scienze e Tecnologie}
\department{Dipartimento di Informatica\\Giovanni Degli Antoni}
\cdl{Corso di Laurea Triennale in\\Sicurezza dei Sistemi e delle Reti Informatiche}

%   TITOLO TESI:
\title{Tesi}
% Questo comando (opzionale) sovrascrive \title per quanto riguarda la copertina
% Può essere usato per stampare caratteri speciali, tenendo i metadati puliti
\printedtitle{(TODO: NON DEFINITIVO) \\
Sensitive Data Detector: \\
Identificazione automatica di dati sensibili all'interno di log di sistemi complessi tramite analisi statica e Large Language Models}

%   AUTORE:
\author{Emanuele Magon}
\matricola{909482}
% "Elaborato Finale" per i CdL triennali
% "Tesi di Laurea" per i CdL magistrali
\typeofthesis{Elaborato Finale}

%   RELATORE E CORRELATORE:
\relatore{Prof. Marco Anisetti}
\correlatore{Dott. Antongiacomo Polimeno}

%   LABORATORIO:
% Questa sezione crea una pagina di chiusura della tesi con
% il logo dell'ente/laboratorio presso cui si è svolto il tirocinio.
% Più afferenze/url/loghi sono supportate,
% e la frase può essere personalizzata.
% Qui trovate alcuni predefiniti del nostro dipartimento
% \adaptlab
% \aislab
% \anacletolab
% \bisplab
% \connetslab
% \everywarelab
% \falselab
% \iebilab
% \islab
% \lailalab
% \lalalab
% \lawlab
% \laserlab
% \limlab
% \mipslab
% \optlab
% \phuselab
% \ponglab
% \sesarlab
% \spdplab

% Esempio di personalizzazione della pagina di chiusura
% (non consegnate con questo esempio!)
% (da commentare in caso sia sufficiente una delle macro precedenti)
% \lab{Laboratorio di Ricerca}
% \lab[in collaborazione con l']{Azienda Specifica}
% \laburl{https://di.unimi.it/it/ricerca/risorse-e-luoghi-della-ricerca/laboratori-di-ricerca}
% \lablogo{immagini/redqmark}

% Con questo comando si può cambiare la dimensione (massima
% altezza e larghezza consentite) dei loghi
% \setlength\lablogosize{25mm}

%   ANNO ACCADEMICO
% \the\year inserisce l'anno corrente
% per specificare manualmente un anno accademico
% NON inserire nel formato 1970-1971, ma
% inserire solo 1970
\academicyear{2025}

%   INDICI:
% elenco delle figure (facoltativo)
% \figurespagetrue
% elenco delle tabelle (facoltativo)
% \tablespagetrue
% prefazioni nell'indice (facoltativo)
% \prefaceintoctrue
% indice nell'indice (facoltativo)
% \tocintoctrue

\setlength {\marginparwidth }{2cm}

% Stile dei blocchi di codice
\usepackage{xcolor}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.97,0.97,0.95}

\lstdefinestyle{codelistingstyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\scriptsize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=t,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=codelistingstyle}

% Cambia il nome usato nelle caption del codice
\renewcommand{\lstlistingname}{Sorgente}
\renewcommand{\lstlistlistingname}{Elenco dei sorgenti}

% --- FINE PREAMBOLO ----------------------------------------------------------

\begin{document}

% Creazione automatica della copertina
% Centra la copertina nel foglio: usa questo comando per la copertina esterna
\makecenteredfrontpage
% Copertina allineata alle altre pagine: usa questo comando per la copertina interna
% \makefrontpage

%
%			PREFAZIONE (facoltativa)
%

% \prefacesection{Prefazione}
% Le prefazioni non sono molto comuni, tuttavia a volte capita che qualcuno voglia dire qualcosa che esuli dal lavoro in sé (come un meta-commento sull'elaborato), o voglia fornire informazioni riguardanti l'eventuale progetto entro cui la tesi si colloca (in questo caso è probabile che sia il relatore a scrivere questa parte).

%
%			RINGRAZIAMENTI (facoltativi)
%

\todoprefacesection{Ringraziamenti}{ringraziamenti}
Questa sezione, facoltativa, contiene i ringraziamenti.

%
%			Creazione automatica dell'indice
%

\afterpreface



%
%			CAPITOLO: Introduzione o Abstract
%

\chapter{Introduzione}
\label{chap:introduzione}

L'obiettivo di questo lavoro è sviluppare \textit{Sensitive Data Detector}, un sistema automatico per il rilevamento della presenza di dati sensibili all'interno di log di grandi dimensioni, fornendo agli amministratori di sistema una soluzione efficace per il monitoraggio della conformità alla privacy, come descritto nel \textbf{Capitolo \ref{chap:stato_arte}}. \\
Sebbene il sistema sia progettato per essere generico e applicabile a diverse tipologie di log, come illustrato nei \textbf{Capitoli \ref{chap:implementazione_tecnologie} e \ref{chap:sensitive_data_detector}} il presente lavoro è stato sviluppato e testato specificamente per l'integrazione nel mondo dell'\textit{IoT}, in particolare per una piattaforma complessa di gestione domotica di \textit{smart city}, composta da svariati micro-servizi eterogenei.

Nel \textbf{Capitolo \ref{chap:metodologia}} viene mostrato come l'approccio progettuale adottato integri un sistema ibrido che combina due metodologie complementari per l'identificazione di dati sensibili nei log. La soluzione utilizza espressioni regolari per l'identificazione efficiente di dati sensibili strutturati e ben definiti (come codici fiscali, numeri di telefono, indirizzi email, JWT\footnote{JWT (JSON Web Token) è uno standard (RFC 7519) per la trasmissione sicura di informazioni codificate file JSON firmati digitalmente. In genere è utilizzato per l'autenticazione e l'autorizzazione di utenti.}, coordinate geografiche etc.) insieme a Large Language Models che, con la loro capacità di comprensione contestuale, permettono il rilevamento di informazioni sensibili non strutturate o espresse in linguaggio naturale. \\
Inoltre, gli LLM sono stati impiegati durante la fase di sviluppo per assistere nell'identificazione di dati sensibili all'interno di campioni di log, facilitando la progettazione di espressioni regolari specifiche e ottimizzate per i pattern identificati.

Come dimostrato nel \textbf{Capitolo \ref{chap:test}}, l'architettura ibrida sviluppata permette di sfruttare i vantaggi di entrambe le tecnologie: l'efficienza computazionale e il determinismo delle espressioni regolari per pattern ricorrenti e la flessibilità semantica degli LLM per l'analisi di contenuti più complessi che potrebbero sfuggire a un approccio basato solamente sull'analisi statica.

Il codice e il materiale sviluppato per questa tesi sono disponibili su GitHub: \\
\url{https://github.com/e-magon/tesi_magon_triennale}


%
%			CAPITOLO: Stato dell'arte
%

\chapter{Stato dell'arte}
\label{chap:stato_arte}

\section{Sicurezza dei log}
\label{sec:sicurezza_log}

Con il termine \textit{log} si indica una sequenza di registrazioni cronologiche generate da sistemi software o hardware per tracciare eventi, operazioni e stati interni. Tali registrazioni rivestono un ruolo fondamentale per la ricostruzione cronologica degli avvenimenti, per il monitoraggio delle prestazioni, per la risoluzione dei problemi, per la sicurezza informatica e per il rispetto dei requisiti di conformità normativa.

I log possono assumere forme eterogenee, spaziando da semplici righe di testo non strutturato a rappresentazioni complesse basate su formati standardizzati. Dal punto di vista del contenuto, le registrazioni possono limitarsi a tracciare in modo sintetico la sequenza degli eventi interni al sistema oppure possono includere descrizioni dettagliate degli stati interni, dei dati in ingresso e in uscita, delle informazioni immesse dagli utenti durante l'interazione con il sistema e dei parametri di configurazione.

Nel corso del tempo sono stati definiti numerosi standard volti a uniformare la struttura dei messaggi di log e dei relativi metadati, al fine di garantire l'interoperabilità tra piattaforme eterogenee. I formati più diffusi sono il protocollo \textit{syslog}\footnote{syslog \url{https://datatracker.ietf.org/doc/html/rfc3164}}, formalizzato dall'IETF nel 2001, il \textit{Common Event Format (CEF)}\footnote{CEF \url{https://www.microfocus.com/documentation/arcsight/arcsight-smartconnectors-8.3/cef-implementation-standard/Content/CEF/Chapter\%201\%20What\%20is\%20CEF.htm}}, introdotto da ArcSight intorno al 2010, il \textit{Log Event Extended Format (LEEF)}\footnote{LEEF \url{https://www.ibm.com/docs/en/dsm?topic=leef-overview}} sviluppato da IBM per QRadar a partire dal 2011 e il \textit{Graylog Extended Log Format (GELF)}\footnote{GELF \url{https://go2docs.graylog.org/current/getting_in_log_data/gelf.html}}, definito nel 2011 per facilitare l'integrazione con la piattaforma Graylog.

In linea generale, ogni voce di log include un \textit{timestamp}, un livello di severità (ad esempio \textit{info}, \textit{warning}, \textit{error}), un identificatore della sorgente (ad esempio il nome del servizio o del componente che ha generato il log) e un messaggio descrittivo dell'evento.

Contenendo informazioni dettagliate sul funzionamento interno dei sistemi, i log rappresentano un obiettivo privilegiato per gli utenti malevoli. Il problema si manifesta su due fronti distinti: da un lato, pratiche di logging poco attente espongono frequentemente informazioni personali identificabili (\textit{PII}) come indirizzi email, credenziali e identificatori di dispositivo. Studi su applicazioni Android hanno rilevato che circa il 6\% delle app analizzate presenta fughe di dati personali nei log. Ciò rende i log un bersaglio appetibile per attacchi informatici finalizzati al furto di informazioni~\cite{aghili2024protecting_privacy_logs, hou2025catamaran}.

Dall'altro lato, la manomissione dei log rappresenta una pratica diffusa: indagini nel campo della risposta agli incidenti riportano che il 72\% degli specialisti ha riscontrato log alterati o cancellati da parte degli attaccanti~\cite{paccagnella2020custos}. Tali attività di \textit{anti-forensics} mirano a ostacolare le indagini eliminando le tracce delle intrusioni~\cite{jena2012audit_log_hash}, mentre in ambienti IoT la compromissione dei dispositivi consente agli avversari di modificare, eliminare o iniettare eventi nei log per eludere l'attribuzione delle responsabilità~\cite{koisser2023accountability_iot_logging}.

\section{Soluzioni esistenti per l'analisi dei log}
\label{sec:soluzioni_esistenti_analisi_log}

Attualmente esistono numerosi strumenti per la rilevazione e l'anonimizzazione di dati sensibili in testi e dataset strutturati. In questa sezione si presentano le soluzioni più rilevanti individuate durante la fase di ricerca, con l'obiettivo di evidenziare punti di forza e limiti rispetto allo scenario specifico dei log applicativi eterogenei.

\textbf{Microsoft Presidio}\footnote{Microsoft Presidio \url{https://microsoft.github.io/presidio}} è un \textit{SDK} per la protezione e la de-identificazione di dati personali che offre moduli separati per l'analisi del testo, l'anonimizzazione, il trattamento di immagini e i dati strutturati. Il sistema combina tecniche di \textit{Named Entity Recognition}, espressioni regolari, regole basate su contesto e checksum per individuare entità \textit{PII} (ad esempio numeri di carte di credito, indirizzi, codici identificativi) e applicare trasformazioni quali mascheramento, sostituzione o cifratura. La piattaforma è estendibile e integra diversi motori NLP e servizi cloud, ma è pensata principalmente come componente generico di data protection e non come soluzione specializzata per flussi continui di log.

\textbf{anonympy}\footnote{anonympy \url{https://pypi.org/project/anonympy}} è una libreria Python orientata all'anonimizzazione di dati tabellari, immagini e PDF. Supporta diverse tecniche per dati di tipologie diverse, come dati numerici e temporali, e include funzionalità per oscurare automaticamente informazioni sensibili presenti in documenti PDF o immagini. La natura di libreria Python la rende facilmente integrabile in software personalizzati, tuttavia il focus primario resta l'elaborazione di dataset strutturati e documenti; per applicarla ai log di testo libero sarebbe necessario introdurre una fase di trasformazione preliminare.

\textbf{Data Protection Framework}\footnote{Data Protection Framework \url{https://github.com/thoughtworks-datakind/anonymizer}} è una libreria e applicazione a riga di comando sviluppata da ThoughtWorks per l'identificazione, l'anonimizzazione e la de-anonimizzazione di dati \textit{PII} in file CSV. Il framework adotta un approccio ibrido simile a Presidio, combinando pattern basati su espressioni regolari con riconoscimento di entità tramite NLP. Il software è progettato per essere eseguito in pipeline di elaborazione dati, ma risulta ancora incompleta e, soprattutto, fortemente orientata a dati tabellari.

\textbf{NgAnonymize}\footnote{NgAnonymize \url{https://github.com/sonbachmi/NgAnonymize}} è una libreria per applicazioni Angular 2+ che offre funzioni di anonimizzazione direttamente a livello di interfaccia utente, tramite una \textit{pipe} che può randomizzare, mescolare o mascherare porzioni di stringhe. Lo strumento è utile per nascondere dati personali in viste front-end (ad esempio nomi, indirizzi o numeri di telefono), ma non include alcun meccanismo di rilevamento automatico: è lo sviluppatore a dover identificare e marcare esplicitamente i campi da anonimizzare. Di conseguenza, NgAnonymize non è adatto all'analisi automatica di grandi volumi di log generati da sistemi eterogenei.

\textbf{ARX}\footnote{ARX \url{https://arx.deidentifier.org}} è una piattaforma open source completa per l'anonimizzazione di dati personali, disponibile sia come applicazione desktop con interfaccia grafica, sia come libreria Java. Il software supporta diversi modelli di privacy (ad esempio $k$-anonymity, $\ell$-diversity, $t$-closeness\footnote{$k$-anonymity richiede che ogni record di un set di dati sia indistinguibile da almeno altri $k-1$ sulla base dei quasi-identificatori; $\ell$-diversity garantisce che, all'interno di ogni gruppo, i valori dell'attributo sensibile presentino sufficiente diversità; $t$-closeness limita quanto la distribuzione dei valori sensibili in ciascun gruppo può discostarsi dalla distribuzione complessiva del dataset.}), molteplici misure di utilità del dato e strumenti avanzati per l'analisi del rischio di re-identificazione, risultando particolarmente adatto per dataset tabellari di grandi dimensioni in contesti clinici, statistici o di ricerca. Sebbene estremamente potente, ARX è progettato per dati strutturati e non fornisce supporto nativo per la semantica specifica dei log di sistema o per integrazioni dirette con piattaforme di log management.

\textbf{LogAnalyzer}\footnote{LogAnalyzer \url{https://github.com/pbek/loganalyzer}} è un'applicazione desktop basata su Qt e pensata per assistere l'analisi manuale di file di log. Lo strumento permette di definire pattern di \textit{ignore} e \textit{report} tramite espressioni regolari, rimuovendo automaticamente dal testo le parti considerate non rilevanti e generando report sulle occorrenze di pattern di interesse. Questa impostazione lo rende utile per l'analisi interattiva da parte di operatori umani, ma l'assenza di un'API libraria e la dipendenza da un ambiente grafico ne limitano l'utilizzo in pipeline automatiche e in contesti server-side.

Infine, \textbf{logredactor}\footnote{logredactor \url{https://pypi.org/project/logredactor}} è una libreria Python focalizzata sulla trasformazione di log prodotti da applicazioni Python. Il pacchetto fornisce un filtro per il modulo \texttt{logging} che applica mascheramenti basati su espressioni regolari ai messaggi e ai campi strutturati (inclusi quelli in formato JSON), permettendo di nascondere informazioni sensibili prima che vengano scritte su file o inviate a sistemi di raccolta centralizzati. Pur risultando semplice da integrare in applicazioni esistenti, \textit{logredactor} opera principalmente al momento della generazione del log e si basa su pattern predefiniti, offrendo quindi capacità limitate nell'analisi retrospettiva di grandi archivi di log eterogenei.

Le soluzioni analizzate mostrano come il problema della protezione dei dati sensibili nei log possa essere affrontato efficacemente tramite espressioni regolari e, in alcuni casi, tecniche di NLP. Tuttavia, nessuno degli strumenti considerati combina in modo nativo un'analisi semantica profonda dei messaggi di log, l'integrazione diretta con piattaforme di log management e un sistema di valutazione quantitativa delle performance. Questa mancanza costituisce la motivazione principale per lo sviluppo del sistema ibrido proposto in questa tesi, basato sulla combinazione di regex e Large Language Models.

\section{Large Language Models}
\label{sec:llm}

I Large Language Models (LLM) rappresentano una delle applicazioni più avanzate e innovative del \textbf{\textit{deep learning}} nel campo dell'elaborazione del linguaggio naturale. Il \textit{deep learning} costituisce un sottoinsieme del machine learning che si basa su reti neurali artificiali con molteplici livelli di elaborazione, capaci di apprendere rappresentazioni gerarchiche sempre più complesse dei dati~\cite{shinde2018review_ml_dl}. Mentre le applicazioni tradizionali del \textit{deep learning} includono il riconoscimento di immagini (\textit{computer vision}), il riconoscimento vocale e l'analisi predittiva, i Large Language Models si sono affermati come strumenti particolarmente potenti per la comprensione e generazione di testo~\cite{shinde2018review_ml_dl}.

Gli LLM sono modelli di intelligenza artificiale addestrati su enormi quantità di testo per apprendere le strutture linguistiche, i pattern semantici e le relazioni contestuali presenti nel linguaggio umano. La loro architettura si basa tipicamente su reti neurali \textbf{\textit{transformer}}\footnote{Transformers \url{https://aws.amazon.com/what-is/transformers-in-artificial-intelligence}}, introdotte nel 2017, che rappresentano un'innovazione fondamentale nel campo del \textit{deep learning}. I \textit{transformer} utilizzano un meccanismo di auto-attenzione (\textit{self-attention}) che permette al modello di valutare l'importanza relativa di ogni parola rispetto alle altre in una frase, indipendentemente dalla loro posizione. Questo consente di elaborare sequenze di testo in parallelo anziché sequenzialmente, migliorando significativamente l'efficienza computazionale e la capacità di comprendere le relazioni tra parole anche distanti tra loro nel testo.

Prima di utilizzare un LLM, il testo viene convertito in \textbf{token}, che rappresentano le unità base di elaborazione. Un token può corrispondere a una parola intera, una porzione di parola (come una sillaba), o anche singoli caratteri, a seconda del metodo di \textit{tokenizzazione} adottato. Il \textbf{\textit{context window}} definisce il numero massimo di token che il modello può elaborare in una singola interazione: ad esempio, un \textit{context window} di 8.192 token consente al modello di analizzare circa 6.000-8.000 parole contemporaneamente, a seconda della lingua e della complessità del testo. Questo limite determina quanta informazione il modello può considerare nel contesto della conversazione o del documento in esame.

Il numero di \textbf{parametri} di un modello, espresso tipicamente in miliardi (\textit{B}), rappresenta invece la quantità di connessioni neurali che compongono il modello. Un modello con 7 miliardi di parametri (7B) ha memorizzato 7 miliardi di \textbf{\textit{pesi}} che determinano come elaborare e interpretare il testo. Modelli con più parametri sono generalmente più adatti a comprendere sfumature linguistiche complesse ed a gestire compiti diversificati, ma richiedono anche maggiori risorse di memoria e potenza di calcolo per funzionare.

\subsection{Machine learning}
\label{subsec:machine_learning}

Il \textbf{machine learning} rappresenta una branca dell'intelligenza artificiale che consente ai sistemi informatici di apprendere e migliorare le proprie prestazioni attraverso l'esperienza, senza essere esplicitamente programmati per ogni specifica situazione~\cite{shinde2018review_ml_dl}. Le sue origini risalgono agli anni '50 del XX secolo, quando ricercatori come Alan Turing e Arthur Samuel iniziarono a esplorare la possibilità di creare macchine capaci di apprendere autonomamente.

Le \textbf{reti neurali artificiali} costituiscono uno dei paradigmi fondamentali del machine learning. Queste architetture computazionali si ispirano al funzionamento del sistema nervoso biologico, organizzando unità di elaborazione artificiali (neuroni) interconnesse in strati. Ogni neurone riceve input, li elabora attraverso funzioni matematiche e propaga il risultato agli strati successivi. Attraverso processi iterativi di addestramento, la rete modifica i \textbf{\textit{pesi}} delle connessioni tra neuroni per minimizzare l'errore tra output predetto e risultato atteso, apprendendo così a riconoscere pattern complessi nei dati.

Nel corso del tempo il machine learning si è affermato come strumento fondamentale nell'analisi dei dati, trovando applicazioni in numerosi settori quali il riconoscimento di pattern, la classificazione di documenti, i sistemi di raccomandazione e l'analisi predittiva. Gli algoritmi di machine learning hanno dimostrato efficacia significativa in contesti dove i dati presentano strutture relativamente semplici e ben definite.

L'avvento del \textbf{\textit{deep learning}} negli ultimi anni ha segnato una svolta nel campo dell'intelligenza artificiale~\cite{shinde2018review_ml_dl}. Questa evoluzione del machine learning si basa su reti neurali artificiali con molteplici livelli di elaborazione, capaci di apprendere rappresentazioni gerarchiche sempre più complesse dei dati. I modelli di deep learning eccellono particolarmente nell'elaborazione di linguaggio naturale, nel riconoscimento di immagini e nella comprensione contestuale, superando significativamente le prestazioni degli approcci tradizionali in task che richiedono una comprensione semantica profonda~\cite{elkhatiba2024performance_benchmarking}.

I \textbf{Large Language Models} rappresentano l'applicazione più avanzata di questa tecnologia nel dominio dell'elaborazione del linguaggio, dimostrando capacità notevolmente superiori nella comprensione e generazione di testo rispetto ai metodi tradizionali, specialmente in compiti di classificazione complessi~\cite{kostina2025llm_text_classification}. Tuttavia, la loro efficacia dipende fortemente dalla capacità di comprendere \textit{feature contestuali}, un aspetto che può variare significativamente tra i diversi tipi di modelli~\cite{zhu2024can_llm_context}.

\subsection{Modelli proprietari e modelli open weight}
\label{subsec:modelli_proprietari_openweight}

I Large Language Model possono essere suddivisi in due categorie principali: modelli \textbf{proprietari} e modelli \textbf{\textit{open weight}}.

I modelli \textbf{proprietari} sono sviluppati e mantenuti da aziende tecnologiche di rilievo internazionale, come, per esempio, \textbf{OpenAI}\footnote{OpenAI \url{https://openai.com}}, che offre i modelli GPT (Generative Pre-trained Transformer); \textbf{Anthropic}\footnote{Anthropic \url{https://www.anthropic.com}}, che offre i modelli Claude; \textbf{Google}\footnote{Google AI \url{https://ai.google}}, che offre i modelli Gemini; e \textbf{Meta}\footnote{Meta AI \url{https://ai.meta.com}}, che offre i modelli Llama.

Questi modelli vengono eseguiti su server appositi e gli utenti possono accedervi tramite interfacce web, applicazioni desktop o mobile, oppure attraverso \textit{API} per l'integrazione programmatica in applicazioni personalizzate. L'accesso ai modelli proprietari comporta generalmente costi basati sul numero di token elaborati o su sottoscrizioni mensili, con diversi LLM a disposizione in base alle esigenze di utilizzo.

I \textbf{modelli \textit{open weight}}, al contrario, sono resi disponibili pubblicamente con i pesi pre-addestrati. Questo approccio permette agli utenti di scaricare i modelli e eseguirli sul proprio hardware o su server privati, utilizzando software specializzati come Ollama\footnote{Ollama \url{https://ollama.com}}, llama.cpp\footnote{llama.cpp \url{https://github.com/ggerganov/llama.cpp}}, o LM Studio\footnote{LM Studio \url{https://lmstudio.ai}}~\cite{bendiouis2024deploying_opensource}.

L'esecuzione locale di modelli \textit{open weight} offre vantaggi significativi in termini di \textbf{privacy}, eliminando la necessità di trasmettere dati sensibili a servizi esterni, e di \textbf{costi operativi} ridotti, evitando le tariffe per token tipiche dei servizi cloud. Tuttavia, questa modalità richiede hardware adeguato in termini di memoria (RAM e VRAM) e capacità di calcolo, specialmente per modelli di grandi dimensioni~\cite{bendiouis2024deploying_opensource}.

È importante distinguere tra diversi livelli di apertura nel contesto dei modelli linguistici. I modelli \textbf{\textit{open source}} forniscono accesso completo al codice sorgente, ai dati di addestramento e all'architettura del modello, permettendo modifiche complete e ridistribuzione. I modelli \textbf{\textit{open weight}}, pur fornendo i pesi pre-addestrati, non sempre includono i dataset di addestramento o documentazione completa sui processi di training. I modelli \textbf{\textit{open data}} condividono i dataset utilizzati per l'addestramento, facilitando la riproducibilità della ricerca ma non necessariamente i pesi del modello finale.

Alcuni provider di modelli proprietari, come Google\footnote{Google Gemma \url{https://deepmind.google/models/gemma}} e Meta\footnote{Meta Llama \url{https://www.llama.com}}, rilasciano anche modelli \textit{open weight}, permettendo agli utenti di eseguire localmente versioni ridotte o specifiche dei loro modelli principali. \\
Questo approccio ibrido consente di beneficiare delle capacità avanzate dei modelli proprietari, pur mantenendo il controllo sui dati e i costi associati all'esecuzione.

La ricerca accademica ha evidenziato che, sebbene i modelli proprietari tendano a mostrare performance superiori in benchmark generali, modelli \textbf{\textit{open weight}} opportunamente ottimizzati possono raggiungere risultati comparabili in task specifici come la comprensione di testi~\cite{alassan2024comparison_opensource_proprietary}. La valutazione delle performance deve inoltre considerare non solo l'accuratezza, ma anche aspetti pratici come la riproducibilità dei risultati e la trasparenza dei processi di valutazione~\cite{white2024livebench}. In particolare, benchmark come \textbf{LiveBench} sono stati sviluppati per fornire valutazioni più eque e resistenti alla contaminazione del test set, problematica che può favorire artificialmente modelli addestrati su dataset che includono i dati di test~\cite{white2024livebench}. Per una comparativa dei modelli proprietari e open weight, si rimanda alla Figura~\refwithpage{fig:livebench_comparison}.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.9\textwidth]{immagini/white2024livebench-1.png}
	\caption{Confronto delle performance di modelli proprietari e open weight su LiveBench~\cite{white2024livebench}}
	\label{fig:livebench_comparison}
\end{figure}

\clearpage
\section{Large Language Models nell'analisi dei testi}
\label{sec:llm_analisi_testi}

L'analisi dei log di sistema presenta sfide significative per le tecniche tradizionali di elaborazione. La varietà dei formati, l'eterogeneità delle sorgenti e la complessità semantica dei messaggi rendono difficoltosa l'estrazione automatica di informazioni significative~\cite{beck2025system_log_parsing}.

I metodi convenzionali basati su \textbf{\textit{pattern matching}} e analisi sintattica mostrano limitazioni quando si confrontano con questa eterogeneità. Le \textbf{espressioni regolari}, pur essendo efficaci per l'identificazione di pattern strutturati ben definiti, faticano ad adattarsi alla variabilità linguistica e semantica dei log di sistemi complessi e tendono a richiedere una manutenzione continua delle regole man mano che evolvono i sistemi software~\cite{beck2025system_log_parsing}. Inoltre, l'evoluzione costante delle applicazioni introduce nuovi formati e pattern che rendono difficile generalizzare modelli creati ad hoc su singoli dataset~\cite{beck2025system_log_parsing,karlsen2023benchmarking_llm_log}.

In questo contesto i \textbf{Large Language Models} rappresentano un cambio di paradigma nell'approccio all'analisi dei log, introducendo capacità di comprensione contestuale e flessibilità semantica. Studi recenti mostrano che attraverso l'uso di LLM per il parsing dei log, con tecniche di \textit{few-shot tuning}\footnote{Per \textit{few-shot tuning} si intende un riaddestramento mirato del modello su un numero molto ridotto di esempi etichettati del compito specifico, tipicamente poche decine o centinaia di istanze.}, è possibile raggiungere una parsing accuracy media intorno al 96\% su 16 sistemi open-source eterogenei~\cite{ma2024llmparser}, mentre benchmark dedicati all'analisi di log applicativi e di sicurezza evidenziano che modelli \textbf{Transformer} \textit{fine-tuned}\footnote{Un modello \textit{fine-tuned} è un modello che è stato riaddestrato in maniera mirata per un determinato scopo} ottimizzati ulteriormente sul compito di analisi dei log possono superare lo stato dell'arte con un \textit{F1-Score} medio di 0{,}998, ovvero la media armonica tra \textbf{precision} e \textbf{recall}\footnote{La \textit{precision} misura la proporzione di elementi previsti come positivi che sono effettivamente positivi (pochi falsi positivi), mentre la \textit{recall} misura la proporzione di elementi positivi correttamente individuati dal modello (pochi falsi negativi).}, metrica ampiamente utilizzata per valutare modelli di classificazione~\cite{karlsen2023benchmarking_llm_log}.

Nonostante queste prestazioni notevoli, l'applicazione efficace dei LLM ai log richiede spesso un adattamento specifico al dominio. I log di sistema utilizzano terminologia tecnica, abbreviazioni e pattern sintattici che differiscono significativamente dal linguaggio naturale su cui i modelli sono tipicamente addestrati, generando un evidente \textbf{\textit{domain gap}}. Per affrontare questo problema, alcuni lavori propongono strategie di \textbf{\textit{continual pre-training}} su conoscenza interpretabile del dominio: il modello \textbf{SuperLog}, ad esempio, viene adattato a partire da LLM \textit{open weight} ottenendo un miglioramento medio del 12{,}01\% in accuracy rispetto al secondo miglior modello su quattro task diversi di analisi dei log~\cite{ji2024adapting_llm_log}.

Un ulteriore aspetto critico riguarda la lunghezza e la struttura delle sequenze da analizzare. Molti scenari reali richiedono di considerare finestre temporali estese o interi documenti di log per ricostruire il contesto di un evento. Studi sui modelli \textbf{Transformer} per la classificazione di documenti lunghi evidenziano come la progettazione del modello e la gestione efficiente del \textbf{\textit{context window}} siano fattori determinanti per mantenere prestazioni elevate su sequenze lunghe~\cite{dai2022revisiting_long_doc}.



%
%			CAPITOLO: Metodologia teorica
%

\chapter{Metodologia}
\label{chap:metodologia}

\section{Architettura proposta}
\label{sec:architettura_proposta}

L'architettura proposta consiste in un sistema di analisi continua dei log, progettato per ricevere flussi di registrazioni in tempo reale, elaborarle individualmente e classificarle in base alla presenza o assenza di dati sensibili. Il sistema opera in modalità \textit{streaming}, processando ciascuna riga di log non appena questa viene acquisita, restituendo un indicatore indicante l'esito dell'analisi.

Il cuore dell'architettura si basa su un \textbf{approccio ibrido} che combina due metodologie di analisi complementari. Il primo livello di elaborazione impiega tecniche di \textbf{analisi statica} basate su espressioni regolari, in grado di identificare \textit{pattern} strutturati e ricorrenti con elevata efficienza computazionale. Questo livello è ottimizzato per riconoscere dati sensibili che seguono formati ben definiti, quali indirizzi email, \textit{token} di autenticazione, coordinate geografiche e simili.

Il secondo livello sfrutta le capacità di comprensione contestuale dei \textit{Large Language Models} per analizzare dinamicamente il contenuto testuale delle righe di log. Questa componente è progettata per intercettare informazioni sensibili espresse in linguaggio naturale o che non seguono schemi sintattici predefinibili, colmando le lacune dell'analisi statica attraverso una valutazione semantica del contenuto.

L'integrazione dei due approcci permette di ottenere una \textbf{copertura completa}: l'analisi statica garantisce prestazioni elevate e risultati deterministici per \textit{pattern} noti, mentre l'analisi dinamica tramite LLM assicura flessibilità e adattabilità a contenuti complessi e variabili. Il sistema elabora ogni riga di log in sequenza, applicando entrambi i livelli di analisi e aggregando i risultati per produrre una classificazione finale affidabile ed esaustiva.

\section{Approccio all'identificazione dei dati sensibili}
\label{sec:approccio_identificazione}

L'approccio metodologico del progetto prevede l'utilizzo dei \textit{Large Language Models} sia come strumento di supporto durante lo sviluppo per la definizione e validazione delle espressioni regolari, sia come componente attiva integrata nel processo di analisi in tempo reale.

La strategia si basa sul principio che l'identificazione di dati sensibili \textbf{strutturati}, quali codici fiscali, numeri di telefono, indirizzi email, JWT o coordinate geografiche, può essere efficacemente realizzata tramite espressioni regolari opportunamente progettate. In questo contesto, gli LLM svolgono un ruolo di \textit{consulenza esperta} durante la fase di sviluppo, contribuendo a identificare \textit{pattern} complessi nei campioni di log, validare e ottimizzare le espressioni regolari analizzando le righe non catturate per verificare la presenza di dati sensibili non identificati, e gestire i casi limite individuando varianti e formati \textit{edge-case} che potrebbero sfuggire alle \textit{regex} iniziali.

Il \textit{workflow} previsto per la generazione delle espressioni regolari si articola attraverso diverse fasi successive. Inizialmente si procede con un'\textbf{analisi esplorativa}, durante la quale gli LLM vengono impiegati per esaminare campioni di log e identificare potenziali dati sensibili, inclusi formati non convenzionali. I dati sensibili identificati vengono quindi classificati in categorie omogenee, quali credenziali, informazioni personali e \textit{token} di autenticazione.

Segue una fase di \textbf{generazione iterativa}, in cui vengono create progressivamente espressioni regolari specifiche per ogni categoria, con il supporto dell'LLM per gestire variazioni e casi speciali. Le \textit{regex} vengono poi sottoposte a \textbf{test e raffinamento}, applicandole su \textit{dataset} più ampi e analizzando tramite LLM le righe non catturate, con l'obiettivo di raggiungere un punto in cui l'LLM non identifichi più dati sensibili nelle righe non rilevate dalle \textit{regex}. Infine, si procede al \textbf{consolidamento}, creando un set finale di espressioni regolari ottimizzate e pronte per l'utilizzo in produzione.

In questo approccio si identificano tuttavia diverse sfide significative. Alcuni tipi di dati sensibili, specialmente quelli espressi in linguaggio naturale, risultano difficilmente catturabili tramite \textit{regex}. Le espressioni regolari non possono comprendere il contesto semantico in cui un dato appare, potendo quindi generare \textbf{falsi positivi}. Inoltre, emerge la necessità di un aggiornamento continuo delle \textit{regex} per adattarsi a nuovi formati e variazioni nei log, e la gestione di un insieme crescente di \textit{regex} specializzate comporta complessità di manutenzione significative.

Queste considerazioni vengono in parte affrontate dalla scelta di un sistema ibrido, descritto nella sezione~\refwithpage{sec:ibrido_llm_regex}, che mantiene i vantaggi delle espressioni regolari integrandoli con le capacità di comprensione contestuale degli LLM.

\section{Integrazione ibrida di LLM e regex per l'identificazione in tempo reale}
\label{sec:ibrido_llm_regex}

L'approccio proposto integra un sistema ibrido che combina le espressioni regolari con i \textit{Large Language Models}, creando una soluzione ibrida per l'identificazione di dati sensibili in tempo reale.

Il primo livello ha un ruolo di \textbf{analisi sintattica}, cioè di identificazione rapida e deterministica di \textit{pattern} strutturati ben definiti mediante espressioni regolari (email, \textit{token} di autenticazione, coordinate geografiche, identificatori numerici, etc.).

Il secondo livello ha invece un ruolo di \textbf{analisi semantica}, quindi di comprensione contestuale di contenuti complessi tramite modelli linguistici, riuscendo quindi a rilevare dati sensibili espressi in linguaggio naturale o formati non convenzionali.

La prima fase consiste nell'\textbf{acquisizione}, durante la quale i log vengono ricevuti dal sistema centralizzato di gestione. Segue la \textbf{pre-elaborazione sintattica}, in cui un primo \textit{layer} applica espressioni regolari ottimizzate per identificare \textit{pattern} comuni e ad alta occorrenza; i log contenenti corrispondenze positive vengono immediatamente marcati come sensibili.

Successivamente avviene la fase di \textbf{instradamento}, dove i log vengono inoltrati al sistema di analisi semantica. Quest'ultimo procede con l'\textbf{analisi contestuale}, durante la quale il modello linguistico esamina ogni unità di log per identificare dati sensibili non rilevati dall'analisi sintattica iniziale, varianti e formattazioni non standard di dati già categorizzati, informazioni sensibili espresse in linguaggio naturale, e contesti in cui dati apparentemente innocui assumono natura sensibile.

La fase di \textbf{consolidamento} prevede che le identificazioni del modello linguistico vengano associate agli identificatori univoci dei log originali e strutturate per l'integrazione nel sistema di gestione. Infine, durante l'\textbf{aggregazione}, il sistema centrale combina i risultati provenienti da entrambi i \textit{layer} di analisi, creando una vista unificata di tutti i dati sensibili identificati e attivando eventuali meccanismi di notifica.

L'integrazione di analisi sintattica e semantica offre diversi vantaggi rispetto a soluzioni basate su una singola tecnologia. La \textbf{copertura completa} rappresenta il beneficio principale: la combinazione delle due metodologie permette di identificare sia dati strutturati che non strutturati, garantendo al contempo una \textbf{riduzione dei falsi negativi}, poiché i dati che sfuggono all'analisi sintattica vengono catturati dalla comprensione contestuale dei modelli linguistici.

L'architettura descritta garantisce \textbf{flessibilità evolutiva} del sistema, perché rende possibile formalizzare in espressioni regolari i nuovi \textit{pattern} identificati dai modelli linguistici, ottimizzando le elaborazioni future attraverso un processo di apprendimento incrementale.

L'architettura offre inoltre \textbf{resilienza}, poiché il fallimento di un \textit{layer} non compromette completamente la capacità di identificazione del sistema, e \textbf{adattabilità}, consentendo di calibrare il sistema in base alle esigenze specifiche e di privilegiare velocità o accuratezza mediante la regolazione del bilanciamento tra i due \textit{layer}.

L'approccio ibrido proposto richiede alcune considerazioni metodologiche fondamentali per garantirne l'efficacia operativa: La chiara distinzione tra analisi sintattica e semantica costituisce un principio architetturale fondamentale che permette di ottimizzare \textbf{indipendentemente} ciascun \textit{layer} secondo le proprie caratteristiche specifiche.

Questa separazione consente di sostituire o aggiornare singoli componenti senza impattare il funzionamento dell'intero sistema, facilitando l'\textbf{evoluzione} tecnologica e la \textbf{manutenzione} a lungo termine. Inoltre, questo approccio modulare permette di valutare separatamente le \textit{performance} di ogni metodologia attraverso metriche appropriate e di adattare dinamicamente la distribuzione del carico computazionale in base alle risorse disponibili e ai requisiti operativi.

La scelta metodologica di elaborare singolarmente ogni unità di log, anziché processarle in \textit{batch}, è motivata da diverse considerazioni architetturali. In termini di precisione, l'elaborazione unitaria garantisce che ogni riga riceva un'analisi dedicata senza interferenze da contesto irrilevante, massimizzando l'accuratezza dell'identificazione. Dal punto di vista della tracciabilità, questo approccio assicura una corrispondenza diretta e biunivoca tra \textit{input} e risultato dell'analisi, semplificando il \textit{debugging} e l'\textit{audit} dei processi. La latenza risulta inoltre controllata e uniforme, con tempi di risposta prevedibili che facilitano la pianificazione delle risorse. Infine, l'isolamento garantito dall'elaborazione unitaria previene la propagazione a cascata di eventuali errori o anomalie, aumentando la resilienza complessiva del sistema.

Questa architettura metodologica rappresenta la base teorica per un sistema di identificazione efficace ed efficiente, bilanciando precisione, prestazioni e manutenibilità, e costituisce il fondamento concettuale di \textit{Sensitive Data Detector}, descritto approfonditamente nei Capitoli~\refwithpage{chap:implementazione_tecnologie}~e~\refwithpage{chap:sensitive_data_detector}.


%
%			CAPITOLO: Implementazione e tecnologie
%

\chapter{Implementazione e tecnologie}
\label{chap:implementazione_tecnologie}

Il progetto \textit{Sensitive Data Detector} integra diverse tecnologie per creare una soluzione completa di analisi automatica dei log. L'architettura si basa su un approccio multi-componente che combina diversi strumenti.

Questo Capitolo descrive nel dettaglio le tecnologie scelte, motivando le decisioni architetturali e spiegando come ogni componente contribuisce al funzionamento complessivo del sistema.

\section{Scenario applicativo}
\label{sec:scenario_applicativo}
La necessità di implementare questo sistema è emersa dalle esigenze operative di un'azienda che opera nel settore dell'\textbf{\textit{IoT}}, sviluppando e integrando diversi micro-servizi destinati alla gestione domotica di \textbf{\textit{smart city}}. \\
L'integrazione di una moltitudine di sistemi software \textbf{eterogenei} ha comportato un incremento significativo sia del volume che della complessità dei log generati. \\
Inoltre, la natura composta, le dimensioni e la provenienza da fornitori terzi di molti di questi sistemi hanno reso impraticabile intervenire direttamente sul codice sorgente per eliminare alla fonte la generazione di log contenenti dati sensibili, introducendo quindi la necessità di un sistema \textbf{automatico}, \textbf{scalabile} e \textbf{preciso} per l'analisi dei log.

\clearpage

\subsection{Formato dei log}
\label{subsec:formato_log}

I log del sistema seguono il formato strutturato definito nella tabella~\refwithpage{tab:formato_log}. \\
Il sorgente~\refwithpage{lst:esempio_log} presenta un campione di cinque righe di log rappresentativo della struttura dati utilizzata.

\begin{table}[h!]
    \centering
    \begin{tabular}{|p{0.11\textwidth}|p{0.25\textwidth}|p{0.27\textwidth}|p{0.25\textwidth}|}
        \hline
        \textbf{Nome} & \textbf{Formato}   & \textbf{Descrizione}                                                    & \textbf{Esempio}                    \\
        \hline
        Nome file     & Stringa            & Nome del file di log compresso                                          & `messages.2.gz`                     \\
        \hline
        Data          & `MMM dd HH:mm:ss`  & Timestamp nel formato syslog                                            & `Apr  7 00:03:39`                   \\
        \hline
        ID            & `ID` + numero      & Identificativo univoco dell'istanza del software che ha generato il log & `ID24167`                           \\
        \hline
        Host+PID      & Stringa + `[PID]:` & Nome host e process ID del processo che ha generato il log              & `sw3-devaccess|55dacabaa 607[561]:` \\
        \hline
        Body          & Oggetto JSON       & Contenuto strutturato con `level` e `message`                           & `\{"level":"info",

        "message":"..."\}`                                                                                                                                 \\
        \hline
    \end{tabular}
    \caption{Formato strutturato dei log del sistema}
    \label{tab:formato_log}
\end{table}

\lstinputlisting[
caption=Esempio di log,
label=lst:esempio_log,
literate={è}{{\`e}}1 {à}{{\`a}}1 {ò}{{\`o}}1 {ù}{{\`u}}1 {ì}{{\`i}}1 {µ}{{$\mu$}}1
]{"../example_logs/raw_logs.txt"}

Poiché gli eventuali dati sensibili sono presenti esclusivamente nel contenuto \textbf{testuale} dei log, il sistema concentra l'analisi unicamente sul campo \textbf{\texttt{message}} del body JSON\footnote{JSON (JavaScript Object Notation) è un formato standard per la rappresentazione di dati strutturati.}, ignorando gli altri elementi strutturali.

\section{Elaborazione}
\label{sec:elaborazione_log}

Il livello di elaborazione costituisce il cuore del sistema ed è responsabile della \textbf{raccolta}, \textbf{gestione} e \textbf{analisi} dei log. Le tecnologie in questo layer sono state selezionate per garantire \textbf{scalabilità}, \textbf{affidabilità} e \textbf{precisione} nell'identificazione dei dati sensibili.

\subsection{GrayLog}
\label{subsec:graylog}

GrayLog\footnote{GrayLog \url{https://graylog.org}} rappresenta la piattaforma centrale per la gestione dei log del sistema. \\
Si tratta di una soluzione enterprise e open-source che fornisce funzionalità avanzate di raccolta, indicizzazione, ricerca e analisi dei log in tempo reale.

La scelta di GrayLog è stata motivata principalmente dal fatto che questa piattaforma viene già utilizzata nell'ambiente di produzione per il quale è stato sviluppato questo progetto, rendendo quindi l'integrazione di quest'ultimo una naturale estensione dell'infrastruttura esistente.

Oltre al vantaggio dell'integrazione con l'ambiente esistente, GrayLog presenta diverse caratteristiche tecniche che lo rendono ideale per questo progetto. La piattaforma offre un sistema di \textbf{raccolta centralizzata} con supporto nativo per diversi protocolli di logging, tra cui GELF\footnote{GELF (GrayLog Extended Log Format) è un formato strutturato progettato specificamente per il trasporto di log su reti TCP/UDP, che estende il formato syslog standard con campi aggiuntivi e supporto nativo per metadati.} e Syslog\footnote{Syslog è un protocollo standard per il logging di sistema definito dall'RFC 3164 (\url{https://datatracker.ietf.org/doc/html/rfc3164}), ampiamente utilizzato per la trasmissione di messaggi di log su reti IP.}, permettendo l'integrazione con sistemi eterogenei.

La capacità di \textbf{indicizzazione \textit{real-time}} consente di rendere i log immediatamente ricercabili dopo la ricezione, mentre il sistema di \textbf{\textit{pipeline} di \textit{processing}} fornisce regole configurabili per trasformare, filtrare e arricchire i log durante la fase di \textit{ingestion}. Le \textbf{API REST complete} offrono interfacce programmatiche per l'automazione della configurazione e l'integrazione con sistemi esterni. Infine, la \textbf{scalabilità orizzontale} garantita dall'architettura distribuita supporta \textit{cluster} multi-nodo per gestire volumi elevati di log.

In \textit{Sensitive Data Detector} GrayLog svolge un ruolo duplice, fungendo sia da \textbf{sorgente di dati} che da \textbf{destinazione per notifiche}. Come sorgente, riceve log dai sistemi di produzione tramite \textit{input} configurabili sulla porta \texttt{TCP 5555}; come destinazione, accoglie le segnalazioni di dati sensibili identificati dal sistema tramite la porta \texttt{TCP 5556}.

La configurazione utilizza stream e output \textbf{GELF} per creare una pipeline che instrada i log in ingresso verso il \textit{Sensitive Data Detector} tramite output GELF sulla porta \texttt{TCP 24367}, raccoglie le notifiche di ritorno per visualizzazione e alerting e applica regole regex aggiuntive per l'identificazione rapida di pattern comuni.

\subsection{Ollama}
\label{subsec:ollama}

Ollama\footnote{Ollama \url{https://ollama.com}} è un framework open-source progettato per semplificare l'esecuzione \textbf{locale} di Large Language Models.\\
Esso offre un'interfaccia uniforme per l'integrazione e l'orchestrazione di differenti modelli LLM.

Ollama si basa su un'architettura \textit{client-server} articolata in diverse componenti. Il \textbf{\textit{server} locale} opera come processo \textit{daemon} responsabile del caricamento e dell'esecuzione dei modelli LLM, esponendo un'\textbf{API REST} che fornisce un'interfaccia HTTP standardizzata per l'interazione con i modelli. La \textbf{gestione dei modelli} è semplificata attraverso comandi \textit{CLI} dedicati per il download, l'aggiornamento e la configurazione di modelli provenienti da \textit{repository} pubblici.

Un elemento particolarmente rilevante è il formato \textit{\textbf{Modelfile}}, che consente la creazione di modelli personalizzati a partire da modelli esistenti, incorporando \textit{prompt} di sistema specifici per il dominio applicativo. Per ulteriori dettagli riguardanti i \textit{Modelfile} si rimanda alla sezione~\refwithpage{subsec:llm}. Il \textit{framework} include inoltre \textbf{ottimizzazioni \textit{hardware}} con supporto all'esecuzione sia tramite CPU che GPU, oltre a configurazioni specifiche per diverse architetture, tra cui Metal\footnote{Metal è l'API grafica e di calcolo di Apple per macOS e iOS.} su macOS, e CUDA\footnote{CUDA è la piattaforma di calcolo parallelo di NVIDIA per GPU.}/ROCM\footnote{ROCM è la piattaforma \textit{open-source} di AMD per calcolo su GPU.} su Windows e GNU/Linux.

La scelta di Ollama ha portato diversi vantaggi architetturali significativi. Il \textbf{\textit{deployment} locale} elimina la dipendenza da servizi \textit{cloud} esterni, preservando la \textit{privacy} dei dati ed evitando la trasmissione di informazioni sensibili a terze parti. I \textbf{costi operativi ridotti}, derivanti dall'assenza di tariffe per \textit{token} o chiamate API, permettono elaborazioni intensive senza vincoli economici. La \textbf{latenza ottimizzata} garantita dalla comunicazione locale riduce significativamente i tempi di risposta, mentre la \textbf{flessibilità dei modelli} consente di sperimentare con diverse soluzioni senza vincoli commerciali.

\subsection{Large Language Models}
\label{subsec:llm}

La strategia prevede l'utilizzo di \textbf{modelli personalizzati} ottimizzati specificamente per il dominio dei log di sistema.

Il progetto supporta nove diversi modelli LLM selezionati per bilanciare performance, accuratezza e requisiti computazionali. I modelli sono elencati nella tabella~\refwithpage{tab:modelli_llm}.

\begin{table}[h!]
    \centering
    \begin{tabular}{|p{0.15\textwidth}|p{0.18\textwidth}|p{0.52\textwidth}|}
        \hline
        \textbf{Nome} & \textbf{Num. di token} & \textbf{Caratteristiche principali}                  \\ \hline
        Llama 3.1     & 8B                     & Modello generale con ottimo rapporto qualità/performance   \\ \hline
        Llama 3.2     & 3B                     & Versione compatta per ambienti con risorse limitate  \\ \hline
        Mistral       & 7B, 12B                & Modelli europei con architettura Mixture of Experts  \\ \hline
        Qwen 2.5      & 7B                     & Modello multilingue trainato su dati di svariati tipi \\ \hline
        Gemma 3       & 4B, 12B                & Modelli Google con ottimizzazioni per task specifici \\ \hline
        DeepSeek R1   & 7B, 8B                 & Modelli con capacità di reasoning avanzate           \\ \hline
    \end{tabular}
    \caption{Modelli LLM supportati dal sistema}
    \label{tab:modelli_llm}
\end{table}

I modelli Mistral sono stati sviluppati seguendo l'architettura \textbf{\textit{Mixture of Experts}}, cioè un approccio di machine learning che suddivide un modello di intelligenza artificiale in sottoreti specializzate chiamate "esperti", ognuna delle quali si concentra su un sottoinsieme specifico dei dati di input~\cite{mu2025moe_survey, ibm_moe}.

Mistral ha implementato questo approccio utilizzando 8 gruppi distinti di parametri: a ogni livello e per ogni token, una rete di instradamento seleziona due di questi esperti per elaborare il token, permettendo di utilizzare solo 12,9 miliardi di parametri sui 46,7 miliardi totali e ottenendo prestazioni superiori con maggiore efficienza computazionale~\cite{mistral_moe}.

I modelli DeepSeek R1 includono capacità di \textbf{reasoning}\footnote{Il reasoning negli LLM si riferisce alla capacità di generare esplicitamente il processo di ragionamento step-by-step che porta alla risposta finale. Questi modelli utilizzano tag speciali (come \texttt{<think>...</think>}) per separare il processo di ragionamento interno dalla risposta definitiva, permettendo maggiore trasparenza nel processo decisionale.} avanzate che permettono di visualizzare il processo di ragionamento interno del modello.

Ogni modello viene personalizzato attraverso file di configurazione \textit{Modelfile} che definiscono diversi aspetti fondamentali. Il \textbf{\textit{prompt} di sistema} contiene istruzioni specifiche per l'identificazione di dati sensibili nei log, mentre i \textbf{parametri ottimizzati} configurano la temperatura\footnote{La temperatura è un parametro che controlla la casualità delle risposte dell'LLM: valori bassi (0.0-0.3) producono risposte più deterministiche e conservative, mentre valori alti (0.7-1.0) aumentano la creatività ma riducono la coerenza.} e il \textit{context window}\footnote{Il \textit{context window} rappresenta il numero massimo di \textit{token} che un LLM può processare simultaneamente in una singola conversazione, inclusi \textit{prompt}, messaggi precedenti e risposta generata.} per massimizzare la precisione. Gli \textbf{esempi di \textit{training}} guidano il modello nel formato di \textit{output} atteso, mentre le \textbf{definizioni precise} specificano in dettaglio cosa costituisce un dato sensibile e cosa escludere dall'analisi.

Il \textit{prompt} di sistema incorpora linee guida specifiche per l'identificazione di email, \textit{token} di autenticazione e coordinate geografiche, per l'esclusione di dati non sensibili come \textit{username}, ID e \textit{timestamp}, per l'estrazione selettiva dei valori sensibili da oggetti JSON complessi e per la produzione di un formato di \textit{output} standardizzato che faciliti il \textit{parsing}.

Per ulteriori dettagli riguardanti i \textit{Modelfile} e la configurazione dei LLM si rimanda alla sezione~\refwithpage{subsubsec:ver2_prompt_engineering}.


\section{Codice}
\label{sec:codice}

Il livello di implementazione si basa su tecnologie consolidate che garantiscono \textbf{affidabilità}, \textbf{manutenibilità} e \textbf{portabilità} del sistema.

\subsection{Python}
\label{subsec:python}

Python\footnote{Python \url{https://www.python.org}} rappresenta il linguaggio di programmazione principale del progetto, scelto per la sua versatilità e l'ampio ecosistema di librerie specializzate per l'integrazione con sistemi di AI.

Il progetto utilizza Python 3.12, che garantisce supporto completo per la \textbf{programmazione asincrona} tramite il modulo \texttt{asyncio}, ottimizzazioni di \textit{performance} specifiche per applicazioni \textit{I/O intensive}, piena compatibilità con le librerie più recenti dell'ecosistema AI e funzionalità avanzate di \textit{type hinting} per migliorare la manutenibilità del codice.

Le librerie Python utilizzate comprendono \textbf{ollama}, la libreria ufficiale per l'interazione con il \textit{server} Ollama tramite API REST; \textbf{httpx}, un \textit{client} HTTP asincrono per comunicazioni non bloccanti; \textbf{anyio}, un \textit{framework} per programmazione asincrona \textit{cross-platform}; e \textbf{requests}, impiegata per la configurazione automatica di GrayLog tramite API REST.

Il core del sistema è implementato in tre versioni evolutive che rappresentano l'evoluzione del progetto dalla dimostrazione di fattibilità all'integrazione enterprise.

La \textbf{prima versione} costituisce un proof of concept per validare l'uso di LLM nell'identificazione di dati sensibili.

La \textbf{seconda versione} introduce sistemi di valutazione quantitativa e modelli personalizzati per migliorare scalabilità e accuratezza.

La \textbf{terza versione} implementa l'integrazione real-time con GrayLog per deployment in ambiente di produzione.

Per una descrizione completa dell'architettura, delle funzionalità e dei risultati di ogni versione si rimanda al Capitolo~\refwithpage{chap:sensitive_data_detector}.

Il file \texttt{\textbf{graylog\_setup.py}} automatizza la configurazione iniziale di GrayLog tramite le API REST per la creazione \textbf{programmatica} di input, stream, output e pipeline; adotta una configurazione \textbf{idempotente} che consente esecuzioni ripetute senza effetti collaterali; include meccanismi di gestione degli errori con validazione delle configurazioni e, dove previsto, rollback automatico; e supporta template personalizzabili per adattare le configurazioni a diversi ambienti.

In particolare, lo script provvede alla definizione degli input \texttt{TCP} per i log in ingresso e per le notifiche di ritorno, alla creazione di stream per il routing intelligente dei messaggi, alla configurazione di output GELF per la comunicazione con il \textit{Sensitive Data Detector} e all'impostazione di pipeline con regole regex per l'identificazione rapida di pattern comuni.

Il codice completo dello script è disponibile nell'appendice~\refwithpage{lst:code_graylog_script}.

\subsection{Docker}
\label{subsec:docker}

Docker\footnote{Docker \url{https://www.docker.com}} viene utilizzato per la containerizzazione dell'ambiente GrayLog, garantendo portabilità, isolamento e semplicità di deployment.

La configurazione \textbf{Docker Compose} definisce un ambiente completo composto da tre servizi principali: \textbf{MongoDB}\footnote{MongoDB \url{https://www.mongodb.com}}, che funge da \textit{database} per la memorizzazione dei metadati di configurazione di GrayLog; \textbf{GrayLog DataNode}, il nodo di indicizzazione responsabile dello \textit{storage} e della ricerca dei log; e \textbf{GrayLog Enterprise}, il \textit{server} principale che espone l'interfaccia \textit{web} e le API REST.

Il codice completo della configurazione del \textit{container} è disponibile nell'appendice~\refwithpage{lst:code_container_config}.

L'ambiente Docker implementa diverse soluzioni per garantire sicurezza e affidabilità. Una \textbf{rete isolata}, realizzata tramite \textit{bridge network} dedicata, assicura la comunicazione sicura tra i \textit{container}. I \textbf{volumi persistenti} garantiscono lo \textit{storage} permanente per i dati di MongoDB, gli indici di GrayLog e le configurazioni. Il \textbf{\textit{port mapping}} consente l'esposizione selettiva delle sole porte necessarie per l'integrazione con sistemi esterni. Infine, gli \textbf{\textit{health checks}} permettono il monitoraggio automatico dello stato dei servizi, attivando le \textit{restart policies} configurate in caso di malfunzionamenti.

L'utilizzo di Docker porta diversi benefici, tra cui un \textit{deployment} semplificato che consente l'avvio dell'intero stack con un singolo comando, l'isolamento delle dipendenze per prevenire conflitti con software preesistente sull'host, la portabilità cross-platform con comportamento consistente su Windows, macOS e Linux, la scalabilità orizzontale per replicare l'ambiente a fini di testing o di deployment distribuito e una gestione puntuale delle versioni dei componenti utilizzati.


%
%			CAPITOLO: Il lavoro svolto
%

\chapter{\textit{Sensitive Data Detector}}
\label{chap:sensitive_data_detector}

Il progetto \textit{Sensitive Data Detector} rappresenta il cuore del sistema ed è stato sviluppato attraverso un \textbf{processo iterativo} che ha portato alla realizzazione di tre versioni evolutive, ognuna con obiettivi specifici e caratteristiche tecniche distinte.

Questo approccio progressivo ha permesso di affrontare le sfide tecniche in modo sistematico, partendo dalla validazione del concetto di base fino all'implementazione di una soluzione pronta per il rilascio in \textbf{produzione}. Ogni versione introduce significativi miglioramenti in termini di scalabilità, accuratezza e facilità di integrazione in ambienti esistenti rispetto alla precedente.

\subsubsection{Versione 1: Proof of Concept}

La prima implementazione costituisce un proof of concept sviluppato per dimostrare la fattibilità dell'utilizzo di Large Language Models nell'identificazione automatica di dati sensibili all'interno dei file di log.

Questa versione utilizza un'architettura \textbf{sincrona} con elaborazione \textbf{\textit{batch}}, interfacciandosi direttamente con l'API Ollama per analizzare file di log pre-esistenti. Il sistema impiega LLM base senza personalizzazione, guidati esclusivamente da un prompt iniziale esterno e implementa strategie di gestione manuale del context window con ripristino periodico.

L'output viene salvato su file per consentire analisi \textbf{offline} e tracciabilità del processo.

Ulteriori dettagli sono disponibili nella sezione~\refwithpage{sec:ver1}.

\subsubsection{Versione 2: Streaming e valutazione}

La seconda versione rappresenta un'evoluzione significativa con focus sulla scalabilità e la valutazione \textbf{quantitativa} delle performance. Introduce l'elaborazione \textbf{riga per riga} dei log per ottimizzare l'uso della memoria e utilizza modelli LLM personalizzati creati tramite \textit{Modelfile} di Ollama, che incorporano prompt specifici per l'identificazione di dati sensibili.

Un aspetto fondamentale di questa versione è l'implementazione di un sistema di \textbf{\textit{ground truth}}\footnote{Ground truth indica i dati di riferimento considerati corretti e utilizzati come standard per valutare la precisione di un sistema di classificazione. Nel contesto di questo progetto, consiste in etichette manuali che indicano se ogni riga di log contiene o meno dati sensibili.} per la valutazione automatica, che consente il calcolo real-time di metriche di classificazione \textbf{binaria} (veri/falsi positivi/negativi) e delle metriche derivate \textit{precision} e \textit{recall}, oltre all'analisi comparativa di diversi modelli LLM.

Ulteriori dettagli sono disponibili nella sezione~\refwithpage{sec:ver2}.

\subsubsection{Versione 3: Integrazione enterprise}

La terza versione costituisce la soluzione \textit{production-ready} del progetto, implementando un'architettura \textbf{asincrona} completa per l'integrazione real-time con GrayLog. \\
Utilizza programmazione asincrona con \texttt{asyncio} per gestione concorrente delle connessioni, protocollo GELF per comunicazione nativa con la piattaforma di gestione log e un server TCP non bloccante per elaborazione in tempo reale.

Il sistema implementa un \textit{feedback loop} bidirezionale che reinvia automaticamente i risultati delle analisi a GrayLog, permettendo la visualizzazione unificata di log originali e notifiche di rilevamento di dati sensibili attraverso le sue dashboard specializzate.

Ulteriori dettagli sono disponibili nella sezione~\refwithpage{sec:ver3}.


\section{Versione 1: Proof of Concept}
\label{sec:ver1}

La prima versione del \textit{Sensitive Data Detector} rappresenta un proof of concept sviluppato per dimostrare la fattibilità dell'utilizzo di Large Language Models nell'identificazione automatica di dati sensibili all'interno dei file di log. \\
Questa versione costituisce la base concettuale dell'intero progetto, implementando un approccio diretto e semplificato per l'analisi di file di log di dimensioni contenute.

\subsubsection{Obiettivi e scopo}
\label{subsubsec:ver1_obiettivi}

L'obiettivo principale di questa prima implementazione è verificare l'efficacia dei modelli LLM nell'identificazione di informazioni sensibili presenti nei log di sistema. La versione si concentra sull'analisi di file di log pre-esistenti, utilizzando un approccio \textbf{batch} che processa il contenuto in modo sequenziale.

Il sistema è progettato per riconoscere diverse tipologie di dati sensibili, tra i quali \textbf{informazioni personali identificabili} come i PII, \textbf{credenziali di accesso e token di autenticazione}, \textbf{indirizzi email e informazioni di contatto}, e \textbf{chiavi crittografiche e certificati}.

\subsubsection{Architettura e funzionamento}
\label{subsubsec:ver1_architettura}

Il sistema è implementato tramite linguaggio Python e utilizza la libreria \texttt{ollama}\footnote{\texttt{ollama} \url{https://pypi.org/project/ollama}} per interfacciarsi con diversi modelli LLM tramite API REST. L'architettura segue un paradigma client-server dove lo script Python agisce come client che invia richieste al server Ollama locale. L'implementazione completa è disponibile nell'appendice~\refwithpage{lst:code_ver1}.

Il flusso di elaborazione si articola nelle seguenti fasi:

\begin{enumerate}
    \item \textbf{Inizializzazione}: Lo script carica il prompt di sistema da un file esterno, lo invia all'LLM come messaggio e configura i parametri specifici per il modello selezionato
    \item \textbf{Lettura dei log}: Il sistema legge il file di log dall'archivio di esempi e lo suddivide in blocchi di dimensioni arbitrarie specifiche per il modello selezionato
    \item \textbf{Elaborazione batch}: Ogni blocco viene inviato all'LLM per l'analisi
    \item \textbf{Raccolta risultati}: Le risposte del modello vengono aggregate e formattate
    \item \textbf{Salvataggio output}: I risultati vengono salvati in file per consentire tracciabilità e analisi successive
\end{enumerate}

\subsubsection{Gestione del context window}
\label{subsubsec:ver1_context_window}

Un aspetto critico dell'implementazione riguarda la gestione del context window limitato dei modelli LLM. Il sistema implementa due strategie principali: \textbf{Segmentazione dei dati}, in cui i file di log vengono suddivisi in blocchi di dimensioni adattive basate sulle capacità del modello, e \textbf{Ripristino periodico della conversazione}, per cui, dopo un numero predefinito di messaggi, la cronologia della chat viene ripristinata per evitare il superamento dei limiti di token. Questo approccio garantisce che il modello mantenga sempre accesso al prompt iniziale e alle linee guida per l'identificazione dei dati sensibili.

\subsubsection{Prompt engineering}
\label{subsubsec:ver1_prompt_engineering}

Il prompt di sistema è stato progettato per guidare il modello nell'identificazione precisa dei dati sensibili, fornendo \textbf{definizioni chiare} di cosa costituisce un dato sensibile, \textbf{esempi specifici} di dati da identificare come token, credenziali e informazioni personali, \textbf{esclusioni esplicite} per dati non sensibili (quali username, ID e timestamp), un \textbf{formato di output strutturato} in bullet point e un \textbf{esempio pratico} con input e output atteso.

Il prompt include inoltre istruzioni per la gestione di oggetti JSON, richiedendo al modello di estrarre solo i dati sensibili contenuti all'interno di essi e non l'intero oggetto. \\
Il prompt completo è disponibile nell'appendice~\refwithpage{lst:prompt_ver1}.

\subsubsection{Modelli supportati e configurazione}
\label{subsubsec:ver1_modelli_supportati}

La prima versione supporta cinque diversi modelli LLM, ognuno con parametri di configurazione ottimizzati, come mostrato nella tabella~\refwithpage{tab:modelli_llm_ver1}.

\begin{table}[h!]
    \centering
    \begin{tabular}{|l|c|r|c|}
        \hline
        \textbf{Modello} & \textbf{Parametri} & \textbf{Righe per messaggio} & \textbf{Reset chat} \\ \hline
        Llama3           & 8B                 & 3                            & Ogni messaggio      \\ \hline
        Llama3           & 70B                & 10                           & Ogni 5 messaggi     \\ \hline
        Llama3.1         & 8B                 & 3                            & Ogni messaggio      \\ \hline
        Llama3.1         & 70B                & 10                           & Ogni 5 messaggi     \\ \hline
        Command-R        & 35B                & 5                            & Ogni 5 messaggi     \\ \hline
    \end{tabular}
    \caption{Configurazioni ottimizzate per modello nella versione 1}
    \label{tab:modelli_llm_ver1}
\end{table}
\todo{TODO: ri-verificare tabella}

\todo{TODO spiegare che il numero di righe per messaggio e reset chat è stato deciso a tentativi, notando che tenendolo troppo alto gli LLM perdevano il prompt iniziale e quindi finivano a non rispettare più la richiesta originale Con riferimento a sottosezione subsec metodologia\_v1}

La distinzione dei parametri riflette le diverse capacità computazionali e di context window dei modelli. I modelli più grandi con 70 miliardi di parametri possono processare blocchi più ampi di dati e mantenere conversazioni più lunghe prima del ripristino del contesto.

\subsubsection{Output e tracciabilità}
\label{subsubsec:ver1_output}

Il sistema genera file di output timestampati che includono \textbf{metadati dell'esecuzione} come timestamp, modello utilizzato e file di log analizzato, il \textbf{prompt iniziale utilizzato}, le \textbf{risposte complete dell'LLM} e \textbf{statistiche di elaborazione} quali righe processate e tempo di esecuzione.

Questa struttura consente la tracciabilità completa del processo di analisi e facilita il confronto tra diversi modelli e configurazioni.

\subsubsection{Limitazioni della versione}
\label{subsubsec:ver1_limitazioni}

La prima versione presenta alcune limitazioni che hanno orientato lo sviluppo delle versioni successive. La \textbf{scalabilità limitata} costituisce una criticità significativa, poiché il processamento batch sequenziale non è ottimale per file di log di grandi dimensioni. Inoltre, la \textbf{mancanza di integrazione real-time} rappresenta un vincolo funzionale rilevante, dato che il sistema opera esclusivamente su file pre-esistenti senza capacità di elaborazione in tempo reale. Infine, l'\textbf{assenza di metriche di valutazione} impedisce una valutazione oggettiva delle performance, non essendo implementati meccanismi automatici per misurare l'accuratezza delle identificazioni.

Nonostante queste limitazioni, la prima versione ha fornito risultati promettenti che hanno giustificato lo sviluppo delle versioni successive più avanzate.

\section{Versione 2: Elaborazione di batch di log}
\label{sec:ver2}

La seconda versione del \textit{Sensitive Data Detector} rappresenta un'evoluzione significativa del proof of concept, introducendo un sistema di valutazione automatica delle performance e l'utilizzo di modelli LLM personalizzati ottimizzati per il compito specifico.

Questa versione si concentra sull'elaborazione efficiente di file di log di dimensioni maggiori tramite streaming delle righe e implementa metriche di valutazione quantitative.

\subsubsection{Obiettivi e scopo}
\label{subsubsec:ver2_obiettivi}

L'obiettivo principale di questa seconda implementazione è duplice: da un lato migliorare la scalabilità del sistema per gestire volumi di log più elevati, dall'altro introdurre un framework di valutazione automatica per misurare obiettivamente le performance dei modelli LLM.

La versione introduce diversi miglioramenti sostanziali: l'\textbf{elaborazione riga per riga} per ottimizzare l'uso della memoria e semplificare il processo di rilevazione della correttezza delle risposte, l'\textbf{utilizzo di modelli personalizzati} con prompt engineering avanzato, un \textbf{sistema di ground truth} per la valutazione automatica e il \textbf{calcolo di metriche di classificazione binaria} quali veri/falsi positivi/negativi, accuratezza complessiva e metriche derivate come \textit{precision} e \textit{recall}.

\subsubsection{Architettura e funzionamento}
\label{subsubsec:ver2_architettura}

L'architettura della seconda versione mantiene il paradigma client-server della versione precedente e al contempo introduce ottimizzazioni nel flusso di elaborazione. \\
Il sistema utilizza modelli LLM personalizzati creati tramite \textit{Modelfile}, una funzionalità di Ollama, che incorporano prompt specifici e configurazioni ottimizzate per l'identificazione di dati sensibili.\\
L'implementazione completa è disponibile nell'appendice~\refwithpage{lst:code_ver2}.
\todo{TODO specificare meglio cosa sono i modelfile, assicurarsi che "funzionalità di Ollama" non sia presente altrove. In caso, spostare la spiegazione in quel punto}

Il flusso di elaborazione ottimizzato si articola nelle seguenti fasi:

\begin{enumerate}
    \item \textbf{Inizializzazione del dataset}: Il sistema carica file di log pre-annotati con etichette ground truth (Y/N) per ogni riga
    \item \textbf{Streaming sequenziale}: Ogni riga viene processata individualmente, eliminando la necessità di caricare grandi volumi in memoria
    \item \textbf{Preprocessamento}: Le etichette di ground truth vengono estratte e conservate per la valutazione, mentre il contenuto della riga di log viene inviato al modello
    \item \textbf{Analisi LLM}: Il modello personalizzato analizza la singola riga e produce una classificazione binaria (sensibile/non sensibile)
    \item \textbf{Post-processing}: Le risposte vengono elaborate per rimuovere eventuali tag di reasoning e standardizzare il formato
    \item \textbf{Valutazione in tempo reale}: Ogni risposta viene immediatamente confrontata con il ground truth e le metriche vengono aggiornate
    \item \textbf{Gestione della memoria}: Il contesto viene ripristinato periodicamente per mantenere performance ottimali indefinitamente, anche su file di log di grandi dimensioni
\end{enumerate}

\subsubsection{Prompt engineering}
\label{subsubsec:ver2_prompt_engineering}

La seconda versione introduce un approccio di prompt engineering più sofisticato attraverso l'utilizzo di modelli personalizzati. \\
A differenza della prima versione, dove il prompt veniva inviato ad ogni interazione, i modelli personalizzati incorporano le istruzioni direttamente nella loro configurazione tramite \textit{Modelfile}.

Questa strategia offre vantaggi significativi: garantisce \textbf{consistenza} mantenendo il prompt sempre presente nel contesto del modello ed eliminando la variabilità dovuta alla gestione manuale, migliora l'\textbf{efficienza} riducendo la lunghezza dei messaggi inviati e ottimizzando l'utilizzo del context window, permette la \textbf{specializzazione} ottimizzando ogni modello con prompt specifici per le sue caratteristiche architetturali e supporta il \textbf{reasoning avanzato} per modelli con capacità di ragionamento che utilizzano tag speciali come \texttt{<think>}.

Il sistema implementa anche un meccanismo di post-processing per gestire le risposte dei modelli reasoning, rimuovendo automaticamente i tag \texttt{<think>...</think>} che contengono il processo di ragionamento interno del modello, considerando solamente la risposta finale.

\subsubsection{Modelli supportati e configurazione}
\label{subsubsec:ver2_modelli_supportati}

La seconda versione espande significativamente il supporto per diversi modelli LLM, introducendo nove modelli personalizzati ottimizzati specificamente per l'identificazione di dati sensibili. \\
Tutti i modelli condividono una configurazione unificata ottimizzata per l'elaborazione sequenziale di singole righe di log, come mostrato nella tabella~\refwithpage{tab:modelli_llm_ver2}.

\begin{table}[h!]
    \centering
    \begin{tabular}{|l|c|r|c|}
        \hline
        \textbf{\textit{Modelfile} basato su} & \textbf{Parametri} & \textbf{Righe per messaggio} & \textbf{Reset chat} \\ \hline
        Llama3.1                              & 8B                 & 1                            & Ogni 15 messaggi    \\ \hline
        Llama3.2                              & 3B                 & 1                            & Ogni 15 messaggi    \\ \hline
        Mistral                               & 7B                 & 1                            & Ogni 15 messaggi    \\ \hline
        Mistral Nemo                          & 12B                & 1                            & Ogni 15 messaggi    \\ \hline
        Qwen2.5                               & 7B                 & 1                            & Ogni 15 messaggi    \\ \hline
        Gemma3                                & 4B                 & 1                            & Ogni 15 messaggi    \\ \hline
        Gemma3                                & 12B                & 1                            & Ogni 15 messaggi    \\ \hline
        DeepSeek R1                           & 7B                 & 1                            & Ogni 15 messaggi    \\ \hline
        DeepSeek R1                           & 8B                 & 1                            & Ogni 15 messaggi    \\ \hline
    \end{tabular}
    \caption{Configurazioni ottimizzate per modello nella versione 2 e 3}
    \label{tab:modelli_llm_ver2}
\end{table}
\todo{TODO: ri-verificare tabella}

A differenza della prima versione, tutti i modelli utilizzano una configurazione unificata con elaborazione di una riga per messaggio e ripristino della conversazione ogni 15 messaggi. Questa standardizzazione facilita il confronto diretto delle performance tra modelli diversi e ottimizza l'utilizzo delle risorse computazionali. \\
Inoltre, l'elaborazione di singole righe consente una valutazione immediata dell'accuratezza delle classificazioni prodotte dal modello.
\todo{TODO spiegare che gestendo una riga alla volta si ottiene una risposta "decisa" dall'LLM per ogni singola riga. Mandando più righe alla volta bisogna parsare la risposta per capire quali righe l'LLM reputa sensibili. Inoltre aumenta il rischio che l'LLM non risponda correttamente o che salti qualche riga}

\subsubsection{Output e tracciabilità}
\label{subsubsec:ver2_output}

La seconda versione introduce un sistema di output e tracciabilità migliorato, con particolare focus sulle metriche di valutazione quantitative. Il sistema genera report dettagliati che includono sia le informazioni di base dell'esecuzione che analisi statistiche delle performance di classificazione.

I file di output contengono le seguenti informazioni:
\begin{itemize}
    \item \textbf{Metadati dell'esecuzione}:
        \begin{itemize}
            \item timestamp
            \item modello utilizzato
            \item file di log analizzato
            \item numero totale di righe processate
        \end{itemize}
    \item \textbf{Metriche di classificazione}:
        \begin{itemize}
            \item veri positivi
            \item veri negativi
            \item falsi positivi
            \item falsi negativi
        \end{itemize}
    \item \textbf{Statistiche di accuratezza}:
        \begin{itemize}
            \item percentuale di risposte corrette
            \item percentuale di risposte errate
        \end{itemize}
    \item \textbf{Log dettagliato}: traccia completa di tutte le interazioni con l'LLM
\end{itemize}

Il sistema calcola e visualizza in tempo reale le metriche di performance durante l'elaborazione, fornendo feedback immediato sull'efficacia del modello. \\
Questa funzionalità è particolarmente utile per confronti comparativi tra diversi modelli e per l'ottimizzazione dei parametri di configurazione.

\subsubsection{Limitazioni della versione}
\label{subsubsec:ver2_limitazioni}

Nonostante i significativi miglioramenti rispetto alla prima versione, la seconda implementazione presenta ancora alcune limitazioni che hanno guidato lo sviluppo della versione successiva.

La principale criticità riguarda la \textbf{dipendenza da dataset pre-annotati}, in quanto il sistema di valutazione richiede file di log manualmente etichettati, limitando la scalabilità dell'approccio di testing. Inoltre, la modalità di \textbf{elaborazione offline} rappresenta un vincolo significativo, poiché il sistema opera esclusivamente su file pre-esistenti senza capacità di integrazione con sistemi di logging in tempo reale.

Queste limitazioni, sebbene non compromettano l'utilità della versione per scopi di ricerca e testing, evidenziano la necessità di un'integrazione più robusta con sistemi di produzione, obiettivo che viene affrontato nella terza versione del progetto.

\section{Versione 3: Integrazione con GrayLog}
\label{sec:ver3}

La terza versione del \textit{Sensitive Data Detector} rappresenta l'evoluzione finale del progetto, trasformando il sistema da uno strumento di analisi offline a una soluzione completamente integrata per l'elaborazione in tempo reale dei log.

Questa versione si interfaccia direttamente con GrayLog, creando un sistema \\
\textbf{\textit{end-to-end}} per il monitoraggio continuo dei log.

\subsubsection{Obiettivi e scopo}
\label{subsubsec:ver3_obiettivi}

L'obiettivo principale della terza versione è fornire una soluzione pronta per il rilascio in produzione per l'identificazione automatica di dati sensibili in ambienti operativi reali. Questa implementazione affronta le principali limitazioni delle versioni precedenti introducendo funzionalità chiave per contesti enterprise.

Il sistema implementa innanzitutto l'\textbf{elaborazione in tempo reale}, permettendo l'analisi immediata dei log non appena vengono generati dai sistemi. L'interfacciamento nativo con GrayLog consente l'inserimento immediato negli stack tecnologici esistenti, mentre l'architettura asincrona garantisce la \textbf{scalabilità operativa} necessaria per gestire volumi elevati di log in produzione. Il sistema include inoltre un \textbf{feedback loop} attraverso il quale i risultati delle analisi vengono automaticamente rimandati a GrayLog, permettendo di visualizzarli tramite interfaccia integrata. Infine, la \textbf{configurazione centralizzata} consente la gestione unificata di input, processing pipeline e output direttamente attraverso l'interfaccia GrayLog.

\subsubsection{Architettura e funzionamento}
\label{subsubsec:ver3_architettura}

L'architettura della terza versione introduce un cambio del paradigma rispetto alle versioni precedenti, passando da un modello client-server a un sistema basato su stream processing. Il sistema opera come un micro-servizio che si posiziona tra GrayLog e i modelli LLM, fungendo da ponte per l'analisi dei log. \\
L'implementazione completa è disponibile nell'appendice~\refwithpage{lst:code_ver3}.

Il flusso di elaborazione real-time si articola nelle seguenti fasi:

\begin{enumerate}
    \item \textbf{Ricezione stream}: Il sistema ascolta sulla porta \texttt{TCP 24367} per ricevere log in formato GELF da GrayLog
    \item \textbf{Parsing dei messaggi}: Ogni messaggio GELF viene deserializzato per estrarre contenuto e metadati del log
    \item \textbf{Analisi LLM}: Il contenuto viene inviato al modello personalizzato per l'identificazione di dati sensibili
    \item \textbf{Gestione risultati}: Le risposte positive vengono elaborate e formattate con gli identificatori originali del log
    \item \textbf{Notifica a GrayLog}: I risultati vengono reinviati a GrayLog sulla porta \texttt{TCP 5556} per integrazione nei flussi di alerting
    \item \textbf{Gestione della memoria}: Il contesto conversazionale dell'LLM viene periodicamente ripristinato per mantenere performance costanti
\end{enumerate}

La comunicazione avviene tramite protocollo GELF, che garantisce la preservazione di tutti i metadati necessari per la tracciabilità e la risoluzione di eventuali problemi.

\todo{TODO: esempio di messaggio GELF (vedere se magari spostare in capitolo 4)}

\subsubsection{Integrazione con GrayLog}
\label{subsubsec:ver3_integrazione}

L'integrazione con GrayLog avviene attraverso una configurazione di input, stream e output che crea una pipeline completa di elaborazione:

\begin{itemize}
    \item \textbf{Input \texttt{TCP 5555}}: Riceve i log originali dai sistemi sorgente
    \item \textbf{Stream di routing}: Indirizza i log verso il Sensitive Data Detector tramite output GELF
    \item \textbf{Input \texttt{TCP 5556}}: Riceve le notifiche di dati sensibili dal sistema di analisi
    \item \textbf{Pipeline di processing}: Applica regole aggiuntive per categorizzazione e alerting
\end{itemize}

Questa architettura permette di mantenere separati i log originali dalle notifiche di sicurezza, facilitando la creazione di dashboard specializzate e la configurazione di alert specifici per la conformità ai regolamenti sulla privacy.

\subsubsection{Modelli supportati e configurazione}
\label{subsubsec:ver3_modelli_supportati}

La terza versione eredita il supporto completo per tutti i modelli personalizzati della versione precedente, mantenendo la stessa configurazione unificata e ottimizzata per l'elaborazione in tempo reale, come mostrato nella tabella~\refwithpage{tab:modelli_llm_ver2}.

La configurazione real-time mantiene l'elaborazione di un messaggio per volta per semplificare il processo di rilevazione della correttezza delle risposte. \\
L'analisi di un messaggio alla volta permette inoltre di ottimizzare la latenza di risposta, aspetto critico in ambienti di produzione dove la velocità di identificazione delle vulnerabilità può essere determinante per la sicurezza del sistema.

\subsubsection{Programmazione asincrona e scalabilità}
\label{subsubsec:ver3_asincrona}

La terza versione introduce l'uso di programmazione asincrona tramite \texttt{asyncio}\footnote{\texttt{asyncio} \url{https://docs.python.org/3/library/asyncio.html}}, permettendo la gestione concorrente di multiple connessioni senza bloccare l'elaborazione. Questa architettura offre vantaggi significativi in termini di performance e affidabilità.

Il sistema implementa \textbf{gestione concorrente} per elaborazione parallela di log provenienti da fonti multiple, garantisce \textbf{resilienza} attraverso isolamento degli errori per singola connessione senza impatto sul sistema globale, ottimizza l'\textbf{efficienza delle risorse} mediante utilizzo ottimale della CPU attraverso I/O non bloccante, e permette \textbf{scalabilità orizzontale} con possibilità di deployment di multiple istanze per load balancing.

\subsubsection{Output e tracciabilità}
\label{subsubsec:ver3_output}

Il sistema di output della terza versione è progettato per l'integrazione operativa, producendo messaggi strutturati che vengono inviati direttamente in GrayLog.

Il sistema genera \textbf{notifiche real-time} in cui ogni identificazione di dato sensibile produce immediatamente una notifica a GrayLog, garantisce la \textbf{preservazione degli identificatori} mantenendo il collegamento con i log originali tramite ID univoci, utilizza un \textbf{formato standardizzato} con output strutturato compatibile con pipeline di processing esistenti e implementa il \textbf{logging delle operazioni} per tracciatura completa delle attività ai fini di audit e debugging.

Le notifiche seguono il formato: \\
\texttt{<log\_id>: <tipo\_dato\_sensibile> <contenuto\_identificato>}, facilitando la correlazione con i log originali e l'automazione delle azioni correttive.

\subsubsection{Limitazioni e considerazioni operative}
\label{subsubsec:ver3_limitazioni}

Nonostante rappresenti una soluzione pronta per ambienti di produzione, la terza versione presenta alcune considerazioni operative importanti.

Il sistema è \textbf{dipendente da Ollama}, richiedendo un'istanza sempre attiva che introduce un \textit{single point of failure}. La \textbf{latenza variabile} rappresenta un'altra criticità, poiché i tempi di risposta dipendono dalle performance dell'LLM selezionato e dal carico del sistema. Inoltre, la \textbf{scalabilità verticale} è limitata dalle risorse hardware disponibili per l'inferenza dell'LLM.

Relativamente all'\textbf{utilizzo di modelli personalizzati}, sebbene questi migliorino significativamente le performance rispetto ai modelli base, il sistema non utilizza tecniche di \textit{fine-tuning} avanzate come LoRA\footnote{LoRA (Low-Rank Adaptation) è una tecnica di fine-tuning efficiente che modifica solo un sottoinsieme dei parametri del modello e permette di creare un nuovo LLM a partire da uno già addestrato.}, che rappresenterebbe la soluzione ottimale per l'adattamento specifico al dominio dei log. Per ulteriori dettagli, si rimanda alla sezione~\refwithpage{subsubsec:addestramento_lora}.

Queste limitazioni, pur non compromettendo l'utilità operativa del sistema, rappresentano aree di miglioramento e suggeriscono direzioni per sviluppi futuri del progetto.



%
%			CAPITOLO: Test
%

\chapter{Test e valutazione}
\label{chap:test}

Il sistema \textit{Sensitive Data Detector} è stato sottoposto a un processo di testing e valutazione sistematico attraverso due fasi evolutive principali, corrispondenti alle prime due versioni del progetto. Ogni fase ha introdotto metodologie di test sempre più raffinate e metriche di valutazione quantitative per misurare l'efficacia del sistema nell'identificazione di dati sensibili.

La terza versione del sistema, pur rappresentando un'evoluzione significativa dal punto di vista architetturale e operativo, non è stata sottoposta a un processo di testing sistematico analogo alle versioni precedenti. \\
Questa scelta è motivata da considerazioni sia tecniche che metodologiche. Da un lato, sussiste un'equivalenza algoritmica con la seconda versione: la logica di identificazione dei dati sensibili rimane sostanzialmente invariata e vengono utilizzati gli stessi LLM e i medesimi \textit{Modelfile}, riducendo il valore informativo di una nuova campagna di test controllati. Dall'altro, la natura operativa della terza versione, che elabora direttamente stream di log provenienti da GrayLog, rende impraticabile l'impiego di dataset pre-annotati con ground truth. \\
Inoltre, l'integrazione con sistemi reali introduce variabili ambientali non controllabili che comprometterebbero la riproducibilità e l'affidabilità di misure quantitative.

\section{Metodologia di test}
\label{sec:metodologia_test}

La metodologia di test si è evoluta significativamente tra le due versioni principali del sistema, passando da un approccio qualitativo basato su osservazione manuale a un sistema quantitativo automatizzato.

\subsubsection{Versione 1: Testing esplorativo}
\label{subsubsec:metodologia_v1}

La prima versione ha utilizzato un approccio esplorativo per validare la fattibilità dell'utilizzo di Large Language Models nell'identificazione di dati sensibili. Il testing si è concentrato sull'\textbf{analisi qualitativa delle risposte}, dove ogni output del modello LLM veniva analizzato manualmente per identificare la presenza di falsi positivi e falsi negativi. Parallelamente, sono stati condotti test di diverse combinazioni di parametri, come il numero di righe per messaggio e i messaggi per chat, per l'\textbf{ottimizzazione delle configurazioni} e massimizzare l'efficacia del sistema. La valutazione dei vincoli di memoria e performance sull'architettura hardware utilizzata ha permesso di \textbf{gestire le limitazioni hardware}, mentre l'analisi del comportamento del sistema su dataset di dimensioni crescenti (100 e 1000 righe) ha fornito indicazioni sulla \textbf{scalabilità} della soluzione.

L'approccio della prima versione ha evidenziato problematiche critiche legate alla gestione del context window limitato dei modelli LLM, portando allo sviluppo di strategie di segmentazione e ripristino delle conversazioni.

\subsubsection{Versione 2: Testing quantitativo}
\label{subsubsec:metodologia_v2}

La seconda versione ha introdotto un framework di testing quantitativo basato su ground truth annotato manualmente. La metodologia prevede innanzitutto la \textbf{preparazione del dataset} attraverso l'annotazione manuale di file di log con etichette binarie (Y/N) per ogni riga, indicando la presenza o assenza di dati sensibili. Successivamente, l'\textbf{elaborazione automatizzata} permette l'implementazione di un sistema di confronto automatico tra le risposte del modello e il ground truth, consentendo il \textbf{calcolo real-time delle metriche} di veri positivi, veri negativi, falsi positivi e falsi negativi. Infine, i \textbf{test comparativi} permettono la valutazione sistematica di modelli LLM differenti con configurazioni standardizzate.

Questa evoluzione metodologica ha permesso un confronto oggettivo e riproducibile delle performance tra diversi modelli e configurazioni.

\section{Metriche di valutazione}
\label{sec:metriche_test}

Il sistema di valutazione utilizza metriche standard di classificazione binaria, adattate al contesto specifico dell'identificazione di dati sensibili nei log.

\subsubsection{Metriche primarie}
\label{subsubsec:metriche_primarie}

Le metriche principali utilizzate per la valutazione sono elencate nella tabella~\refwithpage{tab:metriche_primarie}.

\begin{table}[h!]
    \centering
    \begin{tabular}{|p{0.25\textwidth}|p{0.65\textwidth}|}
        \hline
        \textbf{Metrica} & \textbf{Descrizione} \\
        \hline
        Veri Positivi (TP) & Righe contenenti dati sensibili correttamente identificate dal modello \\
        \hline
        Veri Negativi (TN) & Righe senza dati sensibili correttamente classificate come non sensibili \\
        \hline
        Falsi Positivi (FP) & Righe senza dati sensibili erroneamente identificate come contenenti dati sensibili \\
        \hline
        Falsi Negativi (FN) & Righe contenenti dati sensibili non identificate dal modello \\
        \hline
    \end{tabular}
    \caption{Metriche primarie di classificazione}
    \label{tab:metriche_primarie}
\end{table}

\subsubsection{Metriche derivate}
\label{subsubsec:metriche_derivate}

Dalle metriche primarie vengono calcolate le percentuali elencate nella tabella~\refwithpage{tab:metriche_derivate}.

\begin{table}[h!]
    \centering
    \begin{tabular}{|p{0.3\textwidth}|p{0.25\textwidth}|p{0.35\textwidth}|}
        \hline
        \textbf{Metrica} & \textbf{Formula} & \textbf{Descrizione} \\
        \hline
        Accuratezza Complessiva & $\frac{TP + TN}{TP + TN + FP + FN} \times 100$ & Percentuale complessiva di classificazioni corrette \\
        \hline
        Percentuale di Falsi Positivi & $\frac{FP}{TP + TN + FP + FN} \times 100$ & Incidenza di righe non sensibili classificate come sensibili \\
        \hline
        Percentuale di Falsi Negativi & $\frac{FN}{TP + TN + FP + FN} \times 100$ & Incidenza di righe sensibili non identificate \\
        \hline
    \end{tabular}
    \caption{Metriche derivate di valutazione}
    \label{tab:metriche_derivate}
\end{table}

Nel contesto applicativo, i falsi negativi rappresentano un rischio di sicurezza maggiore rispetto ai falsi positivi, in quanto comportano la mancata identificazione di dati sensibili effettivamente presenti nei log.

\section{Risultati sperimentali}
\label{sec:risultati_sperimentali}

I test sono stati condotti su architettura Apple M1 Max (32 GB di memoria unificata, 32 GPU Core) utilizzando il framework Ollama per l'esecuzione locale dei modelli LLM.

\subsubsection{Risultati Versione 1}
\label{subsubsec:risultati_v1}

La prima versione ha fornito risultati promettenti ma ha evidenziato significative limitazioni hardware, come mostrato nella tabella~\refwithpage{tab:risultati_v1}.

\begin{table}[h!]
    \centering
    \begin{tabular}{|l|c|c|c|c|c|}
        \hline
        \textbf{Modello} & \textbf{Righe}                                        & \textbf{Righe/msg} & \textbf{Msg/chat} & \textbf{Tempo} & \textbf{Falsi pos.} \\ \hline
        Llama3 8b        & 100                                                   & 3                  & 1                 & 01m 42s        & $\approx$15\%       \\ \hline
        Llama3 8b        & 1000                                                  & 3                  & 1                 & 14m 05s        & $\approx$5\%        \\ \hline
        Llama3.1 8b      & 100                                                   & 3                  & 1                 & 02m 27s        & $\approx$5\%        \\ \hline
        Llama3.1 8b      & 1000                                                  & 3                  & 1                 & 20m 26s        & $\approx$5\%        \\ \hline
        Llama3 70b       & \multicolumn{5}{c|}{\textit{Memoria non sufficiente}}                                                                                 \\ \hline
        Llama3.1 70b     & \multicolumn{5}{c|}{\textit{Memoria non sufficiente}}                                                                                 \\ \hline
        Command-R 35b    & \multicolumn{5}{c|}{\textit{Memoria non sufficiente}}                                                                                 \\ \hline
    \end{tabular}
    \caption{Risultati della versione 1 su Apple M1 Max}
    \label{tab:risultati_v1}
\end{table}
\todo{TODO ri-verificare tabella}

I risultati mostrano che:
\begin{itemize}
    \item I modelli a 8 miliardi di parametri sono eseguibili con performance accettabili
    \item L'incremento del dataset da 100 a 1000 righe migliora l'accuratezza riducendo i falsi positivi
    \item I modelli più grandi (70b+ parametri) superano i limiti hardware disponibili
\end{itemize}

\subsubsection{Risultati Versione 2}
\label{subsubsec:risultati_v2}

La seconda versione ha introdotto metriche quantitative precise permettendo un confronto sistematico, come mostrato nella tabella~\refwithpage{tab:risultati_v2} e il grafico~\refwithpage{fig:grafico_risultati_v2}.

\begin{table}[h!]
    \centering
    \begin{tabular}{|l|c|c|c|c|c|c|}
        \hline
        \textbf{Modello} & \textbf{Righe} & \textbf{Tempo} & \textbf{VN}   & \textbf{VP}   & \textbf{FN}   & \textbf{FP}   \\ \hline
        llama3.1:8b      & 100            & 00m 57s        & $\approx$70\% & $\approx$14\% & $\approx$10\% & $\approx$6\%  \\ \hline
        llama3.1:8b      & 1000           & 09m 05s        & $\approx$68\% & $\approx$14\% & $\approx$5\%  & $\approx$13\% \\ \hline
        llama3.2:3b      & 100            & 01m 08s        & $\approx$64\% & $\approx$21\% & $\approx$3\%  & $\approx$12\% \\ \hline
        llama3.2:3b      & 1000           & 07m 15s        & $\approx$59\% & $\approx$16\% & $\approx$2\%  & $\approx$23\% \\ \hline
        mistral:7b       & 100            & 06m 48s        & $\approx$0\%  & $\approx$24\% & $\approx$0\%  & $\approx$76\% \\ \hline
        mistral:7b       & 1000           & 72m 58s        & $\approx$0\%  & $\approx$19\% & $\approx$0\%  & $\approx$81\% \\ \hline
        mistral-nemo:12b & 100            & 04m 10s        & $\approx$45\% & $\approx$13\% & $\approx$11\% & $\approx$31\% \\ \hline
        mistral-nemo:12b & 1000           & 35m 15s        & $\approx$48\% & $\approx$12\% & $\approx$7\%  & $\approx$33\% \\ \hline
        qwen2.5:7b       & 100            & 01m 45s        & $\approx$76\% & $\approx$21\% & $\approx$3\%  & $\approx$0\%  \\ \hline
        qwen2.5:7b       & 1000           & 16m 44s        & $\approx$68\% & $\approx$18\% & $\approx$1\%  & $\approx$13\% \\ \hline
        gemma3:4b        & 100            & 01m 17s        & $\approx$58\% & $\approx$21\% & $\approx$3\%  & $\approx$18\% \\ \hline
        gemma3:12b       & 100            & 03m 18s        & $\approx$74\% & $\approx$23\% & $\approx$1\%  & $\approx$2\%  \\ \hline
        deepseek         & todo           & todo           & todo          & todo          & todo          & todo          \\ \hline
        deepseek         & todo           & todo           & todo          & todo          & todo          & todo          \\ \hline
    \end{tabular}
    \caption{Risultati della versione 2 con metriche quantitative}
    \label{tab:risultati_v2}
\end{table}

\todo{TODO: aggiungere i risultati dei modelli deepseek}
\todo{TODO: aggiungere i risultati dei modelli gemma con 1000 righe}

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
                ybar,
                width=\textwidth,
                height=8cm,
                xlabel={Modelli LLM},
                ylabel={Accuratezza (\%)},
                symbolic x coords={llama3.1:8b, llama3.2:3b, mistral:7b, mistral-nemo:12b, qwen2.5:7b, gemma3:4b, gemma3:12b, deepseek:7b, deepseek:8b},
                xtick=data,
                x tick label style={rotate=45, anchor=east},
                legend pos=north west,
                bar width=15pt,
                ymin=0,
                ymax=100
            ]

            % Accuratezza su 100 righe (VN% + VP%)
            \addplot coordinates {
                    (llama3.1:8b, 84)      % 70+14
                    (llama3.2:3b, 85)      % 64+21
                    (mistral:7b, 24)       % 0+24
                    (mistral-nemo:12b, 58) % 45+13
                    (qwen2.5:7b, 97)       % 76+21
                    (gemma3:4b, 79)        % 58+21
                    (gemma3:12b, 97)       % 74+23
                    (deepseek:7b, 1)       % TODO: placeholder
                    (deepseek:8b, 1)       % TODO: placeholder
                };

            % Accuratezza su 1000 righe (VN% + VP%) dove disponibile
            \addplot coordinates {
                    (llama3.1:8b, 82)      % 68+14
                    (llama3.2:3b, 75)      % 59+16
                    (mistral:7b, 19)       % 0+19
                    (mistral-nemo:12b, 60) % 48+12
                    (qwen2.5:7b, 86)       % 68+18
                    (gemma3:4b, 0)         % Non testato su 1000
                    (gemma3:12b, 0)        % Non testato su 1000
                    (deepseek:7b, 1)       % TODO: placeholder
                    (deepseek:8b, 1)       % TODO: placeholder
                };

            \legend{100 righe, 1000 righe}
        \end{axis}
    \end{tikzpicture}
    \caption{Confronto dell'accuratezza dei modelli LLM sui dataset di test}
    \label{fig:grafico_risultati_v2}
\end{figure}


\subsubsection{Confronto dei modelli LLM utilizzati}
\label{subsubsec:confronto_modelli}

\subsubsection{Modelli ad alte performance}
I modelli che hanno mostrato le migliori performance complessive sono:

\begin{itemize}
    \item \textbf{Qwen2.5:7b}: Eccellente combinazione di accuratezza ($\approx$94\% su 100 righe, $\approx$86\% su 1000 righe) e velocità di esecuzione
    \item \textbf{Gemma3:12b}: Migliore accuratezza ($\approx$97\% su 100 righe) ma tempi di esecuzione più elevati
    \item \textbf{Llama3.1:8b}: Performance equilibrate con buona scalabilità
\end{itemize}

\subsubsection{Modelli problematici}
Alcuni modelli hanno mostrato comportamenti problematici:

\begin{itemize}
    \item \textbf{Mistral:7b}: Tasso di falsi positivi estremamente elevato ($\approx$76-81\%), probabilmente dovuto a una configurazione di prompt non ottimale per questo modello specifico
    \item \textbf{Mistral-nemo:12b}: Performance mediocri nonostante le dimensioni maggiori
\end{itemize}

\subsubsection{Modelli reasoning}
\label{subsubsec:modelli_reasoning}

Alcuni modelli testati includono capacità di reasoning avanzate, utilizzando tag speciali per mostrare il processo di ragionamento interno. I modelli DeepSeek R1 (7B e 8B) utilizzano tag \texttt{<think>...</think>} per visualizzare il ragionamento prima della risposta finale.

Il sistema implementa un meccanismo di post-processing per rimuovere automaticamente questi tag di reasoning e considerare solo la risposta finale, garantendo compatibilità con il formato di output atteso.

\section{Analisi delle performance}
\label{sec:analisi_performance}

\subsubsection{Relazione dimensioni-accuratezza}
\label{subsubsec:relazione_dimensioni_accuratezza}

L'analisi delle performance evidenzia diversi fattori critici per l'efficacia del sistema.

Contrariamente alle aspettative, i modelli più grandi non sempre offrono performance superiori:
\begin{itemize}
    \item Qwen2.5:7b supera modelli più grandi come Mistral-nemo:12b
    \item Llama3.2:3b mostra performance comparabili a modelli significativamente più grandi
    \item La qualità del fine-tuning sembra essere più importante delle dimensioni del modello
\end{itemize}

\subsubsection{Efficienza computazionale}
\label{subsubsec:efficienza_computazionale}

I risultati mostrano variazioni significative nell'efficienza computazionale. In particolare, modelli come Llama3.2:3b, Qwen2.5:7b e Gemma3:4b completano l'elaborazione di 100 righe in meno di due minuti, mentre Mistral:7b richiede oltre sei minuti per lo stesso carico. Da ciò si evince che una maggiore velocità non si traduce necessariamente in minore accuratezza.
\todo{TODO aggiungere più confronti}

\subsubsection{Limitazioni hardware}
\label{subsubsec:limitazioni_hardware}
\todo{TODO Rivalutare in seguito a test con altro hw}

L'hardware utilizzato (Apple M1 Max, 32GB RAM) ha posto vincoli significativi:
\begin{itemize}
    \item Impossibilità di eseguire modelli superiori a 12-15 miliardi di parametri
    \item Necessità di ottimizzazioni specifiche per l'architettura Apple Silicon
    \item Compromessi tra dimensioni del modello e velocità di inferenza
\end{itemize}

I risultati confermano la fattibilità dell'utilizzo di LLM per l'identificazione di dati sensibili, evidenziando l'importanza della selezione del modello appropriato in base ai requisiti specifici di accuratezza, velocità e risorse disponibili.



%
%			CAPITOLO: Conclusioni e sviluppi futuri
%

\todochapter{Conclusioni}{chap:conclusioni}{conclusioni}

\todosection{Risultati raggiunti}{sec:risultati_raggiunti}{finire}

Questo progesso ci ha permesso di confermare l'utilizzo di LLM per l'identificazione e il report di dati sensibili all'interno di log di sistemi complessi in un'ambiente di produzione.

\section{Sviluppi futuri}
\label{sec:sviluppi_futuri}

\subsubsection{Addestramento di LoRA}
\label{subsubsec:addestramento_lora}

LoRA (Low-Rank Adaptation) è una tecnica di fine-tuning efficiente che modifica solo un sottoinsieme dei parametri del modello e permette di creare un nuovo LLM a partire da uno già addestrato.

Tramite questa tecnica è possibile addestrare un modello per un dominio specifico, ad esempio il dominio dei log, senza dover addestrare un nuovo modello completamente da zero, operazione estremamente complicata da eseguire nei progetti di ricerca accademica a causa dei costi computazionali elevati, dei tempi di sviluppo estremamente lunghi, delle competenze multidisciplinari richieste e della necessità di dataset di addestramento di dimensioni elevate.

In seguito a un periodo prolungato di utilizzo del sistema in ambiente di produzione e di raccolta sistematica dei dati e dei report generati, sarebbe possibile utilizzare questi dati come dataset di training per addestrare un LoRA specificamente ottimizzato per questo caso d'uso. \\
Il fine-tuning potrebbe partire da uno dei modelli LLM attualmente utilizzati che ha dimostrato le performance migliori durante la fase di testing.

\subsubsection{Integrazione con altri sistemi}
\label{subsubsec:integrazione_sistemi}

La versione finale del progetto è stata sviluppata specificamente per l'integrazione con GrayLog e lo script di setup dell'ambiente è stato scritto per interagire con questa piattaforma. \\
Un possibile sviluppo futuro consiste nella generalizzazione delle integrazioni attraverso lo sviluppo di un protocollo standard che consenta la creazione di adattatori modulari per diversi sistemi di gestione dei log.

Questa evoluzione permetterebbe di:
\begin{enumerate}
    \item Estendere la compatibilità del sistema a piattaforme alternative come Elasticsearch\footnote{Elasticsearch \url{https://www.elastic.co/elasticsearch}}, Splunk\footnote{Splunk \url{https://www.splunk.com}}, o Fluentd\footnote{Fluentd \url{https://www.fluentd.org}}
    \item Standardizzare le interfacce di comunicazione tra il sistema di analisi e i sistemi di logging
    \item Semplificare la distribuzione del sistema in ambienti enterprise eterogenei
    \item Migliorare la tracciabilità e la gestione delle notifiche attraverso un framework unificato
\end{enumerate}

Un protocollo di integrazione standardizzato ridurrebbe significativamente la complessità di deployment e aumenterebbe l'adozione del sistema in contesti operativi diversificati.

\subsubsection{Rassegna delle alternative a GrayLog}
\label{subsubsec:alternative_graylog}

Tra le alternative più rilevanti si distinguono diverse categorie di strumenti.\\
\textbf{SigNoz}\footnote{SigNoz \url{https://signoz.io}} rappresenta una soluzione moderna open source focalizzata sull'osservabilità completa, offrendo un pannello unificato per visualizzare traces, metriche e log provenienti da OpenTelemetry. Utilizza ClickHouse\footnote{ClickHouse \url{https://clickhouse.com}} come database per garantire performance elevate e supporta l'ingestione di dati da oltre 50 sorgenti diverse, rendendolo particolarmente adatto ad ambienti cloud-native dove l'integrazione con standard aperti è prioritaria. La piattaforma offre inoltre correlazione automatica tra i diversi segnali telemetrici per un debugging più efficace.

\textbf{Logstash}\footnote{Logstash \url{https://www.elastic.co/logstash}} si configura come una pipeline di data processing server-side altamente flessibile con un framework estensibile che conta oltre 200 plugin. Il sistema implementa code persistenti per garantire delivery at-least-once degli eventi e supporta monitoraggio centralizzato delle pipeline attraverso un'interfaccia unificata, risultando ideale in scenari dove è necessaria una pre-elaborazione complessa dei dati.

\textbf{FluentD}\footnote{FluentD \url{https://www.fluentd.org}} emerge come un data collector open source caratterizzato da un'architettura basata su plugin che conta oltre 500 estensioni disponibili. Progetto della Cloud Native Computing Foundation (CNCF)\footnote{CNCF \url{https://www.cncf.io}}, fornisce un layer di logging unificato che consente di raccogliere log da sorgenti eterogenee e inoltrarli a destinazioni multiple, disaccoppiando le sorgenti dati dai sistemi backend.

\textbf{Syslog-ng}\footnote{Syslog-ng \url{https://www.syslog-ng.com}}, implementazione avanzata del protocollo syslog\footnote{Syslog è un protocollo standard per il logging di sistema definito dall'RFC 3164 (\url{https://datatracker.ietf.org/doc/html/rfc3164})}, fornisce funzionalità di raccolta, elaborazione e salvataggio dei log con particolare enfasi sulla compatibilità con sistemi legacy e sulla conformità agli standard di compliance.

Infine, \textbf{Apache Flume}\footnote{Apache Flume \url{https://flume.apache.org}} si posiziona come un servizio distribuito specializzato nel raccoglimento, aggregazione e spostamento di grandi volumi di streaming event data attraverso un modello basato su data flows.

L'integrazione del \textit{Sensitive Data Detector} con queste piattaforme alternative richiederebbe l'adattamento del layer di comunicazione e la definizione delle pipeline di elaborazione, mantenendo invariata la logica di analisi basata su LLM e regex.



%
%			APPENDICE: materiali aggiuntivi e dimostrazioni
%

\appendix

\todochapter{Codice}{chap:appendice_codice}{riguardare e sistemare commenti nel codice}

\section{Versione 1: Proof of Concept}

\lstinputlisting[
caption=Codice sorgente della versione 1,
label=lst:code_ver1,
language=Python,
literate={è}{{\`e}}1 {à}{{\`a}}1 {ò}{{\`o}}1 {ù}{{\`u}}1 {ì}{{\`i}}1
]{"../ollama extractor v1/main.py"}


\lstinputlisting[
caption=Prompt usato nella versione 1,
label=lst:prompt_ver1,
literate={è}{{\`e}}1 {à}{{\`a}}1 {ò}{{\`o}}1 {ù}{{\`u}}1 {ì}{{\`i}}1
]{"../ollama extractor v1/prompt.txt"}
\todo{sistemare testo che esce dai confini del box}

\clearpage

\section{Versione 2: Elaborazione di batch di log}

\lstinputlisting[
caption=Codice sorgente della versione 2,
label=lst:code_ver2,
language=Python,
literate={è}{{\`e}}1 {à}{{\`a}}1 {ò}{{\`o}}1 {ù}{{\`u}}1 {ì}{{\`i}}1
]{"../ollama extractor v2/main.py"}

\clearpage

\section{Versione 3: Integrazione con GrayLog}

\lstinputlisting[
caption=Codice sorgente della versione 3,
label=lst:code_ver3,
language=Python,
literate={è}{{\`e}}1 {à}{{\`a}}1 {ò}{{\`o}}1 {ù}{{\`u}}1 {ì}{{\`i}}1
]{"../ollama extractor v3/main.py"}

\clearpage

\section{\textit{Modelfile}}
\label{sec:code_modelfile}

\lstinputlisting[
caption=\textit{Modelfile} di partenza usato per generare i modelli personalizzati,
label=lst:modelfile,
language=Python,
literate={è}{{\`e}}1 {à}{{\`a}}1 {ò}{{\`o}}1 {ù}{{\`u}}1 {ì}{{\`i}}1
]{"../modelfiles/sensitive-data-detector-BASE.modelfile"}
\todo{sistemare testo che esce dai confini del box}

\clearpage

\section{Script di configurazione GrayLog}
\label{sec:code_graylog_script}

\lstinputlisting[
caption=Script Python per la configurazione automatica di GrayLog,
label=lst:code_graylog_script,
language=Python,
literate={è}{{\`e}}1 {à}{{\`a}}1 {ò}{{\`o}}1 {ù}{{\`u}}1 {ì}{{\`i}}1
]{"../graylog docker/graylog_setup.py"}

\clearpage

\section{Configurazione Docker Compose}
\label{sec:code_container_config}

\lstinputlisting[
caption=File Docker Compose per l'ambiente di test con GrayLog,
label=lst:code_container_config,
literate={è}{{\`e}}1 {à}{{\`a}}1 {ò}{{\`o}}1 {ù}{{\`u}}1 {ì}{{\`i}}1
]{"../graylog docker/docker-compose.yml"}


%
%			BIBLIOGRAFIA
%

% Si può specificare a che livello della TOC deve essere la bibliografia.
% Il default è 'chapter', per 'part' usare
% \beforebibliography[part]
\beforebibliography
\bibliographystyle{unsrt}
\bibliography{bibliografia}

% Pagina di chiusura tesi
% \closingpage

% Pagina bianca finale
% \clearpage
% \thispagestyle{empty}
% \null
% \clearpage

\end{document}

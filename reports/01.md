# Report 01 Tesi Triennale Emanuele Magon, SSRI, Unimi

- Progetto: ricerca di contenuti sensibili all'interno di log (su GrayLog) di sistemi complessi
- Il tool deve trovare i dati sensibili, senza renderli anonimi
- Questo perché il tool deve dare un'indicatore di certezza di quanto i log siano privi di dati sensibili
- Il tool, per analizzare i log, deve applicare delle espressioni regolari e dovrebbe poter anche utilizzare modelli di ML
- Il formato delle righe dei log è:

|             | Nome file        | Data              | ID?       | Hostname?                                | Body in JSON (con chiavi "level" e "message") |
| ----------- | ---------------- | ----------------- | --------- | ---------------------------------------- | --------------------------------------------- |
| **Esempio** | `messages.2.gz:` | `Apr  7 00:03:39` | `ID24167` | `maestro3-devaccess\|55dacabaa607[561]:` | `{"level":"info","message":"..."}`            |

## Problema: come scrivere le espressioni regolari, considerando la grandezza (GB) dei log?

- La soluzione provata è stata quella di usare dei LLM (Large Language Model) per trovare i dati sensibili scorrendo i log
- Ogni volta che il LLM trova un dato sensibile, si crea ad-hoc una regex per quel tipo di dato
- Quando il LLM ha terminato la lettura, si copia il file di log rimuovendo le righe che corrispondono le regex create fino ad ora
- L'idea è che all'aumentare delle righe rimosse, il LLM dovrebbe trovare sempre meno dati sensibili
- È stato usato [ollama](https://ollama.com) per eseguire i LLM in locale
- Ollama permette di eseguire molteplici LLM
- I modelli utilizzati sono stati:
  - Llama3 da 8 miliardi di parametri (4.7 GB)
  - Llama3 da 70 miliardi di parametri (40 GB)
  - Llama3.1 da 8 miliardi di parametri (4.7 GB)
  - Llama3.1 da 70 miliardi di parametri (40 GB)
  - Command-R da 35 miliardi di parametri (20 GB)
- Inizialmente è stato usato un **Apple M1 Max** (32 GB di memoria unificata, 32 GPU Core) per eseguire gli LLM
- ~~In seguito si è passati al server **Jupiter-GPU** hostato su https://jhub.ricerca.sesar.di.unimi.it~~

## Implementazione

- È stato scritto uno script Python che si occupa di:
  - Leggere i file di log
  - Preparare il prompt in modo da istruire il LLM
  - Interfacciarsi con Ollama per generare la risposta
- Per iniziare è stato usato Llama3 da 8b parametri. Quasi immediatamente è emerso che il numero di token degli LLM è un fattore importante da tenere in considerazione. Infatti, essendo i log molto grandi, dopo poche righe si può notare come l'LLM inizi a "perdere la memoria". In altre parole, l'LLM smette di considerare i messaggi precedenti, incluso il prompt iniziale.
- La soluzione più ovvia è stata quella di passare a modelli con più parametri, in modo da avere più token a disposizione
- Si è passati quindi a Llama3 da 70b parametri e Command-R da 35b parametri, ma in entrambi i casi è sorto un altro problema: la VRAM
- Essendo i modelli da più parametri molto grandi, l'Apple M1 Max ha dovuto ricorre allo lo swap su disco e ciò ha rallentato di parecchio l'esecuzione, rendendo impossibile l'utilizzo di questi modelli
- In seguito si è giunti ad una soluzione per entrambi questi problemi. La soluzione prevede di:
  1. Inviare al LLM solo poche righe del log alla volta
  2. Dopo un numero predefinito di messaggi inviati, terminare la "chat" con l'LLM e avviare una nuova istanza tralasciando le righe di log già analizzate
- Questa soluzione non è perfetta, in quanto ogni istanza di chat non conosce le risposte date dalle istanze precedenti e quindi spesso vengono generate risposte ridondanti
- Dopo un periodo di fine-tuning per ottimizzare il numero di righe da inviare per messaggio e il numero di messaggi da inviare per chat, si è riusciti a far analizzare migliaia di righe ai LLM

## Risultati

### Note

- _Il tempo di esecuzione è stato misurato tramite il comando `time`_
- _La percentuale di falsi positivi è calcolata come segue: (numero di tipologie di elementi erroneamente segnalati come sensibili) / (numero totale di righe analizzate) \* 100_

### Risultati

- Risultati dei vari LLM su **Apple M1 Max**:

| Modello       | Numero di righe analizzate  | Righe per messaggio | Messaggi per chat | Tempo di esecuzione         | % di falsi positivi         |
| ------------- | --------------------------- | ------------------- | ----------------- | --------------------------- | --------------------------- |
| Llama3 8b     | `500`                       | `3`                 | `1`               | `01m 42s`                   | `~ 15%`                     |
| Llama3 8b     | `1000`                      | `3`                 | `1`               | `14m 05s`                   | `~ 05%`                     |
| Llama3 70b    | (_Memoria non sufficiente_) | `10`                | `5`               | (_Memoria non sufficiente_) | (_Memoria non sufficiente_) |
| Llama3.1 8b   | `500`                       | `3`                 | `1`               | `02m 27s`                   | `~ 05%`                     |
| Llama3.1 8b   | `1000`                      | `3`                 | `1`               | `20m 26s`                   | `~ 05%`                     |
| Llama3.1 70b  | (_Memoria non sufficiente_) | `10`                | `5`               | (_Memoria non sufficiente_) | (_Memoria non sufficiente_) |
| Command-R 35b | (_Memoria non sufficiente_) | `5`                 | `5`               | (_Memoria non sufficiente_) | (_Memoria non sufficiente_) |

- Risultati dei vari LLM su **Jupiter-GPU**:

| Modello       | Numero di righe analizzate | Righe per messaggio | Messaggi per chat | Tempo di esecuzione | % di falsi positivi |
| ------------- | -------------------------- | ------------------- | ----------------- | ------------------- | ------------------- |
| Llama3 8b     | `500`                      | `3`                 | `1`               | to-do               | to-do               |
| Llama3 8b     | `1000`                     | `3`                 | `1`               | to-do               | to-do               |
| Llama3 70b    | `500`                      | `10`                | `5`               | to-do               | to-do               |
| Llama3 70b    | `1000`                     | `10`                | `5`               | to-do               | to-do               |
| Llama3.1 8b   | `500`                      | `3`                 | `1`               | to-do               | to-do               |
| Llama3.1 8b   | `1000`                     | `3`                 | `1`               | to-do               | to-do               |
| Llama3.1 70b  | `500`                      | `10`                | `5`               | to-do               | to-do               |
| Llama3.1 70b  | `1000`                     | `10`                | `5`               | to-do               | to-do               |
| Command-R 35b | `500`                      | `5`                 | `5`               | to-do               | to-do               |
| Command-R 35b | `1000`                     | `5`                 | `5`               | to-do               | to-do               |

# Report 01 Tesi Triennale Emanuele Magon, SSRI, Unimi

- Progetto: tool per ricerca di contenuti sensibili all'interno di log (su GrayLog) di sistemi complessi
- Il tool deve trovare i dati sensibili, senza renderli anonimi
- Questo perché il tool deve dare un'indicatore di certezza di quanto i log siano privi di dati sensibili
- Il tool, per analizzare i log, deve applicare delle espressioni regolari e dovrebbe poter anche utilizzare modelli di ML
- Il formato delle righe dei log è:

|             | Nome file        | Data              | ID?       | Hostname?                                | Body in JSON (con chiavi "level" e "message") |
| ----------- | ---------------- | ----------------- | --------- | ---------------------------------------- | --------------------------------------------- |
| **Esempio** | `messages.2.gz:` | `Apr  7 00:03:39` | `ID24167` | `maestro3-devaccess\|55dacabaa607[561]:` | `{"level":"info","message":"..."}`            |

## Come scrivere le espressioni regolari, considerando la grandezza (GB) dei log?

- La soluzione provata è stata quella di usare dei LLM (Large Language Model) per trovare i dati sensibili all'interno dei log
- Ogni volta che il LLM trova un dato sensibile, si crea una regex ad-hoc per quel tipo di dato
- Quando il LLM ha terminato la lettura, si modifica il file di log rimuovendo le righe che corrispondono alle regex
- All'aumentare delle righe rimosse, il LLM dovrebbe trovare sempre meno dati sensibili
- È stato usato [ollama](https://ollama.com) per eseguire i LLM in locale
- Ollama permette di eseguire molteplici LLM
- I modelli utilizzati sono stati:
  - Llama3 da 8 miliardi di parametri (4.7 GB)
  - Llama3 da 70 miliardi di parametri (40 GB)
  - Llama3.1 da 8 miliardi di parametri (4.7 GB)
  - Llama3.1 da 70 miliardi di parametri (40 GB)
  - Command-R da 35 miliardi di parametri (20 GB)
- Inizialmente è stato usato un **Apple M1 Max** (32 GB di memoria unificata, 32 GPU Core) per eseguire gli LLM
- ~~In seguito si è passati al server **Jupiter-GPU** hostato su https://jhub.ricerca.sesar.di.unimi.it~~ (momentaneamente sospeso)

## Implementazione

- È stato scritto uno script Python che si occupa di:
  - Leggere il file di log
  - Interfacciarsi con Ollama
  - Inviare il prompt iniziale e il contenuto dei file di log al LLM
  - Ottenere e salvare la risposta del LLM
- Per iniziare è stato usato Llama3 da 8b parametri. Quasi immediatamente è emerso che il numero di token degli LLM è un fattore importante da tenere in considerazione. Infatti, essendo i log molto grandi, dopo poche righe si può notare come l'LLM inizi a "perdere la memoria". In altre parole, l'LLM smette di considerare i messaggi precedenti, incluso il prompt iniziale.
- La soluzione più ovvia è stata quella di passare a modelli con più parametri, in modo da avere più token a disposizione
- Si è passati quindi a Llama3 da 70b parametri e Command-R da 35b parametri, ma in entrambi i casi è sorto un altro problema: la VRAM
- Questi modelli, essendo da più parametri, sono particolarmente grandi in termini di GB. L'Apple M1 Max ha dovuto ricorre allo lo swap su disco e ciò ha rallentato di parecchio l'esecuzione, rendendo impossibile l'utilizzo di questi modelli
- In seguito si è giunti ad una soluzione che prevede di:
  1. Inviare al LLM solo poche righe del log alla volta
  2. Dopo un numero predefinito di messaggi inviati, terminare la "chat" con il LLM e avviare una nuova istanza ripartendo con la lettura del log nel punto in cui la precedente si era fermata
- Questa soluzione non è perfetta, in quanto ogni istanza di chat non conosce le risposte date dalle istanze precedenti e quindi spesso vengono generate risposte ridondanti
- Dopo un periodo di fine-tuning per ottimizzare il numero di righe da inviare per messaggio e il numero di messaggi da inviare per chat, si è riusciti a far analizzare migliaia di righe ai LLM

## Benchmark

### Note

- _Il tempo di esecuzione è stato misurato tramite il comando `time`_
- _La percentuale di falsi positivi è calcolata come segue: (numero di tipologie di elementi erroneamente segnalati come sensibili) / (numero totale di righe analizzate) \* 100_
- _Il prompt iniziale inviato alle istanze dei LLM è riportato alla fine di questo report_

### Risultati

- Risultati dei vari LLM su **Apple M1 Max**:

| Modello       | Numero di righe analizzate  | Righe per messaggio | Messaggi per chat | Tempo di esecuzione         | % di falsi positivi         |
| ------------- | --------------------------- | ------------------- | ----------------- | --------------------------- | --------------------------- |
| Llama3 8b     | `500`                       | `3`                 | `1`               | `01m 42s`                   | `~ 15%`                     |
| Llama3 8b     | `1000`                      | `3`                 | `1`               | `14m 05s`                   | `~ 05%`                     |
| Llama3 70b    | (_Memoria non sufficiente_) | `10`                | `5`               | (_Memoria non sufficiente_) | (_Memoria non sufficiente_) |
| Llama3.1 8b   | `500`                       | `3`                 | `1`               | `02m 27s`                   | `~ 05%`                     |
| Llama3.1 8b   | `1000`                      | `3`                 | `1`               | `20m 26s`                   | `~ 05%`                     |
| Llama3.1 70b  | (_Memoria non sufficiente_) | `10`                | `5`               | (_Memoria non sufficiente_) | (_Memoria non sufficiente_) |
| Command-R 35b | (_Memoria non sufficiente_) | `5`                 | `5`               | (_Memoria non sufficiente_) | (_Memoria non sufficiente_) |

### Prompt iniziale inviato ai LLM

````plaintext
You are given an extract of logs from an application server. Each line represents a single log entry. Your task is to identify and extract any data that could be considered private or sensitive from these logs.

Guidelines:
- Sensitive Data: Extract all private/sensitive data, such as personal information, credentials, or tokens.
- Non-Sensitive Data: usernames, IDs, timestamps and labels ARE NOT considered sensitive. DO NOT extract or include them in your output.
- JSON objects: if you find a JSON object, extract the sensitive data from the object only, not the entire object.
- Output Format: List the extracted sensitive data in bullet points. Say the type of data and the value found. If you found no sensitive data, return "None".
- Avoid Redundancy: Ensure each type of sensitive data is listed only once, even if it appears multiple times in the logs.
- Conciseness: Do not include any additional information, commentary, notes or explanations. Only return the structured list of sensitive data.

Example:
```
Log extract:
Method set_result_command for thing_id 59f3a8c5-a405-47a4-a2e0-e39909deac69 took: 9.693563ms to complete without errors.
Method create_token for user: admin@revetec.it took: 97.468142ms to complete without errors
eneltecService, login success: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE3MTI0NzcxMDAsImlhdCI6MTcxMjQ0MTEwMCwiaXNzIjoibWFpbmZsdXguYXV0aCIsInN1YiI6ImFkbWluQHJldmV0ZWMuaXQiLCJpc3N1ZXJfaWQiOiI2MzNmZWRiYS1hOGE4LTRhOWUtOTU1MC0xODNlN2Y2YjBmMjkiLCJ0eXBlIjowfQ._fdS_wKnl9ARlFYc6KwbMCYSMgj0sbwcQzEZSYzttcI

Expected output:
- token eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE3MTI0NzcxMDAsImlhdCI6MTcxMjQ0MTEwMCwiaXNzIjoibWFpbmZsdXguYXV0aCIsInN1YiI6ImFkbWluQHJldmV0ZWMuaXQiLCJpc3N1ZXJfaWQiOiI2MzNmZWRiYS1hOGE4LTRhOWUtOTU1MC0xODNlN2Y2YjBmMjkiLCJ0eXBlIjowfQ._fdS_wKnl9ARlFYc6Kw
- email admin@revetec.it
```

Log extract:
````
